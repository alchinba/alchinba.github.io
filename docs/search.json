[
  {
    "objectID": "cluster_ipl.html",
    "href": "cluster_ipl.html",
    "title": "Cluster Analysis IPL",
    "section": "",
    "text": "Here I explore the performance of the Indian Premier League batsmen in cricket. The dataset includes the popular performance metrics for 92 players: Average, Total Runs, Strike Rate, Hundreds, Fifties, Fours, Sixes, and salaries. In short, the higher the value of the metrics, the better a batsman is. The Average reflects the total number of runs scored divided by the number of innings in a season. The SR is calculated by dividing the number of runs scored by the number of balls faced and then multiplying by 100. It shows if a batsman scores runs at a faster pace. The rest of the metrics reflect the numbers of times a player scored, for example, fifty or more runs per inning. An inning is a team’s turn to bat.\nI am analyzing the correspondence of the players’ pay and their performance. I am going to look at SR and Avg. These two metrics should be of critical meaning since they signal the relative performance. In other words, if we compare two player’s SR (Strikes Rate) rather than absolute scores, we can observe their ability to score high (runs per 100 balls faced). This is more clear than absolute numbers of runs, since batsmen have to be called to face the ball, which can skew the final number. The same could be said about Avg."
  },
  {
    "objectID": "cluster_ipl.html#instroduction",
    "href": "cluster_ipl.html#instroduction",
    "title": "Cluster Analysis IPL",
    "section": "",
    "text": "Here I explore the performance of the Indian Premier League batsmen in cricket. The dataset includes the popular performance metrics for 92 players: Average, Total Runs, Strike Rate, Hundreds, Fifties, Fours, Sixes, and salaries. In short, the higher the value of the metrics, the better a batsman is. The Average reflects the total number of runs scored divided by the number of innings in a season. The SR is calculated by dividing the number of runs scored by the number of balls faced and then multiplying by 100. It shows if a batsman scores runs at a faster pace. The rest of the metrics reflect the numbers of times a player scored, for example, fifty or more runs per inning. An inning is a team’s turn to bat.\nI am analyzing the correspondence of the players’ pay and their performance. I am going to look at SR and Avg. These two metrics should be of critical meaning since they signal the relative performance. In other words, if we compare two player’s SR (Strikes Rate) rather than absolute scores, we can observe their ability to score high (runs per 100 balls faced). This is more clear than absolute numbers of runs, since batsmen have to be called to face the ball, which can skew the final number. The same could be said about Avg."
  },
  {
    "objectID": "cluster_ipl.html#data-exploration",
    "href": "cluster_ipl.html#data-exploration",
    "title": "Cluster Analysis IPL",
    "section": "Data Exploration",
    "text": "Data Exploration\n\n\nCode\nlibrary(readxl)\nCricket_Data &lt;- read_excel(\"The Indian Premier League Supplemental Data.xlsx\", \n    sheet = \"Sheet1\", col_types = c(\"numeric\", \n        \"skip\", \"text\", \"text\", \"numeric\", \n        \"numeric\", \"numeric\", \"numeric\", \n        \"numeric\", \"numeric\", \"numeric\", \n        \"numeric\"))\n\n\nNew names:\n• `` -&gt; `...1`\n\n\n\n\nCode\nlibrary(psych) #describe()\nlibrary(summarytools) #descr()\n\n\nHere is a quick preview of the dataset:\n\n\nCode\nhead(Cricket_Data)\n\n\n# A tibble: 6 × 11\n   ...1 Player       Team   Runs   Avg    SR Hundreds Fifties Fours Sixes Salary\n  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1 AB de Villi… Roya…   442  44.2 154          0       5    31    26  1.72 \n2     2 Ajinkya Rah… Raja…   393  32.8 138.         1       1    45     9  0.625\n3     3 Akshdeep Na… Roya…    61  12.2 107.         0       0     5     2  0.514\n4     4 Ambati Rayu… Chen…   282  23.5  93.1        0       1    20     7  0.344\n5     5 Andre Russe… Kolk…   510  56.7 205.         0       4    31    52  1.33 \n6     6 Axar Patel   Roya…   110  18.3 125          0       0    10     3  0.714\n\n\nHere we can see a table of descriptive statistics for the performance metrics:\n\n\nCode\ndescr(Cricket_Data[, -c(1,2,3)],  #without ...1, Player, Team\n      stats = c(\"mean\", \"sd\",\"med\", \"min\", \"max\"), \n      transpose = TRUE)\n\n\nDescriptive Statistics  \nCricket_Data  \nN: 70  \n\n                   Mean   Std.Dev   Median     Min      Max\n-------------- -------- --------- -------- ------- --------\n           Avg    28.58     14.89    27.07    3.60    83.20\n       Fifties     1.49      1.73     1.00    0.00     8.00\n         Fours    22.40     16.84    19.00    1.00    64.00\n      Hundreds     0.09      0.28     0.00    0.00     1.00\n          Runs   249.26    168.75   216.50   12.00   692.00\n        Salary     0.91      0.67     0.70    0.03     2.66\n         Sixes    10.51      9.57     8.00    0.00    52.00\n            SR   133.12     25.18   133.31   63.15   204.81\n\n\n\n\nCode\nlibrary(ggplot2)\n\n\n\nAttaching package: 'ggplot2'\n\n\nThe following objects are masked from 'package:psych':\n\n    %+%, alpha\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n✔ purrr   1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()   masks psych::%+%()\n✖ ggplot2::alpha() masks psych::alpha()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ tibble::view()   masks summarytools::view()\n\n\nHere I plot the Salary against the Avg.\n\n\nCode\nCricket_Data %&gt;% \n  ggplot(aes(x=Avg, y=Salary)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\nWe can see that generally a higher Avg is associated with higher salary.\nI now plot SR against the Salary.\n\n\nCode\nCricket_Data %&gt;% \n  ggplot(aes(x=SR, y=Salary)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\nHere we can observe almost no visual relationship between the two.\nI then build a simple correlation matrix to see which metrics are highly correlated with salary:\n\n\nCode\nCricket_Data[, -c(1,2,3)] %&gt;% \n  cor() %&gt;% \n  as.data.frame() %&gt;% \n  dplyr::select(Salary) |&gt;\n  arrange(desc(Salary))\n\n\n            Salary\nSalary   1.0000000\nRuns     0.3234302\nAvg      0.2919955\nFifties  0.2900521\nFours    0.2550323\nSixes    0.2468059\nHundreds 0.2331481\nSR       0.1055668\n\n\nWe observe a positive but weak correlation between Salary and most performance metrics. SR, however, has the lowest correlation.\nI generate boxplots for both metrics with outliers named, so we can see the distribution of the data and the names of the players who score the highest. The limitation would be that we do not see other players. This can be overcome by sorting the data by our metrics and see who ends up above the mean or another threshold.\n\n\nCode\nCricket_Data %&gt;% \n  ggplot(aes(x=\"\", y=Avg)) +\n  geom_boxplot() +\n    geom_jitter(color=\"black\", size=0.4, alpha=0.9) +\n    geom_text(data = subset(Cricket_Data, Avg&gt;60), aes(label = Player)) +\n    ggtitle(\"Boxplot for Avg\")\n\n\n\n\n\nWe can see that these two players have the highest Avg.\n\n\nCode\nCricket_Data %&gt;% \n  ggplot(aes(x=\"\", y=SR)) +\n  geom_boxplot() +\n    geom_jitter(color=\"black\", size=0.4, alpha=0.9) +\n    geom_text(data = subset(Cricket_Data, SR&gt;180), aes(label = Player))+\n    ggtitle(\"Boxplot for SR\")\n\n\n\n\n\nThe other two have the highest SR.\nI know plot Avg vs SR, to see the distribution of players according to these metrics.\n\n\nCode\nCricket_Data %&gt;% \n  ggplot(aes(x=SR, y=Avg)) +\n  geom_text(aes(label = Player), size = 2) +\n  labs(title = \"Plot of the Players' Avg vs SR\")\n\n\n\n\n\nHere we can identify the players with the highest combination of SR and Avg metrics. The closer the player is to the top right corner, the better performance he has, and reversely the closer he is to the bottom left, the worse their performance is. We can also observe that relatively few players have Avg above 40."
  },
  {
    "objectID": "cluster_ipl.html#clustering",
    "href": "cluster_ipl.html#clustering",
    "title": "Cluster Analysis IPL",
    "section": "Clustering",
    "text": "Clustering\nI then perform cluster analysis to discover any subgroups within the dataset. I use a clustering method called kmeans, which uses the Euclidean distances to subset our datapoints.\n\n\nCode\n#library(tidyverse)  # data manipulation\nlibrary(cluster)    # clustering algorithms\nlibrary(factoextra) # clustering algorithms & visualization (fviz...)\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n\nI remove the variables Team and “…1” in the data.\n\n\nCode\ndf = select(Cricket_Data, -Team, -\"...1\")\n\n\nI set the Players’ names as rownames to label the datapoints for better vizualization.\n\n\nCode\ndf2 = column_to_rownames(df, \"Player\")\n\n\nI check clustering without scaling the data first.\n\n\nCode\nset.seed(123)\n\nk4.2 &lt;- kmeans(df2, centers = 4, nstart = 25)\nfviz_cluster(k4.2, data = df2)\n\n\n\n\n\nAs we can see, unscaled data overlaps signficantly.\nI standardize the data for better clustering using scale() function.\n\n\nCode\ndf3 = as.data.frame(scale(df2))\n\n\nI check the scaling:\n\n\nCode\n#check scaling. Mean should be 0 and Std 1.\ndescr(df3, \n      stats = c(\"mean\", \"sd\",\"med\", \"min\", \"max\"), \n      transpose = TRUE)\n\n\nDescriptive Statistics  \ndf3  \nN: 70  \n\n                 Mean   Std.Dev   Median     Min    Max\n-------------- ------ --------- -------- ------- ------\n           Avg   0.00      1.00    -0.10   -1.68   3.67\n       Fifties   0.00      1.00    -0.28   -0.86   3.76\n         Fours   0.00      1.00    -0.20   -1.27   2.47\n      Hundreds   0.00      1.00    -0.30   -0.30   3.24\n          Runs   0.00      1.00    -0.19   -1.41   2.62\n        Salary   0.00      1.00    -0.31   -1.32   2.60\n         Sixes   0.00      1.00    -0.26   -1.10   4.33\n            SR   0.00      1.00     0.01   -2.78   2.85\n\n\n\n\nCode\nk2 &lt;- kmeans(df3, centers = 2, nstart = 25)\n\n\nI visualize initial kmeans clustering with 2 clusters.\n\n\nCode\nfviz_cluster(k2, data = df3)\n\n\n\n\n\nI plot Clusters by Teams to see if it produces any insight.\n\n\nCode\nfviz_cluster(object = list(data = df3, \n                           cluster = Cricket_Data$Team),\n             geom = \"point\",\n             show.clust.cent = FALSE)\n\n\n\n\n\nWe can see that Players cannot be meaningfully clustered by Teams. That is, the teams do not differ from each other based on the metrics presented.\nI perform Elbow method to identify optimal number of Clusters.\n\n\n\n\n\nI use factoextra functions for better visualization.\n\n\nCode\nset.seed(123)\n\nfviz_nbclust(df3, kmeans, method = \"wss\") +\n  geom_hline(yintercept = 190, \n             linetype = \"dashed\", \n             color = \"red\") +\n  geom_hline(yintercept = 230, \n             linetype = \"dashed\", \n             color = \"brown\")\n\n\n\n\n\nCode\n#should check if k=5 is good too\n\n\nThis chart shows that Total Within Sum of Squares drop rate decreases after 4 and even more so after 5 clusters. We can evaluate both cases and use Silhouette Method for further investigation.\n\n\nCode\nk4 &lt;- kmeans(df3, centers = 4, nstart = 25)\n\n\n\n\nCode\nfviz_cluster(k4, data = df3)\n\n\n\n\n\nClusters are almost not overlapping. Cluster 1 is quite big compared to others.\nHere are the cluster centers (means) and sizes of clusters.\n\n\nCode\nk4$centers\n\n\n        Runs        Avg          SR   Hundreds    Fifties      Fours      Sixes\n1 -0.8138651 -0.5593812 -0.32277994 -0.3039913 -0.7165009 -0.7826572 -0.6495237\n2  1.2370084  1.5176928  1.33963042 -0.3039913  1.0654740  0.5896937  2.2273208\n3  0.6760145  0.2087239  0.06149259 -0.3039913  0.6810246  0.8186736  0.3293470\n4  1.4157758  1.2012910  0.43562184  3.2425739  0.9693617  1.3713348  0.6253612\n       Salary\n1 -0.26225776\n2  1.06586416\n3 -0.05845941\n4  0.75599997\n\n\nCode\nk4$size\n\n\n[1] 37  6 21  6\n\n\nHere is the plot for Average Silhouette Method. ASM determines how well each object lies within its cluster. A high average silhouette width signals a good clustering.\n\n\nCode\nfviz_nbclust(df3, kmeans, method = \"silhouette\") +\n  geom_vline(xintercept = 5, \n             linetype = \"dashed\", \n             color = \"red\")\n\n\n\n\n\nCode\n#abline(v = 5, col = \"red\", lty = \"dashed\")\n#seems k=5 is the second highest \n\n\nAccording to the ASM plot, k=5 should be more optimal than k=4. I perform a new cluster analysis using 5 centers.\n\n\nCode\nk5 &lt;- kmeans(df3, centers = 5, nstart = 25)\nfviz_cluster(k5, data = df3)\n\n\n\n\n\nHere are the Cluster centers which we can interpret as means for each cluster (data is scaled).\n\n\nCode\nk5$centers\n\n\n        Runs        Avg          SR   Hundreds    Fifties      Fours      Sixes\n1  0.7470225  0.2122767 -0.04890149 -0.3039913  0.7518443  0.9510921  0.2651953\n2 -0.5874998 -0.0488647  0.61959027 -0.3039913 -0.6193189 -0.6313657 -0.2934097\n3  1.2370084  1.5176928  1.33963042 -0.3039913  1.0654740  0.5896937  2.2273208\n4  1.4157758  1.2012910  0.43562184  3.2425739  0.9693617  1.3713348  0.6253612\n5 -0.9146652 -0.8871118 -0.92070090 -0.3039913 -0.7257107 -0.8683501 -0.7803108\n        Salary\n1 -0.008588271\n2 -0.590968548\n3  1.065864161\n4  0.755999969\n5 -0.032797378\n\n\nI create a combined dataset with the cluster numbers as variables.\n\n\nCode\n#creating df with cluster indices as variable\nclusterdf = as.data.frame(k5$cluster)\n\n\n\n\nCode\n#Adding cluster indices and creating df4\ndf4 = df2 %&gt;% \n  bind_cols(clusterdf) %&gt;% \n  bind_cols(Cricket_Data$Team) %&gt;% \n  rename(cluster = `k5$cluster`) %&gt;% \n  rename(Team = \"...10\")\n\n\nNew names:\n• `` -&gt; `...10`\n\n\n\n\nCode\n#structure of df4\nstr(df4)\n\n\n'data.frame':   70 obs. of  10 variables:\n $ Runs    : num  442 393 61 282 510 110 123 12 490 405 ...\n $ Avg     : num  44.2 32.8 12.2 23.5 56.7 ...\n $ SR      : num  154 137.9 107 93.1 204.8 ...\n $ Hundreds: num  0 1 0 0 0 0 0 0 0 0 ...\n $ Fifties : num  5 1 0 1 4 0 0 0 4 4 ...\n $ Fours   : num  31 45 5 20 31 10 8 1 45 41 ...\n $ Sixes   : num  26 9 2 7 52 3 4 0 34 22 ...\n $ Salary  : num  1.719 0.625 0.514 0.344 1.328 ...\n $ cluster : int  3 4 5 5 3 5 5 5 3 1 ...\n $ Team    : chr  \"Royal Challengers Bangalore\" \"Rajasthan Royals\" \"Royal Challengers Bangalore\" \"Chennai Super Kings\" ...\n\n\nHere are the descriptive statistics for each cluster (data is unscaled).\n\n\nCode\nstby(\n  data = df4,\n  INDICES = df4$cluster, # by clusters\n  FUN = descr, # descriptive statistics from summarytools\n  stats = c(\"mean\", \"sd\",\"med\", \"min\", \"max\"), \n  transpose = TRUE\n)\n\n\nNon-numerical variable(s) ignored: Team\n\n\nDescriptive Statistics  \ndf4  \nGroup: cluster = 1  \nN: 19  \n\n                   Mean   Std.Dev   Median      Min      Max\n-------------- -------- --------- -------- -------- --------\n           Avg    31.74      5.73    31.62    22.06    43.00\n       cluster     1.00      0.00     1.00     1.00     1.00\n       Fifties     2.79      0.92     3.00     1.00     5.00\n         Fours    38.42     11.04    41.00    21.00    64.00\n      Hundreds     0.00      0.00     0.00     0.00     0.00\n          Runs   375.32     74.26   373.00   253.00   529.00\n        Salary     0.91      0.66     0.69     0.16     2.34\n         Sixes    13.05      5.56    11.00     4.00    25.00\n            SR   131.89     10.48   130.86   115.10   151.70\n\nGroup: cluster = 2  \nN: 17  \n\n                   Mean   Std.Dev   Median      Min      Max\n-------------- -------- --------- -------- -------- --------\n           Avg    27.85     10.89    26.62    14.60    52.75\n       cluster     2.00      0.00     2.00     2.00     2.00\n       Fifties     0.41      0.62     0.00     0.00     2.00\n         Fours    11.76      5.17    13.00     2.00    19.00\n      Hundreds     0.00      0.00     0.00     0.00     0.00\n          Runs   150.12     68.37   160.00    63.00   279.00\n        Salary     0.52      0.52     0.29     0.03     1.95\n         Sixes     7.71      5.76     7.00     1.00    22.00\n            SR   148.72     17.69   151.31   125.87   175.00\n\nGroup: cluster = 3  \nN: 6  \n\n                   Mean   Std.Dev   Median      Min      Max\n-------------- -------- --------- -------- -------- --------\n           Avg    51.18     16.97    44.43    37.53    83.20\n       cluster     3.00      0.00     3.00     3.00     3.00\n       Fifties     3.33      1.37     3.50     1.00     5.00\n         Fours    32.33      7.89    31.00    22.00    45.00\n      Hundreds     0.00      0.00     0.00     0.00     0.00\n          Runs   458.00     44.24   465.00   402.00   510.00\n        Salary     1.63      0.76     1.72     0.31     2.34\n         Sixes    31.83     10.53    28.00    23.00    52.00\n            SR   166.85     26.24   158.33   134.62   204.81\n\nGroup: cluster = 4  \nN: 6  \n\n                   Mean   Std.Dev   Median      Min      Max\n-------------- -------- --------- -------- -------- --------\n           Avg    46.47     15.31    44.05    32.75    69.20\n       cluster     4.00      0.00     4.00     4.00     4.00\n       Fifties     3.17      3.13     2.00     0.00     8.00\n         Fours    45.50      9.57    47.00    28.00    57.00\n      Hundreds     1.00      0.00     1.00     1.00     1.00\n          Runs   488.17    130.70   454.50   342.00   692.00\n        Salary     1.42      0.87     1.48     0.31     2.66\n         Sixes    16.50      5.92    15.50     9.00    25.00\n            SR   144.09      7.94   142.66   135.38   157.24\n\nGroup: cluster = 5  \nN: 22  \n\n                   Mean   Std.Dev   Median     Min      Max\n-------------- -------- --------- -------- ------- --------\n           Avg    15.37      7.49    16.31    3.60    35.33\n       cluster     5.00      0.00     5.00    5.00     5.00\n       Fifties     0.23      0.43     0.00    0.00     1.00\n         Fours     7.77      6.51     5.50    1.00    20.00\n      Hundreds     0.00      0.00     0.00    0.00     0.00\n          Runs    94.91     68.50    85.00   12.00   282.00\n        Salary     0.89      0.48     0.81    0.07     1.95\n         Sixes     3.05      1.91     3.00    0.00     7.00\n            SR   109.93     21.24   116.50   63.15   150.00\n\n\nWe can observe the means of the produced clusters and check if the high average performance corresponds with higher or lower average salaries. This would help to identify clusters which contain underpaid or overpaid players.\nIn the given tables, we can observe that Cluster 3 has the top average salary of 1.63. The second highest earning cluster is Cluster 4. We can also observe that these clusters contain the highest performing players since their average performing metrics are the highest among the cluster. The exceptions are SR, where Cluster 2 is performing better than Cluster 4, taking second place, and Fours, where Cluster 1 performs better than Cluster 3, coming second. Otherwise, we can see that they are the top performers and thus the top earners. Their performance in the key metrics is, on average, in the top 3 (even top 2, mostly) and have the highest average salaries. Even their minimums in Avg and SR are as high as average values of most other clusters, if not higher. The 6 players in Cluster 4 are the only ones scoring Hundreds, separating them from the 6 players of Cluster 3. We can therefore identify them as 12 top performers and top earners. Therefore, we can call the Cluster 3 “Top Performers” and Cluster 4 “Hundred Scorers”.\nThe other cluster is Cluster 1. We can call them as the “Average Player”. They are the second most populous cluster and perform average. Their metrics is in the middle between most of the Clusters, except SR (they are 3rd) and Fours (2nd). We can also identify them as average since they get the average pay. We can see that in the scaled cluster centers, where the closer the value is to 0, the closer it is to the mean.\nThe remaining two clusters are 2 and 5. Interestingly, we can see that Cluster 2 overperforms Cluster 5 by all metrics but gets the lowest salary on average (0.52 vs 0.89). Not only that, Cluster 5, on average, performs worse than any other cluster but the salary is very close to the pay of Cluster “Average” (0.89 and 0.91). It is also worth noticing that Cluster 2 performs the 2nd best by SR, one of the key metrics Mitra is interested in. All this indicates that Cluster 2 is underpaid. Thus, I call cluster 5 “Overpaid” and Cluster 2 “Underpaid”."
  },
  {
    "objectID": "cluster_ipl.html#parallel-coordinate-plot-for-cluster-interpretation",
    "href": "cluster_ipl.html#parallel-coordinate-plot-for-cluster-interpretation",
    "title": "Cluster Analysis IPL",
    "section": "Parallel Coordinate Plot for Cluster interpretation",
    "text": "Parallel Coordinate Plot for Cluster interpretation\n\n\nCode\nk5centers = k5$centers %&gt;% \n  as.data.frame() %&gt;% \n  rownames_to_column(var = \"cluster\")\n\n\n\n\nCode\nlibrary(GGally) #ggparcoord\nlibrary(plotly)\nlibrary(MASS) #parcoord\n\n\nTo better analyze and compare the clusters I use Parallel Coordinate Plot to see the Cluster centers (means) all at the same time. In this plot the higher values of the metrics are, the better. It means that high metrics values should be followed by high salary.\n\n\nCode\nggparcoord(data = k5centers,\n           columns = c(2:9),\n           groupColumn = \"cluster\",\n           scale = \"globalminmax\") +\n  geom_line(size = 1) +\n  scale_color_discrete(name = \"Cluster\",\n                       labels = c(\"Average Player\", \n                                  \"Underpaid\", \n                                  \"Top Performers\", \n                                  \"Hundred Scorers\",\n                                  \"Overpaid\")) +\n  labs(title = \"Parallel Coordinate Plot of Cluster Centers by Variables\")\n\n\n\n\n\nHundreds Variable skews the view a little, so I remove it from the plot but keep in mind that only “Hundred Scorers” cluster (Blue) has players who scored Hundreds.\n\n\nCode\nggparcoord(data = dplyr::select(k5centers, -Hundreds), #MASS masks select\n           columns = c(2:8),\n           groupColumn = \"cluster\",\n           scale = \"globalminmax\") +\n  geom_line(size = 1) +\n  scale_color_discrete(name = \"Cluster\",\n                       labels = c(\"Average Player\", \n                                  \"Underpaid\", \n                                  \"Top Performers\", \n                                  \"Hundred Scorers\",\n                                  \"Overpaid\")) +\n  labs(title = \"Parallel Coordinate Plot of Cluster Centers by Variables\")\n\n\n\n\n\nWe can observe that Clusters Top Performers and Hundred Scorers perform generally well and earn higher than average salaries. Cluster Average Player is somewhat in the middle and earns average salary. Most interestingly, Clusters Underpaid and Overpaid on average perform the worst but the Underpaid plays better than the Overpaid by all metrics and even has the second highest SR yet earns the least. This happens while the Overpaid earns as much as Average Player. It should also be noticed that the scaled average salaries of the Clusters Average Player, Top Performers, and Hundred Scorers are in the vicinity of the scaled Avg and SR, whereas the Clusters Underpaid and Overpaid are not. This might again indicate that players are underpaid and overpaid in the respective clusters, on average.\nI also show the cluster centers separately side by side.\n\n\nCode\nggparcoord(data = k5centers,\n           columns = c(2:9),\n           groupColumn = \"cluster\",\n           scale = \"globalminmax\") +\n  facet_wrap(~ cluster) +\n  labs(title = \"Cluster Centers by Variables\")\n\n\n\n\n\n\n\nCode\nggparcoord(data = dplyr::select(k5centers, -Hundreds),\n           columns = c(2:8),\n           groupColumn = \"cluster\",\n           scale = \"globalminmax\") +\n  facet_wrap(~ cluster) +\n  labs(title = \"Cluster Centers by Variables without Hundreds\")\n\n\n\n\n\n\n\nCode\nk4centers = k4$centers %&gt;% \n  as.data.frame() %&gt;% \n  rownames_to_column(var = \"cluster\")\n\n\nI create Parallel Coordinate Plot to identify the differences when we use 4 clusters instead of 5.\n\n\nCode\nggparcoord(data = dplyr::select(k4centers, -Hundreds),\n           columns = c(2:8),\n           groupColumn = \"cluster\",\n           scale = \"globalminmax\") +\n  geom_line(size = 1) +\n  labs(title = \"Parallel Coordinate Plot of Cluster Centers by Variables\")\n\n\n\n\n\nHere we can observe that Cluster Analysis with only 4 centers merges two worst performing Clusters together, so we do not see underpaid players. I, therefore, suggest sticking with 5 clusters.\nThese are average values, so we might want to look at the individual data.\n\n\nCode\ndf5 = df3 %&gt;% \n  bind_cols(clusterdf) %&gt;% \n  rename(cluster = `k5$cluster`)\n\n\n\n\nCode\ndplyr::select(df5, -Hundreds) %&gt;% #MASS masks select\n  mutate_at(vars(\"cluster\"), as.factor) %&gt;% #needs to factor cluster\n  ggparcoord(columns = c(1:7),\n           groupColumn = \"cluster\",\n           scale = \"globalminmax\") +\n  geom_line(size = 0.5) +\n  scale_color_manual(values = c(\"red\", \"green\", \"blue\", \"orange\", \"black\")) +\n  labs(title = \"Parallel Coordinate Plot of Cluster Centers by Variables\")\n\n\n\n\n\n\n\nCode\ndplyr::select(df5, -Hundreds) %&gt;% #MASS masks select\n  mutate_at(vars(\"cluster\"), as.factor) %&gt;% \n  ggparcoord(columns = c(1:7),\n           groupColumn = \"cluster\",\n           scale = \"globalminmax\") +\n  geom_line(size = 0.5) +\n  scale_color_manual(values = c(\"red\", \"green\", \"blue\", \"orange\", \"black\")) +\n  facet_wrap(~ cluster) +\n  labs(title = \"Parallel Coordinate Plot of Cluster Centers by Variables\")\n\n\n\n\n\nThe lines intertwine in some places but generally stay within its cluster. Most intertwining happens in the Salary. There is also some between Clusters 1 and 2 in Avg and SR metrics.\n\n\nCode\nCricket_Data %&gt;% \n  bind_cols(as.factor(clusterdf$`k5$cluster`)) %&gt;% \n  rename(cluster = ...12) %&gt;% #`k5$cluster` is renamed into ...12\n  ggplot(aes(x=SR, y=Avg)) +\n  geom_text(aes(label = Player, \n                color = cluster), \n            size = 2) +\n  scale_color_discrete(name = \"Cluster\",\n                       labels = c(\"Average Player\", \n                                  \"Underpaid\", \n                                  \"Top Performers\", \n                                  \"Hundred Scorers\",\n                                  \"Overpaid\")) +\n  guides(color = guide_legend(override.aes = list(size = 5))) + #size=5 overrides the size of colored letters in legend\n  ggtitle(\"Plot of the Players' Avg vs SR clustered\")\n\n\nNew names:\n• `` -&gt; `...12`\n\n\n\n\n\nHere we can also observe separation of the key metrics by clusters and that Cluster Underpaid is closer to Average Player than Overpaid Cluster. In other words, it has higher SR and Avg than Overpaid clusters, but their salary does not reflect that."
  },
  {
    "objectID": "cluster_ipl.html#conclusion",
    "href": "cluster_ipl.html#conclusion",
    "title": "Cluster Analysis IPL",
    "section": "Conclusion",
    "text": "Conclusion\nI suggest reevaluating the pay of the players from Cluster 2. The Teams might also reconsider the salaries of the players from Cluster 5 who perform the worst but get paid the average rate. The cluster centers showed the average performance of the players from the same subgroups but a more individual evaluation should take place before the final decision."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Dengue Fever\n\nForecasting the total weekly cases of Dengue Fever using ARIMA and Neural Networks.\n\nHydropower\n\nPredicting Net Conventional Hydroelectric power generation, using popular forecasting techniques.\n\nHydropower 2\n\nForecasting Net Conventional Hydroelectric power generation, using neural networks."
  },
  {
    "objectID": "projects.html#forecasting",
    "href": "projects.html#forecasting",
    "title": "Projects",
    "section": "",
    "text": "Dengue Fever\n\nForecasting the total weekly cases of Dengue Fever using ARIMA and Neural Networks.\n\nHydropower\n\nPredicting Net Conventional Hydroelectric power generation, using popular forecasting techniques.\n\nHydropower 2\n\nForecasting Net Conventional Hydroelectric power generation, using neural networks."
  },
  {
    "objectID": "projects.html#market-research",
    "href": "projects.html#market-research",
    "title": "Projects",
    "section": "Market Research",
    "text": "Market Research\n\nCluster Analysis\n\nExploring the performance of IPL batsmen using analytical data tools.\n\nBoston Renters Sample\n\nThe tables illustrate the proportion of cost-burdened and severely cost-burdened households by race, size, and income groups in Boston."
  },
  {
    "objectID": "projects.html#machine-learning",
    "href": "projects.html#machine-learning",
    "title": "Projects",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nProcurement Contracts\n\nUsed various Machine Learning techniques to predict Procurement Contracts value."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dinmukhamed (Dimash) Alchinbayev",
    "section": "",
    "text": "BOSTON COLLEGE, Woods College of Advancing Studies | Boston, MA.\nM.S. Applied Economics; M.S. Applied Analytics | Jan. 2023 – May 2024\nSUFFOLK UNIVERSITY, College of Arts and Sciences | Boston, MA\nB.A. in Economics; B.A. in Politics, Philosophy, and Econ (Honors) | Sep.2018–May 2022\nGPA: 3.9"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Dinmukhamed (Dimash) Alchinbayev",
    "section": "",
    "text": "BOSTON COLLEGE, Woods College of Advancing Studies | Boston, MA.\nM.S. Applied Economics; M.S. Applied Analytics | Jan. 2023 – May 2024\nSUFFOLK UNIVERSITY, College of Arts and Sciences | Boston, MA\nB.A. in Economics; B.A. in Politics, Philosophy, and Econ (Honors) | Sep.2018–May 2022\nGPA: 3.9"
  },
  {
    "objectID": "index.html#related-courses",
    "href": "index.html#related-courses",
    "title": "Dinmukhamed (Dimash) Alchinbayev",
    "section": "RELATED COURSES",
    "text": "RELATED COURSES\nMachine Learning Algorithms; Econometrics; Market Research and Data Analysis;\nStatistics and Probability; Linear Algebra; Multivariable Calculus;\nPublic Finance; Forecasting; Politics and Data Analysis; Managerial Economics"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "Dinmukhamed (Dimash) Alchinbayev",
    "section": "WORK EXPERIENCE",
    "text": "WORK EXPERIENCE\nAsian Development Bank | Remote | Sep. 2022–Jan. 2023\nResearch Assistant\n\nWorked with the team of economists to identify key hypotheses and metrics to identify the impact of digital finance on agricultural enterprises.\nComposed exhaustive reports on the latest academic and business progress in digitalization of agricultural finance and its effects.\nWorked with and cleaned the WDI and FINDEX survey datasets; visualized the statistics, using Stata and Excel.\nFound relevant data from the official and secondary sources, such as central banks and business reports; cleaned and transformed the data, using Stata, R, and Excel for easier use and analysis.\nDid OLS regression and statistical analyses on the individual characteristics associated with borrowing, among agricultural cultivators using Stata.\nDelivered outcomes within tight deadlines.\n\nBeacon Hill Institute | Boston, MA | Nov. 2021–Aug. 2022\nData Analyst and Research Assistant\n\nCollaborated with the team of researchers to identify the key metrics needed for the research of the effects of taxes on businesses and government spending in Massachusetts.\nComposed literature reviews on the effects of the earmarked tax revenue on the government spending.\nAnalyzed the flow of earmarked taxes revenue across different government agencies and funds.\nFound, cleaned, and organized data from the Federal and State sources, using R and Stata.\nDid regression analysis and visualized earmark tax revenue and relevant spending data using R and Stata.\nPresented the key findings to the Directors of the BHI and stakeholders.\n\nQazindustry, Center of Industry and Export | Astana, Kazakhstan | Jun. 2019–Aug. 2019\nInternational Experience and Export Analyst Intern\n\nTranslated technical information about government subsidies into non-technical presentations for the small and medium domestic exporters.\nConducted interviews with multiple business owners over phone and in person and advised them corresponding to their needs.\nHelped organizing the Conference for the largest non-oil exporters in Astana.\nFound and analyzed successful cases of the international analogs of the government support of the exporters. Communicated key findings of the analysis to the stakeholders.\nDesigned a blueprint of a virtual assistant program that improved clients’ interaction with the service."
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Dinmukhamed (Dimash) Alchinbayev",
    "section": "SKILLS",
    "text": "SKILLS\nSoftware: R, Stata, Excel, PowerPoint, SQL, Python, Tableau\nHard: Regression, Logit, Splines, KNN, SVM, Regression Trees, Tuning, Clustering, Conjoint\nSoft: Analytical Thinking, Communication, Teamwork, Report Writing, Presenting"
  },
  {
    "objectID": "index.html#independent-research",
    "href": "index.html#independent-research",
    "title": "Dinmukhamed (Dimash) Alchinbayev",
    "section": "INDEPENDENT RESEARCH",
    "text": "INDEPENDENT RESEARCH\nApplied Machine Learning Project\nTrained multiple ML models in R (Ridge Regression, Splines, KNN, Regression Trees, Trees Bagging, and Support Vector Machines) to predict the value of Procurement contracts based on their available descriptions and corruption indices. Result: KNN (categorical) and Bagged Tree (continuous) are the most optimal models to predict Procurement Contract values.\nEconomics Thesis on Effectiveness of Economic Sanctions\nPerformed literature review; statistical data analysis; combined multiple databases (GSDB, World Bank, and Freedom House), and used a logistic regression model to find what factors lead to the increase in the probability of the success of sanctions. Result: Sanctions are more likely to succeed if they aim narrowly, ban trade completely, and are aimed towards politically non-free countries. Link\nSenior Thesis on Political Economy of Corruption in Russia and the Effects of Sanctions on it\nPerformed extensive literature review of Russian sources, data analysis, and application of existing philosophical paradigms to answer multidisciplinary question of the nature of corruption in Russia."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Pretty chill\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "NNETAR.html",
    "href": "NNETAR.html",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US using NNETAR",
    "section": "",
    "text": "This is a part 2 to the Hydropower Project I did before."
  },
  {
    "objectID": "NNETAR.html#data-preparation",
    "href": "NNETAR.html#data-preparation",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US using NNETAR",
    "section": "Data Preparation",
    "text": "Data Preparation\nI am importing my data on the total number of Conventional Hydropower Plants in the US and Monthly Precipitation.\n\n\nCode\nlibrary(readxl)\nplants &lt;- read_excel(\"EHA_Monthly_Net_Generation.xlsx\", \n    sheet = \"MonthlyNet\")\n\n\n\n\nCode\nlibrary(fpp3)\n\n\n── Attaching packages ────────────────────────────────────────────── fpp3 0.5 ──\n\n\n✔ tibble      3.1.8     ✔ tsibble     1.1.3\n✔ dplyr       1.1.0     ✔ tsibbledata 0.4.1\n✔ tidyr       1.2.0     ✔ feasts      0.3.1\n✔ lubridate   1.8.0     ✔ fable       0.3.3\n✔ ggplot2     3.3.5     ✔ fabletools  0.3.3\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\n\nCode\nplants %&gt;% \n  select(Year, Month, Net_Generation_MWh, EHA_PtID) -&gt; plants\n\n\n\n\nCode\nhead(plants)\n\n\n# A tibble: 6 × 4\n   Year Month Net_Generation_MWh EHA_PtID  \n  &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;     \n1  2003 APR                    0 hc0115_p01\n2  2003 AUG                    0 hc0115_p01\n3  2003 DEC                    0 hc0115_p01\n4  2003 FEB                    0 hc0115_p01\n5  2003 JAN                    0 hc0115_p01\n6  2003 JUL                  122 hc0115_p01\n\n\n\n\nCode\nlength(unique(plants$EHA_PtID))\n\n\n[1] 1519\n\n\n\n\nCode\nplants %&gt;%                              \n  group_by(Year, Month) %&gt;%\n  summarise(Count = n_distinct(EHA_PtID)) %&gt;% \n  mutate(Date = yearmonth(paste(Year, Month))) %&gt;% \n  arrange(Date) |&gt;\n  as_tsibble(index = Date) -&gt; plants2\n\n\n`summarise()` has grouped output by 'Year'. You can override using the\n`.groups` argument.\n\n\nCode\nplants2\n\n\n# A tsibble: 240 x 4 [1M]\n# Groups:    Year [20]\n    Year Month Count     Date\n   &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;    &lt;mth&gt;\n 1  2003 JAN    1386 2003 Jan\n 2  2003 FEB    1386 2003 Feb\n 3  2003 MAR    1386 2003 Mar\n 4  2003 APR    1386 2003 Apr\n 5  2003 MAY    1386 2003 May\n 6  2003 JUN    1386 2003 Jun\n 7  2003 JUL    1386 2003 Jul\n 8  2003 AUG    1386 2003 Aug\n 9  2003 SEP    1386 2003 Sep\n10  2003 OCT    1386 2003 Oct\n# … with 230 more rows\n\n\n\n\nCode\nplants2 |&gt;\n  autoplot(Count)\n\n\n\n\n\n\n\nCode\nlibrary(readr)\nprcpt &lt;- read_csv(\"Contiguous_us_Precipitation_data.csv\", \n    skip = 4)\n\n\nRows: 283 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): Date, Value, Anomaly\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nprcpt |&gt;\n  mutate(date = seq(as.Date(\"2000-01-01\"),\n                    as.Date(\"2023-07-01\"),\n                    by = \"month\"),\n         Date = yearmonth(date)) |&gt;\n  as_tsibble(index = Date) |&gt;\n  select(Date, Value) -&gt; prcpt2\nprcpt2\n\n\n# A tsibble: 283 x 2 [1M]\n       Date Value\n      &lt;mth&gt; &lt;dbl&gt;\n 1 2000 Jan  2.14\n 2 2000 Feb  2.12\n 3 2000 Mar  2.44\n 4 2000 Apr  2.32\n 5 2000 May  2.66\n 6 2000 Jun  3.45\n 7 2000 Jul  2.3 \n 8 2000 Aug  1.91\n 9 2000 Sep  2.18\n10 2000 Oct  2.26\n# … with 273 more rows\n\n\n\n\nCode\nprcpt2 |&gt;\n  autoplot() +\n  labs(title = \"Contiguous U.S. Precipitation\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Inches\",\n       x = \"Date\",\n       caption = \"Source: National Centers for Environmental Information\") +\n  theme_minimal()\n\n\nPlot variable not specified, automatically selected `.vars = Value`\n\n\n\n\n\n\n\nCode\nplants3 &lt;- read_csv(\"RectifHyd_v1.0.csv\", \n    skip = 27)\n\n\nRows: 324024 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): plant, state, month, EIA_obs_freq, RectifHyd_method, recommended_data\ndbl (6): EIA_ID, year, EIA_fraction, EIA_MWh, RectifHyd_fraction, RectifHyd_MWh\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nplants3 %&gt;%    \n  select(EIA_ID, plant, year, month) %&gt;% \n  group_by(year, month) %&gt;%\n  summarise(Count = n_distinct(plant), .groups = \"drop\") %&gt;% \n  mutate(Date = yearmonth(paste(year, month))) %&gt;% \n  arrange(Date) |&gt;\n  as_tsibble(index = Date) -&gt; plants3\nplants3\n\n\n# A tsibble: 240 x 4 [1M]\n    year month Count     Date\n   &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;    &lt;mth&gt;\n 1  2001 Jan    1337 2001 Jan\n 2  2001 Feb    1337 2001 Feb\n 3  2001 Mar    1337 2001 Mar\n 4  2001 Apr    1337 2001 Apr\n 5  2001 May    1337 2001 May\n 6  2001 Jun    1337 2001 Jun\n 7  2001 Jul    1337 2001 Jul\n 8  2001 Aug    1337 2001 Aug\n 9  2001 Sep    1337 2001 Sep\n10  2001 Oct    1337 2001 Oct\n# … with 230 more rows\n\n\n\n\nCode\nwhich(plants3$year==2003)[1]\n\n\n[1] 25\n\n\n\n\nCode\nplants3[1:24,] |&gt;\n  bind_rows(plants2) |&gt;\n  select(Count, Date) -&gt; plants_t\nplants_t\n\n\n# A tsibble: 264 x 2 [1M]\n   Count     Date\n   &lt;int&gt;    &lt;mth&gt;\n 1  1337 2001 Jan\n 2  1337 2001 Feb\n 3  1337 2001 Mar\n 4  1337 2001 Apr\n 5  1337 2001 May\n 6  1337 2001 Jun\n 7  1337 2001 Jul\n 8  1337 2001 Aug\n 9  1337 2001 Sep\n10  1337 2001 Oct\n# … with 254 more rows\n\n\n\n\nCode\nplants_t |&gt;\n  autoplot() +\n  labs(title = \"The Total Number of Operational Conventional\nHydropower Plants\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Plants\",\n       x = \"Date\",\n       caption = \"Source: Existing Hydropower Assets (EHA) Net\nGeneration Plant Database, 2003-2022; RectifHyd\") +\n  theme_minimal()\n\n\nPlot variable not specified, automatically selected `.vars = Count`\n\n\n\n\n\n\n\nCode\nhydro &lt;- read_csv(\"Net_generation_United_States_all_sectors_monthly.csv\", \n    skip = 4)\n\n\nRows: 268 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Month\ndbl (1): conventional hydroelectric thousand megawatthours\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nI am importing the data on Net Energy Generation.\n\n\nCode\nhydro = hydro %&gt;% \n  rename(MWH = \"conventional hydroelectric thousand megawatthours\")\n\n\n\n\nCode\nhydro=hydro[order(nrow(hydro):1),]\n\n\n\n\nCode\nhydro = hydro %&gt;% \n  mutate(Date = yearmonth(Month)) |&gt;\n  as_tsibble(index = Date)\n\n\n\n\nCode\nhydro %&gt;% \n  autoplot(MWH) +\n  labs(title = \"Net Conventional Hydroelectric Power Generation\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\nhydro |&gt;\n  left_join(plants_t) |&gt;\n  select(Date, MWH, Count) |&gt;\n  left_join(prcpt2) |&gt;\n  mutate(Count = ifelse(is.na(Count),\n                        1351,\n                        Count)) |&gt; #inputting missing 4 obs with the last ob\n  rename(Precipitation = Value) -&gt; hydro_comp\n\n\nJoining with `by = join_by(Date)`\nJoining with `by = join_by(Date)`\n\n\nCode\nhead(hydro_comp)\n\n\n# A tsibble: 6 x 4 [1M]\n      Date    MWH Count Precipitation\n     &lt;mth&gt;  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 2001 Jan 18852.  1337          1.85\n2 2001 Feb 17473.  1337          2.21\n3 2001 Mar 20477.  1337          2.57\n4 2001 Apr 18013.  1337          2.1 \n5 2001 May 19176.  1337          2.78\n6 2001 Jun 20728.  1337          3.12\n\n\nI am combining all the data into one tsibble.\n\n\nCode\nhydro_comp |&gt;\n  pivot_longer(c(MWH, Count, Precipitation)) |&gt;\n  ggplot(aes(x = Date, y = value)) +\n  geom_line() +\n  facet_grid(name ~ ., scales = \"free_y\") + ylab(\"\")\n\n\n\n\n\n\n\nCode\nhydro |&gt; gg_tsdisplay(MWH,\n                     plot_type='partial', lag_max = 24)\n\n\n\n\n\n\n\nCode\nhydro |&gt;\n  model(stl = STL(MWH)) |&gt;\n  components() |&gt; \n  gg_subseries(season_year) +\n  theme(axis.text.x = element_text(size = 5))\n\n\n\n\n\n\n\nCode\nhydro |&gt;\n  model(stl = STL(MWH)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\nWarning: Ignoring unknown parameters: linewidth\n\n\n\n\n\nSTL decomposition shows high seasonality of the data."
  },
  {
    "objectID": "NNETAR.html#preliminary-model-estimation",
    "href": "NNETAR.html#preliminary-model-estimation",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US using NNETAR",
    "section": "Preliminary Model Estimation",
    "text": "Preliminary Model Estimation\n\n\nCode\ntotal_obs.hydro = dim(hydro_comp)[1] #puts n of obs into total_obs\ntrain_obs = total_obs.hydro * 0.8\ntest_obs = total_obs.hydro - train_obs\nhydro_train = head(hydro_comp, train_obs)\nhydro_test = tail(hydro_comp, test_obs)\n\n\n\n\nCode\nhydro_train |&gt;\n  pivot_longer(c(MWH, Count, Precipitation)) |&gt;\n  ggplot(aes(x = Date, y = value)) +\n  geom_line() +\n  facet_grid(name ~ ., scales = \"free_y\") + ylab(\"\")\n\n\n\n\n\n\n\nCode\nnn_model = hydro_train |&gt;\n  model(nnetar = NNETAR(MWH ~ Count + Precipitation),\n        Arima.reg = ARIMA(MWH ~ Count + Precipitation))\n\n\n\n\nCode\nnn_model\n\n\n# A mable: 1 x 2\n             nnetar                              Arima.reg\n            &lt;model&gt;                                &lt;model&gt;\n1 &lt;NNAR(1,1,2)[12]&gt; &lt;LM w/ ARIMA(1,0,0)(1,1,0)[12] errors&gt;\n\n\nA mable: 1 x 2 nnetar Arima.reg   1 &lt;NNAR(1,1,2)[12]&gt; &lt;LM w/ ARIMA(1,0,0)(1,1,0)[12] errors&gt;\n\n\nCode\nhydro_train[214,\"Count\"]\n\n\n# A tibble: 1 × 1\n  Count\n  &lt;dbl&gt;\n1  1440\n\n\nCode\nmean_prcpt = mean(hydro_train$Precipitation)\n\nhydro_train2 = hydro_train[-214,]\n\n\n\n\nCode\nf_scenarios_hydro &lt;- scenarios(\n  Mean = new_data(hydro_train, 54) |&gt;\n    mutate(Count = 1440,\n           Precipitation = mean_prcpt))\n\n\n\n\nCode\nstart_time = Sys.time()\n\nnn_fit = nn_model |&gt;\n  forecast(new_data = f_scenarios_hydro)\n\nprint(Sys.time() - start_time)\n\n\nTime difference of 3.74252 mins\n\n\n\n\nCode\nhydro_train |&gt;\n  autoplot(MWH) +\n  autolayer(nn_fit) +\n  labs(title = \"Net Conventional Hydroelectric Power Generation\nNN Forecast\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\nhydro_test |&gt;\n  autoplot(MWH) +\n  autolayer(nn_fit) +\n  geom_line(aes(y = MWH)) +\n  labs(title = \"Net Conventional Hydroelectric Power Generation\nNN Forecast\",\n       subtitle = \"Nov 2018 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\nnn_fit |&gt;\n  accuracy(hydro_comp) |&gt;\n  select(.model, RMSE, MPE)\n\n\n# A tibble: 2 × 3\n  .model     RMSE    MPE\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 Arima.reg 3688. -11.4 \n2 nnetar    2929.  -2.86\n\n\nNeural Networks predict better."
  },
  {
    "objectID": "NNETAR.html#comparing-nnetar-arima-and-ets",
    "href": "NNETAR.html#comparing-nnetar-arima-and-ets",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US using NNETAR",
    "section": "Comparing NNETAR, ARIMA, and ETS",
    "text": "Comparing NNETAR, ARIMA, and ETS\nFor convenience I am using only the last five years of observation.\n\n\nCode\nhydro_train2 = hydro_comp[208:256,]\nhydro_test2 = tail(hydro_comp, 12)\nhydro_comp2 = hydro_comp[208:268,]\n\n\n\n\nCode\nlagged_arima &lt;- hydro_train2 |&gt;\n  # Restrict data so models use same fitting period\n  mutate(MWH = c(NA, NA, NA, NA, NA, MWH[6:49])) |&gt;\n  model(\n    lag1 = ARIMA(MWH ~ pdq(d = 0)\n                 + lag(Precipitation)\n                 + lag(Count)),\n    lag2 = ARIMA(MWH ~ pdq(d = 0) + lag(Precipitation) +\n                 lag(Precipitation, 2) + lag(Count) + lag(Count, 2)),\n    lag3 = ARIMA(MWH ~ pdq(d = 0) + lag(Precipitation) +\n                 lag(Precipitation, 2) + lag(Precipitation, 3) + lag(Count) + lag(Count, 2) + lag(Count, 3)),\n    lag4 = ARIMA(MWH ~ pdq(d = 0) + lag(Precipitation) +\n                 lag(Precipitation, 2) + lag(Precipitation, 3) + lag(Precipitation, 4) + lag(Count) + lag(Count, 2) + lag(Count, 3) + lag(Count, 4)),\n    lag5 = ARIMA(MWH ~ pdq(d = 0) + lag(Precipitation) +\n                 lag(Precipitation, 2) + lag(Precipitation, 3) + lag(Precipitation, 4) + lag(Precipitation, 5) + lag(Count) + lag(Count, 2) + lag(Count, 3) + lag(Count, 4) + lag(Count, 5))\n  )\n\nglance(lagged_arima)\n\n\n# A tibble: 5 × 8\n  .model   sigma2 log_lik   AIC  AICc   BIC ar_roots  ma_roots \n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;    &lt;list&gt;   \n1 lag1   3085088.   -286.  579.  581.  586. &lt;cpl [0]&gt; &lt;cpl [1]&gt;\n2 lag2   3207038.   -285.  583.  585.  592. &lt;cpl [0]&gt; &lt;cpl [1]&gt;\n3 lag3   3136314.   -284.  584.  589.  597. &lt;cpl [0]&gt; &lt;cpl [1]&gt;\n4 lag4   2900213.   -282.  583.  592.  599. &lt;cpl [0]&gt; &lt;cpl [1]&gt;\n5 lag5   2973819.   -281.  585.  598.  605. &lt;cpl [0]&gt; &lt;cpl [1]&gt;\n\n\nBased on AICc, I am choosing lag1.\n\n\nCode\nnn_model2 = hydro_train2 |&gt;\n  model(nnetar2 = NNETAR(MWH ~ Count + Precipitation),\n        Arima.reg2 = ARIMA(MWH ~ lag(Count) + lag(Precipitation),\n                           stepwise = FALSE,\n                           approx = FALSE),\n        ETS = ETS(MWH))\n\n\n\n\nCode\nlagged_count = hydro_train2$Count[38:49]\nlagged_prcpt = hydro_train2$Precipitation[38:49]\n#I am creating a list of the last 12 values from the hydro_train2.\nmean_count = rep(mean(mean(hydro_train2$Count)), times=12)\nmean_prcpt = rep(mean(mean(hydro_train2$Precipitation)), times=12)\n#I am creating a list of the 12 repeating values equaling the average observation from the training set.\n\nfuture_scenarios &lt;- scenarios(\n  Lagged = new_data(hydro_train2, 12) |&gt;\n    mutate(Count=lagged_count,\n           Precipitation = lagged_prcpt),\n  Mean = new_data(hydro_train2, 12) |&gt;\n    mutate(Count = mean_count,\n           Precipitation = mean_prcpt),\n  names_to = \"Forecast Scenarios\")\n#By putting lagged_vars, I am using the last 12 values of hydro_train2 as the values in the future_scenarios. This way I am creating new values lagged at 12: April 2023 will equal April 2022, etc. Thus, I can use them as the predictors for my forecast. The same is with the Mean values.\n\n\n\n\nCode\nstart_time = Sys.time()\n\nnn_fit2 = nn_model2 |&gt;\n  forecast(new_data = future_scenarios)\n\nprint(Sys.time() - start_time)\n\n\nTime difference of 2.633055 mins\n\n\n\n\nCode\nhydro_test2 = hydro_test2[,c(\"Date\",\"MWH\")]\n\n\n\n\nCode\nhead(hilo(nn_fit2))\n\n\n# A tibble: 6 × 9\n  `Forecast Scenarios` .model      Date          MWH  .mean Count Precipitation\n  &lt;chr&gt;                &lt;chr&gt;      &lt;mth&gt;       &lt;dist&gt;  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 Lagged               nnetar2 2022 May sample[5000] 21545.  1276          2.98\n2 Lagged               nnetar2 2022 Jun sample[5000] 21850.  1276          2.97\n3 Lagged               nnetar2 2022 Jul sample[5000] 21306.  1276          3.41\n4 Lagged               nnetar2 2022 Aug sample[5000] 20481.  1276          3.14\n5 Lagged               nnetar2 2022 Sep sample[5000] 17878.  1276          2.39\n6 Lagged               nnetar2 2022 Oct sample[5000] 17819.  1276          3.15\n# … with 2 more variables: `80%` &lt;hilo&gt;, `95%` &lt;hilo&gt;\n\n\nHere is the plot of the training data and the forecast.\n\n\nCode\nhydro_train2 |&gt;\n  autoplot(MWH) +\n  autolayer(nn_fit2, level = NULL)\n\n\n\n\n\nHere is the plot of the actual data and the forecast.\n\n\nCode\nhydro_test2 |&gt;\n  autoplot(MWH) +\n  autolayer(nn_fit2, level = NULL)\n\n\n\n\n\n\n\nCode\nnn_fit2 %&gt;% \n  filter(`Forecast Scenarios`==\"Lagged\") |&gt;\n  accuracy(hydro_comp2) |&gt;\n  select(.model, RMSE, MPE)\n\n\n# A tibble: 3 × 3\n  .model      RMSE   MPE\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 Arima.reg2 2585. -5.78\n2 ETS        2181.  4.16\n3 nnetar2    2430. -2.24\n\n\n\n\nCode\nnn_fit2 %&gt;% \n  filter(`Forecast Scenarios`==\"Mean\") |&gt;\n  accuracy(hydro_comp2) |&gt;\n  select(.model, RMSE, MPE)\n\n\n# A tibble: 3 × 3\n  .model      RMSE    MPE\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1 Arima.reg2 3267. -12.9 \n2 ETS        2181.   4.16\n3 nnetar2    2270.  -4.33\n\n\nWe can see that ETS has the lowest RMSE, while NNETAR with external regressors lagged at 12 has the lowest MPE.\nNow I creating an ensemble model that averages the predictions.\n\n\nCode\nstart_time = Sys.time()\n\nnn_ensemble = hydro_train2 |&gt;\n  model(Ensemble = (NNETAR(MWH ~ Count + Precipitation) +\n                     ARIMA(MWH ~ lag(Count) + lag(Precipitation),\n                           stepwise = FALSE,\n                           approx = FALSE) +\n                     ETS(MWH))/3)\n\nprint(Sys.time() - start_time)\n\n\nTime difference of 6.888192 secs\n\n\n\n\nCode\nnn_fit2.nsmbl = nn_ensemble |&gt;\n  forecast(new_data = future_scenarios)\n\n\n\n\nCode\nhydro_test2 |&gt;\n  autoplot(MWH) +\n  autolayer(nn_fit2.nsmbl, level = NULL)\n\n\n\n\n\n\n\nCode\nnn_fit2.nsmbl %&gt;% \n  filter(`Forecast Scenarios`==\"Mean\") |&gt;\n  accuracy(hydro_comp2) |&gt;\n  select(.model, RMSE, MPE)\n\n\n# A tibble: 1 × 3\n  .model    RMSE   MPE\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 Ensemble 2017. -4.57\n\n\nThe average prediction, is the best one, in terms of variance (RMSE). However, NNETAR is still better in terms of bias (MPE)."
  },
  {
    "objectID": "rents.html",
    "href": "rents.html",
    "title": "Boston Renters",
    "section": "",
    "text": "Affordable rental housing or income-restricted rental housing usually means a dwelling unit for a lease that is capped for qualified households. Usually, they are reserved for people earning less than 80%, 50%, or 30% of Area Median Income (AMI). Boston already has one of the highest shares of income restricted housing per total units in the country. According to City of Boston, around 27% of all rental units are income restricted and 95% of all income restricted units are rental. Nevertheless, CHMA reports that for the next three years, the demand is estimated to be 34,675 new rental units, with only 16,450 units underway, facilitating shortage and further rent increase. Not to mention the record costs of building in Boston and limited space.\nBelow are the illustrations of the sample dimensions, calculated using American Community Survey public use samples (2021).\nThe tables illustrate the proportion of cost-burdened and severely cost-burdened households by race, size, and income groups in Boston. We can see that the distribution of different races, for example, is different from general population. According to the Boston Neighborhoods Demographics, in Boston overall, there are 44% of White and 22.7% of Black population, in contrast of around 36% and 18% in the target population, respectively. Not proportionate sample might introduce bias by overrepresenting and underrepresenting different groups. We can also observe that 9.6% do qualify for 80% AMI housing, even though they are cost burdened."
  },
  {
    "objectID": "rents.html#abstract",
    "href": "rents.html#abstract",
    "title": "Boston Renters",
    "section": "",
    "text": "Affordable rental housing or income-restricted rental housing usually means a dwelling unit for a lease that is capped for qualified households. Usually, they are reserved for people earning less than 80%, 50%, or 30% of Area Median Income (AMI). Boston already has one of the highest shares of income restricted housing per total units in the country. According to City of Boston, around 27% of all rental units are income restricted and 95% of all income restricted units are rental. Nevertheless, CHMA reports that for the next three years, the demand is estimated to be 34,675 new rental units, with only 16,450 units underway, facilitating shortage and further rent increase. Not to mention the record costs of building in Boston and limited space.\nBelow are the illustrations of the sample dimensions, calculated using American Community Survey public use samples (2021).\nThe tables illustrate the proportion of cost-burdened and severely cost-burdened households by race, size, and income groups in Boston. We can see that the distribution of different races, for example, is different from general population. According to the Boston Neighborhoods Demographics, in Boston overall, there are 44% of White and 22.7% of Black population, in contrast of around 36% and 18% in the target population, respectively. Not proportionate sample might introduce bias by overrepresenting and underrepresenting different groups. We can also observe that 9.6% do qualify for 80% AMI housing, even though they are cost burdened."
  },
  {
    "objectID": "rents.html#data-preparation",
    "href": "rents.html#data-preparation",
    "title": "Boston Renters",
    "section": "Data Preparation",
    "text": "Data Preparation\n\n\nCode\nlibrary(readr)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.5     ✔ dplyr   1.1.0\n✔ tibble  3.1.8     ✔ stringr 1.4.0\n✔ tidyr   1.2.0     ✔ forcats 0.5.1\n✔ purrr   1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nrents &lt;- read_csv(\"usa_00003.csv\")\n\n\nRows: 6605 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (17): YEAR, SAMPLE, SERIAL, CBSERIAL, HHWT, CLUSTER, CITY, STRATA, GQ, R...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nrents = rents %&gt;% \n  dplyr::mutate(rentyearly = RENTGRS*12) %&gt;% \n  dplyr::mutate(rentratio = rentyearly/HHINCOME)\n\n\n\n\nCode\ncount(rents, rentratio==0)\n\n\n# A tibble: 3 × 2\n  `rentratio == 0`     n\n  &lt;lgl&gt;            &lt;int&gt;\n1 FALSE             3439\n2 TRUE              3145\n3 NA                  21\n\n\n\n\nCode\nwhich(is.na(rents$rentratio))\n\n\n [1] 1266 1519 1527 1765 1766 1998 2019 2020 2827 2873 2890 3990 4037 4373 5229\n[16] 5739 5740 6122 6138 6139 6140\n\n\n\n\nCode\nrents[1266,\"rentratio\"]\n\n\n# A tibble: 1 × 1\n  rentratio\n      &lt;dbl&gt;\n1       NaN\n\n\n\n\nCode\nsum(is.na(rents))\n\n\n[1] 21\n\n\n\n\nCode\nrents = na.omit(rents)\n\n\n\n\nCode\nrents2 = rents %&gt;% \n  filter(rentratio!=0)\n\n\n\n\nCode\nplot(density(rents2$HHINCOME))\n\n\n\n\n\n\n\nCode\nplot(density(rents2$rentratio))\n\n\n\n\n\n\n\nCode\ncount(rents2, rentratio&gt;=0.3)\n\n\n# A tibble: 2 × 2\n  `rentratio &gt;= 0.3`     n\n  &lt;lgl&gt;              &lt;int&gt;\n1 FALSE               1781\n2 TRUE                1658\n\n\n\n\nCode\ncount(rents2, rentratio&gt;=0.5)\n\n\n# A tibble: 2 × 2\n  `rentratio &gt;= 0.5`     n\n  &lt;lgl&gt;              &lt;int&gt;\n1 FALSE               2526\n2 TRUE                 913\n\n\n\n\nCode\nrents2 = rents2 %&gt;% \n  dplyr::mutate(costbur = ifelse(rentratio&lt;0.3,\"Not CB\",\n                        ifelse(0.3&lt;=rentratio&rentratio&lt;0.5,\"CB\", \"Severely CB\")))\n\n\n\n\nCode\nrents2 = rents2 %&gt;% \n  filter(HHINCOME!=0)\n\n\n\n\nCode\nunique(rents2$RACE)\n\n\n[1] 1 2 7 8 6 4 9 5 3\n\n\n\n\nCode\nrace_df = data.frame(RACE = c(1,2,3,4,5,6,7,8,9),\n                        LRACE = c(\"White\", \n               \"Black\",\n               \"Native\",\n               \"Chinese\",\n               \"Japanese\",\n               \"Other Asian\",\n               \"Other\",\n               \"Two Races\",\n               \"Three Races\"))\n\nrents3 = merge(rents2, race_df, by = \"RACE\", all = FALSE)\n\n\n\n\nCode\nrents3 = rents3 %&gt;% \n  mutate(LRACE = ifelse(RACE==7,\"Hispanic\",LRACE)) %&gt;% \n  mutate(LRACE = ifelse(LRACE==\"Chinese\"|LRACE==\"Japanese\"|LRACE==\"Other Asian\",\"Asian\",LRACE)) %&gt;%\n  rename(Race=LRACE) %&gt;% \n  group_by(SERIAL) %&gt;% \n  mutate(hhsize = max(PERNUM))%&gt;% \n  mutate(ami = ifelse(hhsize==1&HHINCOME&lt;=78550,1,\n               ifelse(hhsize==2&HHINCOME&lt;=89750,1,\n               ifelse(hhsize==3&HHINCOME&lt;=100950,1,\n               ifelse(hhsize==4&HHINCOME&lt;=112150,1,\n               ifelse(hhsize&gt;=5&HHINCOME&lt;=130100,1,\n                      0)))))) %&gt;% \n  ungroup()\n\n\n\n\nCode\nrents3 %&gt;% \n  filter(costbur!=\"Not CB\") %&gt;% \n  filter(ami!=0) %&gt;% \n  count()\n\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  1400"
  },
  {
    "objectID": "rents.html#tables",
    "href": "rents.html#tables",
    "title": "Boston Renters",
    "section": "Tables",
    "text": "Tables\n\n\nCode\nrents3 %&gt;% \n  filter(ami!=0) %&gt;% \n  filter(costbur!=\"Not CB\") %&gt;%\n  mutate_at(vars(\"Race\"), as.factor) %&gt;% \n  group_by(Race, costbur) %&gt;% \n  summarize(Count = n()) %&gt;%\n  group_by(costbur) %&gt;% \n  mutate(Percent = Count/1400*100) %&gt;% \n  ungroup() %&gt;%\n  mutate(costbur = factor(costbur, levels = c(\"Not CB\",\n                                              \"Severely CB\",\n                                              \"CB\"))) %&gt;%\n  ggplot(aes(x=reorder(Race, desc(Percent)), \n             y=Percent, \n             fill=costbur)) + \n  scale_fill_manual(values = c(\"steelblue\",\n                               \"blue\"),\n                    name = element_blank()) +\n  geom_bar(stat='identity', position= \"stack\") +\n  geom_text(aes(label = round(Percent, 2)),\n            size = 2,\n            position = position_stack(vjust = 0.5),\n            color = \"white\") +\n  theme_minimal() +\n  xlab(element_blank()) +\n  labs(title = \"Cost Burdened Renter Households by Race in Boston\",\n       subtitle = \"Excluding those who do not qualify by 80% AMI\",\n       caption = \"Source: IPUMS USA (2021 ACS), Own Calculations\n       Note: Shares of cost burdened households were derived by deviding annual rent by income\") +\n  theme(legend.position = c(0.85,0.9),\n        title = element_text(size = 10))\n\n\n`summarise()` has grouped output by 'Race'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\nCode\nrents3 %&gt;% \n  group_by(SERIAL) %&gt;% \n  distinct(SERIAL, hhsize, costbur, ami) %&gt;% \n  filter(costbur!=\"Not CB\" & ami!=0)\n\n\n# A tibble: 713 × 4\n# Groups:   SERIAL [713]\n   SERIAL hhsize costbur       ami\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 641897      1 Severely CB     1\n 2 636155      6 Severely CB     1\n 3 633351      2 CB              1\n 4 642035      1 Severely CB     1\n 5 633475      2 Severely CB     1\n 6 642054      2 CB              1\n 7 623095      3 Severely CB     1\n 8 633640      3 Severely CB     1\n 9 623115      2 Severely CB     1\n10 642183      1 CB              1\n# … with 703 more rows\n\n\n\n\nCode\nrents3 %&gt;% \n  group_by(SERIAL) %&gt;% \n  dplyr::select(SERIAL,PERNUM,hhsize,HHINCOME,rentratio,costbur,Race,ami) %&gt;% \n  distinct(SERIAL, hhsize, HHINCOME, rentratio, costbur, ami) %&gt;% \n  mutate(hhsize = ifelse(hhsize&gt;=5, 6, hhsize)) %&gt;%\n  dplyr::mutate_at(vars(hhsize), ~ as.character(.)) %&gt;%\n  mutate(hhsize = ifelse(hhsize==\"6\", \"5+\", hhsize)) %&gt;% \n  filter(ami!=0) %&gt;% \n  filter(costbur!=\"Not CB\") %&gt;% \n  group_by(hhsize, costbur) %&gt;% \n  summarize(Count = n()) %&gt;%\n  group_by(costbur) %&gt;% \n  mutate(Percent = Count/713*100) %&gt;% \n  ungroup() %&gt;%\n  mutate(costbur = factor(costbur, levels = c(\"Not CB\",\n                                              \"Severely CB\",\n                                              \"CB\"))) %&gt;%\n  ggplot(aes(x=reorder(hhsize, desc(Percent)), \n             y=Percent, \n             fill=costbur)) + \n  scale_fill_manual(values = c(\"steelblue\",\n                               \"blue\"),\n                    name = element_blank()) +\n  geom_bar(stat='identity', position= \"stack\") +\n  geom_text(aes(label = round(Percent, 2)),\n            size = 2,\n            position = position_stack(vjust = 0.5),\n            color = \"white\") +\n  theme_minimal() +\n  xlab(\"Number of Household Members\") +\n  labs(title = \"Cost Burdened Renter Households by Size in Boston\",\n       subtitle = \"Excluding those who do not qualify by 80% AMI\",\n       caption = \"Source: IPUMS USA (2021 ACS), Own Calculations\") +\n  theme(legend.position = c(0.85,0.9),\n        title = element_text(size = 10))\n\n\n`summarise()` has grouped output by 'hhsize'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\nCode\nsum(!is.na(unique(rents3$SERIAL)))\n\n\n[1] 1576\n\n\n\n\nCode\nrents4 = rents3 %&gt;% \n  mutate(ami30 = ifelse(hhsize==1&HHINCOME&lt;=29450,1,\n               ifelse(hhsize==2&HHINCOME&lt;=33650,1,\n               ifelse(hhsize==3&HHINCOME&lt;=37850,1,\n               ifelse(hhsize==4&HHINCOME&lt;=42050,1,\n               ifelse(hhsize&gt;=5&HHINCOME&lt;=48800,1,\n                      0)))))) %&gt;%\n  mutate(ami50 = ifelse(hhsize==1&HHINCOME&lt;=49100,1,\n               ifelse(hhsize==2&HHINCOME&lt;=56100,1,\n               ifelse(hhsize==3&HHINCOME&lt;=63100,1,\n               ifelse(hhsize==4&HHINCOME&lt;=70100,1,\n               ifelse(hhsize&gt;=5&HHINCOME&lt;=81350,1,\n                      0)))))) %&gt;% \n  mutate(ami.cat = ifelse(ami30==1,\"30%&lt;\",\n                   ifelse(ami50==1,\"50%&lt;\",\n                   ifelse(ami==1,\"80%&lt;\",\n                          \"80%&gt;\"))))\n\n\n\n\nCode\nrents4 %&gt;% \n  group_by(SERIAL) %&gt;% \n  distinct(SERIAL,ami.cat,costbur) %&gt;% \n  filter(costbur!=\"Not CB\") %&gt;% \n  group_by(ami.cat, costbur) %&gt;% \n  summarize(Count = n()) %&gt;%\n  group_by(costbur) %&gt;% \n  mutate(Percent = Count/796*100) %&gt;% \n  ungroup() %&gt;%\n  mutate(costbur = factor(costbur, levels = c(\"Not CB\",\n                                              \"Severely CB\",\n                                              \"CB\"))) %&gt;%\n  ggplot(aes(x=reorder(ami.cat, desc(Percent)), \n             y=Percent, \n             fill=costbur)) + \n  scale_fill_manual(values = c(\"steelblue\",\n                               \"blue\"),\n                    name = element_blank()) +\n  geom_bar(stat='identity', position= \"stack\") +\n  geom_text(aes(label = round(Percent, 2)),\n            size = 2,\n            position = position_stack(vjust = 0.5),\n            color = \"white\") +\n  theme_minimal() +\n  xlab(\"Most Common AMI thresholds\") +\n  labs(title = \"Cost Burdened Renter Households by AMI % category\nin Boston\",\n       subtitle = \"Household Income as % of AMI\",\n       caption = \"Source: IPUMS USA (2021 ACS), Own Calculations\nNote: Percentages of the households are on the right side\") +\n  theme(legend.position = c(0.85,0.9),\n        title = element_text(size = 10))\n\n\n`summarise()` has grouped output by 'ami.cat'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "Dengue_fc.html",
    "href": "Dengue_fc.html",
    "title": "Dengue Fever",
    "section": "",
    "text": "Dengue fever is a virus that is spread to people through the bite of an infected mosquito. A severe dengue fever case can lead to circulatory system failure, shock, and even death. Almost half of the world’s population, about 4 billion people, live in areas with a risk of dengue. Studying and forecasting the total number of infection incidents can help to tame this disease.\nI am using the fpp3 package to forecast the next total weekly cases of Dengue fever two cities in Peru and Puerto Rico.\nThe dataset is from drivendata.org competition.\n\n\nCode\nlibrary(fpp3)\n\n\n── Attaching packages ────────────────────────────────────────────── fpp3 0.5 ──\n\n\n✔ tibble      3.1.8     ✔ tsibble     1.1.3\n✔ dplyr       1.1.0     ✔ tsibbledata 0.4.1\n✔ tidyr       1.2.0     ✔ feasts      0.3.1\n✔ lubridate   1.8.0     ✔ fable       0.3.3\n✔ ggplot2     3.3.5     ✔ fabletools  0.3.3\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\n\nI am importing and combining the data on the number of cases and 20 possible predictor variables.\n\n\nCode\nlibrary(readr)\ndengue_fts &lt;- read_csv(\"DengAI_Predicting_Disease_Spread_-_Training_Data_Features.csv\")\n\n\nRows: 1456 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): city\ndbl  (22): year, weekofyear, ndvi_ne, ndvi_nw, ndvi_se, ndvi_sw, precipitati...\ndate  (1): week_start_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\ndengue_label &lt;- read_csv(\"DengAI_Predicting_Disease_Spread_-_Training_Data_Labels.csv\")\n\n\nRows: 1456 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): city\ndbl (3): year, weekofyear, total_cases\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\ndengue_label |&gt;\n  left_join(dengue_fts,\n            by = c(\"city\",\n                   \"year\",\n                   \"weekofyear\")) -&gt; dengue\n\n\nI am performing a quick random test to see if everything matches up.\n\n\nCode\ndengue_fts$reanalysis_dew_point_temp_k[256]==dengue$reanalysis_dew_point_temp_k[256]\n\n\n[1] TRUE\n\n\nI am transforming the combined dataset into a tsibble, the format for fpp3 package. The newly created year_week is an index variable and the two cities are keys to treat their observations as separate time series.\n\n\nCode\ndengue |&gt;\n  mutate(year_week = yearweek(week_start_date)) |&gt;\n  as_tsibble(key = city,\n             index = year_week) |&gt;\n  arrange(desc(city)) -&gt; dengue_ts\n\n\nHere is the plot fro both cities.\n\n\nCode\ndengue_ts |&gt;\n  #filter(city==\"sj\") |&gt;\n  autoplot(total_cases) +\n  labs(title = \"Total Cases of Dengue fever in San Juan and Iquitos\",\n       subtitle = \"April 1990 - June 2010, Weekly\",\n       y = \"Total Cases\",\n       x = \"Date\",\n       caption = \"Source: U.S. Centers for Disease Control and prevention\") +\n  theme_minimal()\n\n\n\n\n\nWe can see that the data for San Juan and Iquitos span different time periods but still overlap. We can also observe that most of the time the total number of cases is less than 50, however there are numerous spikes.\n\n\nCode\nplot(density(dengue_ts$total_cases))\nabline(v = 50, col = \"red\", lty = \"dashed\")\n\n\n\n\n\n\n\nCode\ndengue_ts |&gt;\n  filter(city==\"sj\") |&gt;\n  tail(1)\n\n\n# A tsibble: 1 x 26 [1W]\n# Key:       city [1]\n  city   year weekofyear total_cases week_start_date ndvi_ne ndvi_nw ndvi_se\n  &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 sj     2008         17           5 2008-04-22       -0.037 -0.0104  0.0773\n# … with 18 more variables: ndvi_sw &lt;dbl&gt;, precipitation_amt_mm &lt;dbl&gt;,\n#   reanalysis_air_temp_k &lt;dbl&gt;, reanalysis_avg_temp_k &lt;dbl&gt;,\n#   reanalysis_dew_point_temp_k &lt;dbl&gt;, reanalysis_max_air_temp_k &lt;dbl&gt;,\n#   reanalysis_min_air_temp_k &lt;dbl&gt;, reanalysis_precip_amt_kg_per_m2 &lt;dbl&gt;,\n#   reanalysis_relative_humidity_percent &lt;dbl&gt;,\n#   reanalysis_sat_precip_amt_mm &lt;dbl&gt;,\n#   reanalysis_specific_humidity_g_per_kg &lt;dbl&gt;, reanalysis_tdtr_k &lt;dbl&gt;, …\n\n\nCode\ndengue_ts |&gt;\n  filter(city==\"iq\") |&gt;\n  tail(1)\n\n\n# A tsibble: 1 x 26 [1W]\n# Key:       city [1]\n  city   year weekofyear total_cases week_start_date ndvi_ne ndvi_nw ndvi_se\n  &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 iq     2010         25           4 2010-06-25        0.298   0.233   0.274\n# … with 18 more variables: ndvi_sw &lt;dbl&gt;, precipitation_amt_mm &lt;dbl&gt;,\n#   reanalysis_air_temp_k &lt;dbl&gt;, reanalysis_avg_temp_k &lt;dbl&gt;,\n#   reanalysis_dew_point_temp_k &lt;dbl&gt;, reanalysis_max_air_temp_k &lt;dbl&gt;,\n#   reanalysis_min_air_temp_k &lt;dbl&gt;, reanalysis_precip_amt_kg_per_m2 &lt;dbl&gt;,\n#   reanalysis_relative_humidity_percent &lt;dbl&gt;,\n#   reanalysis_sat_precip_amt_mm &lt;dbl&gt;,\n#   reanalysis_specific_humidity_g_per_kg &lt;dbl&gt;, reanalysis_tdtr_k &lt;dbl&gt;, …\n\n\nMost of the models cannot work with gaps in time, so I am checking for explicit gaps.\n\n\nCode\nhas_gaps(dengue_ts, .full = T)\n\n\n# A tibble: 2 × 2\n  city  .gaps\n  &lt;chr&gt; &lt;lgl&gt;\n1 iq    TRUE \n2 sj    TRUE \n\n\nBoth cities have gaps, so I am filling gaps and using the previous value to fill them.\n\n\nCode\nlibrary(zoo)\n\n\n\nAttaching package: 'zoo'\n\n\nThe following object is masked from 'package:tsibble':\n\n    index\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\n\nCode\ndengue_ts |&gt;\n  fill_gaps() %&gt;%\n  mutate_all( ~ na.locf(.x, na.rm = FALSE)) -&gt; dengue_ts_c\n\n\nI am checking if there are any missing values left.\n\n\nCode\ncolSums(is.na(dengue_ts_c))\n\n\n                                 city                                  year \n                                    0                                     0 \n                           weekofyear                           total_cases \n                                    0                                     0 \n                      week_start_date                               ndvi_ne \n                                    0                                     0 \n                              ndvi_nw                               ndvi_se \n                                    0                                     0 \n                              ndvi_sw                  precipitation_amt_mm \n                                    0                                     0 \n                reanalysis_air_temp_k                 reanalysis_avg_temp_k \n                                    0                                     0 \n          reanalysis_dew_point_temp_k             reanalysis_max_air_temp_k \n                                    0                                     0 \n            reanalysis_min_air_temp_k       reanalysis_precip_amt_kg_per_m2 \n                                    0                                     0 \n reanalysis_relative_humidity_percent          reanalysis_sat_precip_amt_mm \n                                    0                                     0 \nreanalysis_specific_humidity_g_per_kg                     reanalysis_tdtr_k \n                                    0                                     0 \n                   station_avg_temp_c               station_diur_temp_rng_c \n                                    0                                     0 \n                   station_max_temp_c                    station_min_temp_c \n                                    0                                     0 \n                    station_precip_mm                             year_week \n                                    0                                     0 \n\n\nThere are no missing values in the dengue_ts_c.\n\n\nCode\ndengue_ts_c |&gt;\n  #filter(city==\"sj\") |&gt;\n  autoplot(total_cases)\n\n\n\n\n\nIt seems that there is some seasonality present. The seasonal plot shows that most of the spikes happen in the same months.\n\n\nCode\ndengue_ts_c |&gt;\n  gg_season(total_cases)\n\n\n\n\n\nI am performing an STL decomposition to identify seasonality and trend, if any.\n\n\nCode\ndengue_ts_c |&gt;\n  filter(city==\"sj\") |&gt;\n  model(\n    STL(total_cases)\n  ) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"STL decomposition for San Juan\")\n\n\nWarning: Ignoring unknown parameters: linewidth\n\n\n\n\n\n\n\nCode\ndengue_ts_c |&gt;\n  filter(city==\"iq\") |&gt;\n  model(\n    STL(total_cases)\n  ) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"STL decomposition for Iquitos\")\n\n\nWarning: Ignoring unknown parameters: linewidth\n\n\n\n\n\nIt seems that there is some seasonality when all the spikes are in the remainder. There also might be a slight increase in the trend for Iquitos.\nI am creating an ordered list of all the climate-related variables, descending from the highest correlation with the total cases to lowest.\n\n\nCode\ncor(dengue_ts_c[,c(6:25)],\n    dengue_ts_c[,4]) |&gt;\n  as.table() |&gt;\n  as.data.frame() |&gt;\n  select(-Var2) |&gt;\n  arrange(desc(Freq))\n\n\n                                    Var1        Freq\n1              reanalysis_min_air_temp_k  0.32528700\n2                     station_min_temp_c  0.26445735\n3                  reanalysis_air_temp_k  0.26295412\n4                  reanalysis_avg_temp_k  0.14823702\n5            reanalysis_dew_point_temp_k  0.13943398\n6  reanalysis_specific_humidity_g_per_kg  0.12666319\n7                     station_avg_temp_c  0.11350812\n8        reanalysis_precip_amt_kg_per_m2 -0.01146877\n9                     station_max_temp_c -0.04013282\n10                  precipitation_amt_mm -0.04243715\n11          reanalysis_sat_precip_amt_mm -0.04243715\n12                     station_precip_mm -0.07344970\n13  reanalysis_relative_humidity_percent -0.13335667\n14                               ndvi_sw -0.14512247\n15                               ndvi_nw -0.16844856\n16             reanalysis_max_air_temp_k -0.19310534\n17                               ndvi_ne -0.20513633\n18                               ndvi_se -0.21315790\n19               station_diur_temp_rng_c -0.23704112\n20                     reanalysis_tdtr_k -0.27946425\n\n\nThere seems to be no really strong correlation between the total cases and any individual predictor. Nevertheless, I am going to use this ranking when choosing the variables for time-consuming models.\nHere are plots of the variables with the highest correlations (in absolute values).\n\n\nCode\ndengue_ts_c %&gt;% \n  #filter(total_cases&lt;=100) %&gt;% \n  ggplot(aes(total_cases,\n             reanalysis_min_air_temp_k,\n             color = year)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\nCode\ndengue_ts_c %&gt;% \n  #filter(total_cases&lt;=100) %&gt;% \n  ggplot(aes(total_cases,\n             reanalysis_tdtr_k,\n             color = year)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nHere is the plot of all three variables stacked. There seems to be no obvious relationship between the predictors and the spikes.\n\n\nCode\ndengue_ts |&gt;\n  filter(city==\"sj\") |&gt;\n  pivot_longer(c(total_cases,\n                 reanalysis_min_air_temp_k,\n                 reanalysis_tdtr_k)) |&gt;\n  ggplot(aes(x = year_week, y = value)) +\n  geom_line() +\n  facet_grid(name ~ ., scales = \"free_y\") + ylab(\"\")\n\n\n\n\n\nWe can also observe relatively high correlation between the cases in one week and the week before.\n\n\nCode\ndengue_ts |&gt;\n  filter(city==\"sj\",\n         total_cases&lt;=100) |&gt;\n  gg_lag(y = total_cases,\n         lags = 1:8,\n         geom = \"point\")"
  },
  {
    "objectID": "Dengue_fc.html#significance",
    "href": "Dengue_fc.html#significance",
    "title": "Dengue Fever",
    "section": "",
    "text": "Dengue fever is a virus that is spread to people through the bite of an infected mosquito. A severe dengue fever case can lead to circulatory system failure, shock, and even death. Almost half of the world’s population, about 4 billion people, live in areas with a risk of dengue. Studying and forecasting the total number of infection incidents can help to tame this disease.\nI am using the fpp3 package to forecast the next total weekly cases of Dengue fever two cities in Peru and Puerto Rico.\nThe dataset is from drivendata.org competition.\n\n\nCode\nlibrary(fpp3)\n\n\n── Attaching packages ────────────────────────────────────────────── fpp3 0.5 ──\n\n\n✔ tibble      3.1.8     ✔ tsibble     1.1.3\n✔ dplyr       1.1.0     ✔ tsibbledata 0.4.1\n✔ tidyr       1.2.0     ✔ feasts      0.3.1\n✔ lubridate   1.8.0     ✔ fable       0.3.3\n✔ ggplot2     3.3.5     ✔ fabletools  0.3.3\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\n\nI am importing and combining the data on the number of cases and 20 possible predictor variables.\n\n\nCode\nlibrary(readr)\ndengue_fts &lt;- read_csv(\"DengAI_Predicting_Disease_Spread_-_Training_Data_Features.csv\")\n\n\nRows: 1456 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): city\ndbl  (22): year, weekofyear, ndvi_ne, ndvi_nw, ndvi_se, ndvi_sw, precipitati...\ndate  (1): week_start_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\ndengue_label &lt;- read_csv(\"DengAI_Predicting_Disease_Spread_-_Training_Data_Labels.csv\")\n\n\nRows: 1456 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): city\ndbl (3): year, weekofyear, total_cases\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\ndengue_label |&gt;\n  left_join(dengue_fts,\n            by = c(\"city\",\n                   \"year\",\n                   \"weekofyear\")) -&gt; dengue\n\n\nI am performing a quick random test to see if everything matches up.\n\n\nCode\ndengue_fts$reanalysis_dew_point_temp_k[256]==dengue$reanalysis_dew_point_temp_k[256]\n\n\n[1] TRUE\n\n\nI am transforming the combined dataset into a tsibble, the format for fpp3 package. The newly created year_week is an index variable and the two cities are keys to treat their observations as separate time series.\n\n\nCode\ndengue |&gt;\n  mutate(year_week = yearweek(week_start_date)) |&gt;\n  as_tsibble(key = city,\n             index = year_week) |&gt;\n  arrange(desc(city)) -&gt; dengue_ts\n\n\nHere is the plot fro both cities.\n\n\nCode\ndengue_ts |&gt;\n  #filter(city==\"sj\") |&gt;\n  autoplot(total_cases) +\n  labs(title = \"Total Cases of Dengue fever in San Juan and Iquitos\",\n       subtitle = \"April 1990 - June 2010, Weekly\",\n       y = \"Total Cases\",\n       x = \"Date\",\n       caption = \"Source: U.S. Centers for Disease Control and prevention\") +\n  theme_minimal()\n\n\n\n\n\nWe can see that the data for San Juan and Iquitos span different time periods but still overlap. We can also observe that most of the time the total number of cases is less than 50, however there are numerous spikes.\n\n\nCode\nplot(density(dengue_ts$total_cases))\nabline(v = 50, col = \"red\", lty = \"dashed\")\n\n\n\n\n\n\n\nCode\ndengue_ts |&gt;\n  filter(city==\"sj\") |&gt;\n  tail(1)\n\n\n# A tsibble: 1 x 26 [1W]\n# Key:       city [1]\n  city   year weekofyear total_cases week_start_date ndvi_ne ndvi_nw ndvi_se\n  &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 sj     2008         17           5 2008-04-22       -0.037 -0.0104  0.0773\n# … with 18 more variables: ndvi_sw &lt;dbl&gt;, precipitation_amt_mm &lt;dbl&gt;,\n#   reanalysis_air_temp_k &lt;dbl&gt;, reanalysis_avg_temp_k &lt;dbl&gt;,\n#   reanalysis_dew_point_temp_k &lt;dbl&gt;, reanalysis_max_air_temp_k &lt;dbl&gt;,\n#   reanalysis_min_air_temp_k &lt;dbl&gt;, reanalysis_precip_amt_kg_per_m2 &lt;dbl&gt;,\n#   reanalysis_relative_humidity_percent &lt;dbl&gt;,\n#   reanalysis_sat_precip_amt_mm &lt;dbl&gt;,\n#   reanalysis_specific_humidity_g_per_kg &lt;dbl&gt;, reanalysis_tdtr_k &lt;dbl&gt;, …\n\n\nCode\ndengue_ts |&gt;\n  filter(city==\"iq\") |&gt;\n  tail(1)\n\n\n# A tsibble: 1 x 26 [1W]\n# Key:       city [1]\n  city   year weekofyear total_cases week_start_date ndvi_ne ndvi_nw ndvi_se\n  &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 iq     2010         25           4 2010-06-25        0.298   0.233   0.274\n# … with 18 more variables: ndvi_sw &lt;dbl&gt;, precipitation_amt_mm &lt;dbl&gt;,\n#   reanalysis_air_temp_k &lt;dbl&gt;, reanalysis_avg_temp_k &lt;dbl&gt;,\n#   reanalysis_dew_point_temp_k &lt;dbl&gt;, reanalysis_max_air_temp_k &lt;dbl&gt;,\n#   reanalysis_min_air_temp_k &lt;dbl&gt;, reanalysis_precip_amt_kg_per_m2 &lt;dbl&gt;,\n#   reanalysis_relative_humidity_percent &lt;dbl&gt;,\n#   reanalysis_sat_precip_amt_mm &lt;dbl&gt;,\n#   reanalysis_specific_humidity_g_per_kg &lt;dbl&gt;, reanalysis_tdtr_k &lt;dbl&gt;, …\n\n\nMost of the models cannot work with gaps in time, so I am checking for explicit gaps.\n\n\nCode\nhas_gaps(dengue_ts, .full = T)\n\n\n# A tibble: 2 × 2\n  city  .gaps\n  &lt;chr&gt; &lt;lgl&gt;\n1 iq    TRUE \n2 sj    TRUE \n\n\nBoth cities have gaps, so I am filling gaps and using the previous value to fill them.\n\n\nCode\nlibrary(zoo)\n\n\n\nAttaching package: 'zoo'\n\n\nThe following object is masked from 'package:tsibble':\n\n    index\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\n\nCode\ndengue_ts |&gt;\n  fill_gaps() %&gt;%\n  mutate_all( ~ na.locf(.x, na.rm = FALSE)) -&gt; dengue_ts_c\n\n\nI am checking if there are any missing values left.\n\n\nCode\ncolSums(is.na(dengue_ts_c))\n\n\n                                 city                                  year \n                                    0                                     0 \n                           weekofyear                           total_cases \n                                    0                                     0 \n                      week_start_date                               ndvi_ne \n                                    0                                     0 \n                              ndvi_nw                               ndvi_se \n                                    0                                     0 \n                              ndvi_sw                  precipitation_amt_mm \n                                    0                                     0 \n                reanalysis_air_temp_k                 reanalysis_avg_temp_k \n                                    0                                     0 \n          reanalysis_dew_point_temp_k             reanalysis_max_air_temp_k \n                                    0                                     0 \n            reanalysis_min_air_temp_k       reanalysis_precip_amt_kg_per_m2 \n                                    0                                     0 \n reanalysis_relative_humidity_percent          reanalysis_sat_precip_amt_mm \n                                    0                                     0 \nreanalysis_specific_humidity_g_per_kg                     reanalysis_tdtr_k \n                                    0                                     0 \n                   station_avg_temp_c               station_diur_temp_rng_c \n                                    0                                     0 \n                   station_max_temp_c                    station_min_temp_c \n                                    0                                     0 \n                    station_precip_mm                             year_week \n                                    0                                     0 \n\n\nThere are no missing values in the dengue_ts_c.\n\n\nCode\ndengue_ts_c |&gt;\n  #filter(city==\"sj\") |&gt;\n  autoplot(total_cases)\n\n\n\n\n\nIt seems that there is some seasonality present. The seasonal plot shows that most of the spikes happen in the same months.\n\n\nCode\ndengue_ts_c |&gt;\n  gg_season(total_cases)\n\n\n\n\n\nI am performing an STL decomposition to identify seasonality and trend, if any.\n\n\nCode\ndengue_ts_c |&gt;\n  filter(city==\"sj\") |&gt;\n  model(\n    STL(total_cases)\n  ) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"STL decomposition for San Juan\")\n\n\nWarning: Ignoring unknown parameters: linewidth\n\n\n\n\n\n\n\nCode\ndengue_ts_c |&gt;\n  filter(city==\"iq\") |&gt;\n  model(\n    STL(total_cases)\n  ) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"STL decomposition for Iquitos\")\n\n\nWarning: Ignoring unknown parameters: linewidth\n\n\n\n\n\nIt seems that there is some seasonality when all the spikes are in the remainder. There also might be a slight increase in the trend for Iquitos.\nI am creating an ordered list of all the climate-related variables, descending from the highest correlation with the total cases to lowest.\n\n\nCode\ncor(dengue_ts_c[,c(6:25)],\n    dengue_ts_c[,4]) |&gt;\n  as.table() |&gt;\n  as.data.frame() |&gt;\n  select(-Var2) |&gt;\n  arrange(desc(Freq))\n\n\n                                    Var1        Freq\n1              reanalysis_min_air_temp_k  0.32528700\n2                     station_min_temp_c  0.26445735\n3                  reanalysis_air_temp_k  0.26295412\n4                  reanalysis_avg_temp_k  0.14823702\n5            reanalysis_dew_point_temp_k  0.13943398\n6  reanalysis_specific_humidity_g_per_kg  0.12666319\n7                     station_avg_temp_c  0.11350812\n8        reanalysis_precip_amt_kg_per_m2 -0.01146877\n9                     station_max_temp_c -0.04013282\n10                  precipitation_amt_mm -0.04243715\n11          reanalysis_sat_precip_amt_mm -0.04243715\n12                     station_precip_mm -0.07344970\n13  reanalysis_relative_humidity_percent -0.13335667\n14                               ndvi_sw -0.14512247\n15                               ndvi_nw -0.16844856\n16             reanalysis_max_air_temp_k -0.19310534\n17                               ndvi_ne -0.20513633\n18                               ndvi_se -0.21315790\n19               station_diur_temp_rng_c -0.23704112\n20                     reanalysis_tdtr_k -0.27946425\n\n\nThere seems to be no really strong correlation between the total cases and any individual predictor. Nevertheless, I am going to use this ranking when choosing the variables for time-consuming models.\nHere are plots of the variables with the highest correlations (in absolute values).\n\n\nCode\ndengue_ts_c %&gt;% \n  #filter(total_cases&lt;=100) %&gt;% \n  ggplot(aes(total_cases,\n             reanalysis_min_air_temp_k,\n             color = year)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\nCode\ndengue_ts_c %&gt;% \n  #filter(total_cases&lt;=100) %&gt;% \n  ggplot(aes(total_cases,\n             reanalysis_tdtr_k,\n             color = year)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nHere is the plot of all three variables stacked. There seems to be no obvious relationship between the predictors and the spikes.\n\n\nCode\ndengue_ts |&gt;\n  filter(city==\"sj\") |&gt;\n  pivot_longer(c(total_cases,\n                 reanalysis_min_air_temp_k,\n                 reanalysis_tdtr_k)) |&gt;\n  ggplot(aes(x = year_week, y = value)) +\n  geom_line() +\n  facet_grid(name ~ ., scales = \"free_y\") + ylab(\"\")\n\n\n\n\n\nWe can also observe relatively high correlation between the cases in one week and the week before.\n\n\nCode\ndengue_ts |&gt;\n  filter(city==\"sj\",\n         total_cases&lt;=100) |&gt;\n  gg_lag(y = total_cases,\n         lags = 1:8,\n         geom = \"point\")"
  },
  {
    "objectID": "Dengue_fc.html#outliers",
    "href": "Dengue_fc.html#outliers",
    "title": "Dengue Fever",
    "section": "Outliers",
    "text": "Outliers\n\n\nCode\ndengue_ts_c |&gt;\n  filter(city==\"iq\") |&gt;\n  ggplot(aes(total_cases)) +\n  geom_density()\n\n\n\n\n\nI am creating a dummy identifying if there was a spike in weekly cases.\n\n\nCode\ndengue_ts_c |&gt;\n  mutate(outlier = ifelse(total_cases&gt;=100&city==\"sj\",\n                          1,\n                          ifelse(total_cases&gt;=15&city==\"iq\",\n                                 1,\n                                 0)\n                          )) -&gt; dengue_100up\n\n\nHere is a visual representation.\n\n\nCode\ndengue_100up %&gt;% \n  filter(city==\"sj\") %&gt;% \n  ggplot(aes(x = year_week, y = total_cases)) +\n  geom_rect(aes(xmin = lag(year_week),\n                  xmax = year_week,\n                  ymin = 0, \n                  ymax = Inf, \n                  fill = outlier == 1), \n                  alpha = 0.5) +\n  geom_line() +\n  scale_fill_manual(values = c(\"transparent\", \n                               \"red\"),\n                    guide = \"none\")\n\n\nWarning: Removed 1 rows containing missing values (geom_rect).\n\n\n\n\n\nI am then creating a new list based on the variables’ correlation with the spike incidents.\n\n\nCode\ncor(dengue_100up[,c(6:25)],\n    dengue_100up[,27]) |&gt;\n  as.table() |&gt;\n  as.data.frame() |&gt;\n  select(-Var2) |&gt;\n  arrange(desc(Freq))\n\n\n                                    Var1         Freq\n1                     station_max_temp_c  0.198169569\n2  reanalysis_specific_humidity_g_per_kg  0.184571781\n3                     station_avg_temp_c  0.180714057\n4            reanalysis_dew_point_temp_k  0.174252235\n5              reanalysis_max_air_temp_k  0.157736465\n6                station_diur_temp_rng_c  0.145657342\n7   reanalysis_relative_humidity_percent  0.137849906\n8                  reanalysis_avg_temp_k  0.130503427\n9                   precipitation_amt_mm  0.117053029\n10          reanalysis_sat_precip_amt_mm  0.117053029\n11                     reanalysis_tdtr_k  0.107245826\n12                               ndvi_nw  0.101885734\n13       reanalysis_precip_amt_kg_per_m2  0.099191371\n14                               ndvi_ne  0.092681542\n15                               ndvi_sw  0.091356494\n16                    station_min_temp_c  0.080883428\n17                     station_precip_mm  0.071557679\n18                 reanalysis_air_temp_k  0.066923290\n19                               ndvi_se  0.006051806\n20             reanalysis_min_air_temp_k -0.018410823\n\n\n\n\nCode\ndengue_ts |&gt;\n  filter(city==\"sj\") |&gt;\n  pivot_longer(c(total_cases,\n                 station_max_temp_c,\n                 reanalysis_specific_humidity_g_per_kg)) |&gt;\n  ggplot(aes(x = year_week, y = value)) +\n  geom_line() +\n  facet_grid(name ~ ., scales = \"free_y\") + ylab(\"\")\n\n\n\n\n\nNo obvious relationship between the spikes and the two variables.\nAfter looking at the data, I am proceeding to create the forecast models."
  },
  {
    "objectID": "Dengue_fc.html#stl-decomposition-and-non-seasonal-method",
    "href": "Dengue_fc.html#stl-decomposition-and-non-seasonal-method",
    "title": "Dengue Fever",
    "section": "STL decomposition and non-seasonal method",
    "text": "STL decomposition and non-seasonal method\nAccording to Hyndman, et al., the simplest method is STL decomposition with a non-seasonal model applied to the seasonally adjusted data. I am creating a decomposition model with STL and non-seasonal ETS.\n\n\nCode\ndecomposition_model(\n  STL(total_cases),\n  ETS(season_adjust ~ season(\"N\"))\n) -&gt; den_dcmp_envm\n\ndengue_ts_c |&gt;\n  model(stl_ets = den_dcmp_envm) |&gt;\n  forecast(h = \"5 years\") -&gt; den_dcmp_fc\n\n\n\n\nCode\nden_dcmp_fc |&gt;\n  autoplot(dengue_ts_c) +\n  labs(title = \"Decomposition and STL forecast\",\n       y = \"Total Cases\",\n       x = \"Date\")\n\n\n\n\n\nI am forecasting 5 years for both of the cities to then cut the last two years of Iquitos.\nDue to its simplicity, it is similar to Seasonal Naive."
  },
  {
    "objectID": "Dengue_fc.html#arima-w-fourier",
    "href": "Dengue_fc.html#arima-w-fourier",
    "title": "Dengue Fever",
    "section": "ARIMA w/ Fourier",
    "text": "ARIMA w/ Fourier\nHere, I am creating an ARIMA model where seasonality is captured by Fourier series.\nI am creating several models with increasing number of K to then choose one based on the AICc.\n\n\nCode\ndengue_ts_c |&gt;\n  model(\n    K1 = ARIMA(total_cases ~ PDQ(0, 0, 0) + fourier(K = 1)),\n    K2 = ARIMA(total_cases ~ PDQ(0, 0, 0) + fourier(K = 2)),\n    K3 = ARIMA(total_cases ~ PDQ(0, 0, 0) + fourier(K = 3)),\n    K4 = ARIMA(total_cases ~ PDQ(0, 0, 0) + fourier(K = 4)),\n    K5 = ARIMA(total_cases ~ PDQ(0, 0, 0) + fourier(K = 5)),\n    K6 = ARIMA(total_cases ~ PDQ(0, 0, 0) + fourier(K = 6)),\n    K12 = ARIMA(total_cases ~ PDQ(0, 0, 0) + fourier(K = 12)),\n    K15 = ARIMA(total_cases ~ PDQ(0, 0, 0) + fourier(K = 15)),\n    K26 = ARIMA(total_cases ~ PDQ(0, 0, 0) + fourier(K = 26)),\n        ) -&gt; dengue_arima3\n\n\n\n\nCode\nglance(dengue_arima3) |&gt;\n  select(city, .model, AICc)\n\n\n# A tibble: 18 × 3\n   city  .model  AICc\n   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1 iq    K1     3539.\n 2 iq    K2     3543.\n 3 iq    K3     3545.\n 4 iq    K4     3546.\n 5 iq    K5     3548.\n 6 iq    K6     3549.\n 7 iq    K12    3545.\n 8 iq    K15    3549.\n 9 iq    K26    3571.\n10 sj    K1     7521.\n11 sj    K2     7520.\n12 sj    K3     7523.\n13 sj    K4     7525.\n14 sj    K5     7529.\n15 sj    K6     7532.\n16 sj    K12    7543.\n17 sj    K15    7548.\n18 sj    K26    7565.\n\n\nThe larger numbers of K worsen the AICc. Only the model for San Juan is slightly better at K = 2. Nevertheless, since the improvement is small, to stay consistent, I keep K = 1.\n\n\nCode\ndengue_arima3 |&gt;\n  select(K1) |&gt;\n  forecast(h = \"5 years\") -&gt; den_arima_fc\n\n\n\n\nCode\nden_arima_fc |&gt;\n  autoplot(dengue_ts_c) +\n  labs(title = \"ARIMA with Fourier forecast\",\n       y = \"Total Cases\",\n       x = \"Date\")"
  },
  {
    "objectID": "Dengue_fc.html#nnetar",
    "href": "Dengue_fc.html#nnetar",
    "title": "Dengue Fever",
    "section": "NNETAR",
    "text": "NNETAR\nThe third model is Neural Networks. This is the most time consuming model, but the results can be very promising.\n\n\nCode\nden_nn_model = dengue_ts_c |&gt;\n  model(nnetar = NNETAR(total_cases))\n\n\n\n\nCode\nstart_time = Sys.time()\n\nden_nn_model |&gt;\n  forecast(h = \"3 years\") -&gt; den_nn_fc\n\nprint(Sys.time() - start_time)\n\n\nTime difference of 19.48274 mins\n\n\n\n\nCode\nden_nn_fc |&gt;\n  autoplot(dengue_ts_c) +\n  labs(title = \"NNETAR with no external regs forecast\",\n       y = \"Total Cases\",\n       x = \"Date\")\n\n\n\n\n\nHere we can see that the model for Iquitos picked up the trend but then flattened out, which doesn’t look good. The model for San Juan, however looks promising but also looses all seasonality later on.\nI am no trying NNETAR with external regressors: station_max_temp_c\nreanalysis_specific_humidity_g_per_kg\nreanalysis_min_air_temp_k reanalysis_air_temp_k.\nHowever, in order to use them, I need features from the test data.\n\n\nCode\nden_test_fts &lt;- read_csv(\"DengAI_Predicting_Disease_Spread_-_Test_Data_Features.csv\")\n\n\nRows: 416 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): city\ndbl  (22): year, weekofyear, ndvi_ne, ndvi_nw, ndvi_se, ndvi_sw, precipitati...\ndate  (1): week_start_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe test dataset also has some gaps, so I am filling them.\n\n\nCode\nden_test_fts |&gt;\n  mutate(year_week = yearweek(week_start_date)) |&gt;\n  as_tsibble(key = city,\n             index = year_week) |&gt;\n  fill_gaps() %&gt;% \n  mutate_all( ~ na.locf(.x, na.rm = FALSE)) %&gt;% \n  arrange(desc(city)) -&gt; den_test_ts2\n\n\n\n\nCode\nden_nn_model2 = dengue_ts_c |&gt;\n  model(nnetar = NNETAR(total_cases ~ station_max_temp_c +\n                          reanalysis_specific_humidity_g_per_kg +\n                          reanalysis_min_air_temp_k +\n                          reanalysis_air_temp_k))\n\n\n\n\nCode\nstart_time = Sys.time()\n\nden_nn_model2 |&gt;\n  forecast(new_data = den_test_ts2) -&gt; den_nn_fc2\n\nprint(Sys.time() - start_time)\n\n\nTime difference of 26.52379 mins\n\n\n\n\nCode\nden_nn_fc2 |&gt;\n  autoplot(dengue_ts_c) +\n  labs(title = \"NNETAR w/ Regressors forecast\",\n       y = \"Total Cases\",\n       x = \"Date\")\n\n\n\n\n\nThe resulting forecast looks very promising, indeed. The prediction intervals also seem to cover most of the probable observations, without getting into negative territory."
  },
  {
    "objectID": "Dengue_fc.html#arima-w-regressors",
    "href": "Dengue_fc.html#arima-w-regressors",
    "title": "Dengue Fever",
    "section": "ARIMA w/ Regressors",
    "text": "ARIMA w/ Regressors\nTo incorporate more predictor variables from the dataset, I am using another ARIMA model with external regressors.\n\n\nCode\ndengue_ts_c |&gt;\n  model(ARIMA.reg = ARIMA(total_cases ~ PDQ(0, 0, 0) + \n                            reanalysis_min_air_temp_k +\n                            reanalysis_air_temp_k +\n                            station_min_temp_c)) -&gt; den_arreg_fit\n\n\n\n\nCode\nden_arreg_fit |&gt;\n  forecast(new_data = den_test_ts2) -&gt; den_arreg_fc2\n\n\n\n\nCode\nden_arreg_fc2 |&gt;\n  autoplot(dengue_ts_c)\n\n\n\n\n\nUsing only the variables with highest correlation doesn’t seem to improve the forecast, all that much. Therefore, I am going to use all of them. I am also adding Fourier for seasonality.\n\n\nCode\ncolnames(dengue_ts_c)\n\n\n [1] \"city\"                                 \n [2] \"year\"                                 \n [3] \"weekofyear\"                           \n [4] \"total_cases\"                          \n [5] \"week_start_date\"                      \n [6] \"ndvi_ne\"                              \n [7] \"ndvi_nw\"                              \n [8] \"ndvi_se\"                              \n [9] \"ndvi_sw\"                              \n[10] \"precipitation_amt_mm\"                 \n[11] \"reanalysis_air_temp_k\"                \n[12] \"reanalysis_avg_temp_k\"                \n[13] \"reanalysis_dew_point_temp_k\"          \n[14] \"reanalysis_max_air_temp_k\"            \n[15] \"reanalysis_min_air_temp_k\"            \n[16] \"reanalysis_precip_amt_kg_per_m2\"      \n[17] \"reanalysis_relative_humidity_percent\" \n[18] \"reanalysis_sat_precip_amt_mm\"         \n[19] \"reanalysis_specific_humidity_g_per_kg\"\n[20] \"reanalysis_tdtr_k\"                    \n[21] \"station_avg_temp_c\"                   \n[22] \"station_diur_temp_rng_c\"              \n[23] \"station_max_temp_c\"                   \n[24] \"station_min_temp_c\"                   \n[25] \"station_precip_mm\"                    \n[26] \"year_week\"                            \n\n\n\n\nCode\ndengue_ts_c |&gt;\n  model(ARIMA.reg = ARIMA(total_cases ~ PDQ(0, 0, 0) + \n                            ndvi_ne + ndvi_nw + ndvi_se + ndvi_sw +\n                            precipitation_amt_mm + \n                            reanalysis_air_temp_k + \n                            reanalysis_avg_temp_k +\n                            reanalysis_dew_point_temp_k +\n                            reanalysis_max_air_temp_k +\n                            reanalysis_min_air_temp_k +\n                            reanalysis_relative_humidity_percent +\n                            reanalysis_specific_humidity_g_per_kg +\n                            reanalysis_precip_amt_kg_per_m2 +\n                            reanalysis_tdtr_k +\n                            station_avg_temp_c +\n                            station_diur_temp_rng_c +\n                            station_max_temp_c +\n                            station_min_temp_c +\n                            station_precip_mm +\n                            fourier(K = 1)))-&gt;den_arreg_fit2\n\n#Removed reanalysis_sat_precip_amt_mm to avoid rank deficiency\n\n\n\n\nCode\nden_arreg_fit2 |&gt;\n  forecast(new_data = den_test_ts2) -&gt; den_arreg_fc3\n\n\n\n\nCode\nden_arreg_fc3 |&gt;\n  autoplot(dengue_ts_c)\n\n\n\n\n\nThe forecast looks better now, but it looks too clean compared to the previous values.\n\n\nCode\ndengue_ts_c |&gt;\n  model(ARIMA.reg = ARIMA(total_cases ~ PDQ(0, 0, 0) + \n                            ndvi_ne + ndvi_nw + ndvi_se + ndvi_sw +\n                            precipitation_amt_mm + \n                            reanalysis_air_temp_k + \n                            reanalysis_avg_temp_k +\n                            reanalysis_dew_point_temp_k +\n                            reanalysis_max_air_temp_k +\n                            reanalysis_min_air_temp_k +\n                            reanalysis_relative_humidity_percent +\n                            reanalysis_specific_humidity_g_per_kg +\n                            reanalysis_precip_amt_kg_per_m2 +\n                            reanalysis_tdtr_k +\n                            station_avg_temp_c +\n                            station_diur_temp_rng_c +\n                            station_max_temp_c +\n                            station_min_temp_c +\n                            station_precip_mm +\n                            season()))-&gt;den_arreg_fit3\n\n#Removed reanalysis_sat_precip_amt_mm to avoid rank deficiency\n\n\n\n\nCode\nden_arreg_fit3 |&gt;\n  forecast(new_data = den_test_ts2) -&gt; den_arreg_fc4\n\n\n\n\nCode\nden_arreg_fc4 |&gt;\n  autoplot(dengue_ts_c) +\n  labs(title = \"ARIMA w/ Regressors and Seasonality forecast\",\n       y = \"Total Cases\",\n       x = \"Date\")\n\n\n\n\n\nThe same ARIMA model with seasonality included looks better than that with Fourier but worse than NNETAR."
  },
  {
    "objectID": "Dengue_fc.html#results",
    "href": "Dengue_fc.html#results",
    "title": "Dengue Fever",
    "section": "Results",
    "text": "Results\nI had to upload the forecasts to the drivendata.org to obtain the evaluation of the models. The forecast used the observations at time T to forecast the number of cases at time T, because it was a forecast for the internally missing data.\nI also had to manually remove some 3 observations from the predictions, since the evaluation dataset didn’t have some dates.\nGiven the Score and the Rank, NNETAR model is by far the best one. Decomposition model’s performance was quite low, ARIMA with seasonality and external regressors got a score of 25.93 and rank of 2042. NNETAR did better by a wide margin: the MAE score of 22.48 and rank of 512. This is out of approximately 12 thousand participants.\nThe respective submission IDs are id-240847, id-240852, and id-240853."
  },
  {
    "objectID": "Dengue_fc.html#further-work",
    "href": "Dengue_fc.html#further-work",
    "title": "Dengue Fever",
    "section": "Further work",
    "text": "Further work\nAdding more external variables will most probably increase the performance of NNETAR model. There also might be some calendar variables that should be included. For example, in the beginning we saw that the spikes occur in the same months. Adding dummies for high number months might improve the forecasts."
  },
  {
    "objectID": "ETS.html",
    "href": "ETS.html",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US",
    "section": "",
    "text": "In the efforts of reaching net zero emissions, renewable energy sources get more and more important. One of the cheapest and most sustainable sources of electricity is conventional hydroelectric power generation.\nEven though, the US has been using conventional hydropower since 1950s, the industry is far from reaching its full potential. With thousands of yet to be used dams and an increasing pressure to switch to the renewable energy, the demand and investments are going to rise significantly. Meanwhile, high seasonality and annual variation of the hydropower output can produce more accurate forecasting models. Here Seasonal Naïve, Regression, ETS, ARIMA, and ARIMA with external regressors are tested using RMSE and MPE to see which model better predicts unseen data. Using the net monthly conventional hydropower generation dataset, I identified ARIMA as the best one for the role.\nTechnical Note: A megawatt is a unit for measuring instantaneous power equivalent to one million watts or thousand kilowatts. A megawatt hour (MWh) is equal to 1,000 kilowatt hours (kWh). It is equal to 1,000 kilowatts of energy consumed or produced in one hour. It is a bit more than the amount of electricity used by an average US residential utility customer in one month (886 kWh), according to the EIA.\n\n\nCode\nlibrary(fpp3)\n\n\n── Attaching packages ────────────────────────────────────────────── fpp3 0.5 ──\n\n\n✔ tibble      3.1.8     ✔ tsibble     1.1.3\n✔ dplyr       1.1.0     ✔ tsibbledata 0.4.1\n✔ tidyr       1.2.0     ✔ feasts      0.3.1\n✔ lubridate   1.8.0     ✔ fable       0.3.3\n✔ ggplot2     3.3.5     ✔ fabletools  0.3.3\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()"
  },
  {
    "objectID": "ETS.html#abstract",
    "href": "ETS.html#abstract",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US",
    "section": "",
    "text": "In the efforts of reaching net zero emissions, renewable energy sources get more and more important. One of the cheapest and most sustainable sources of electricity is conventional hydroelectric power generation.\nEven though, the US has been using conventional hydropower since 1950s, the industry is far from reaching its full potential. With thousands of yet to be used dams and an increasing pressure to switch to the renewable energy, the demand and investments are going to rise significantly. Meanwhile, high seasonality and annual variation of the hydropower output can produce more accurate forecasting models. Here Seasonal Naïve, Regression, ETS, ARIMA, and ARIMA with external regressors are tested using RMSE and MPE to see which model better predicts unseen data. Using the net monthly conventional hydropower generation dataset, I identified ARIMA as the best one for the role.\nTechnical Note: A megawatt is a unit for measuring instantaneous power equivalent to one million watts or thousand kilowatts. A megawatt hour (MWh) is equal to 1,000 kilowatt hours (kWh). It is equal to 1,000 kilowatts of energy consumed or produced in one hour. It is a bit more than the amount of electricity used by an average US residential utility customer in one month (886 kWh), according to the EIA.\n\n\nCode\nlibrary(fpp3)\n\n\n── Attaching packages ────────────────────────────────────────────── fpp3 0.5 ──\n\n\n✔ tibble      3.1.8     ✔ tsibble     1.1.3\n✔ dplyr       1.1.0     ✔ tsibbledata 0.4.1\n✔ tidyr       1.2.0     ✔ feasts      0.3.1\n✔ lubridate   1.8.0     ✔ fable       0.3.3\n✔ ggplot2     3.3.5     ✔ fabletools  0.3.3\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()"
  },
  {
    "objectID": "ETS.html#data-preparation-and-analysis",
    "href": "ETS.html#data-preparation-and-analysis",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US",
    "section": "Data Preparation and Analysis",
    "text": "Data Preparation and Analysis\nI import the data on net energy generation of electricity from the conventional hydroelectric sources from the EIA website. The units are measured in thousand megawatt hours. The timeframe is from January 2001 through April 2023.\n\n\nCode\nlibrary(readr)\nnet_gen &lt;- read_csv(\"Net_generation_United_States_all_sectors_monthly.csv\", \n    skip = 4)\n\n\nRows: 268 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Month\ndbl (1): conventional hydroelectric thousand megawatthours\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHere is a preview of the dataset:\n\n\nCode\nhead(net_gen)\n\n\n# A tibble: 6 × 2\n  Month    `conventional hydroelectric thousand megawatthours`\n  &lt;chr&gt;                                                  &lt;dbl&gt;\n1 Apr 2023                                              17917.\n2 Mar 2023                                              20630.\n3 Feb 2023                                              19338.\n4 Jan 2023                                              22954.\n5 Dec 2022                                              21870.\n6 Nov 2022                                              18764.\n\n\n\n\nCode\nlibrary(summarytools)\n\n\n\nAttaching package: 'summarytools'\n\n\nThe following object is masked from 'package:tibble':\n\n    view\n\n\nI am changing the name of the variable to MWH for brevity.\n\n\nCode\n# rename the variable to MWH\nnet_gen = net_gen %&gt;% \n  rename(MWH = \"conventional hydroelectric thousand megawatthours\")\n\n\nHere is a table of some descriptive statistics:\n\n\nCode\ndescr(net_gen,\n      stats = c(\"mean\", \"sd\",\"med\", \"min\", \"max\"), \n      transpose = TRUE)\n\n\nNon-numerical variable(s) ignored: Month\n\n\nDescriptive Statistics  \nnet_gen$MWH  \nN: 268  \n\n                Mean   Std.Dev     Median        Min        Max\n--------- ---------- --------- ---------- ---------- ----------\n      MWH   22469.85   3972.41   22187.54   14638.18   32607.12\n\n\nI reorder the rows, so that the last observation (earliest date) is now the first one.\n\n\nCode\nnet_gen=net_gen[order(nrow(net_gen):1),]\n\n\n\n\nCode\n#changing to yearmonth format, naming Date, and indexing\nnet_gen = net_gen %&gt;% \n  mutate(Date = yearmonth(Month)) |&gt;\n  as_tsibble(index = Date)\n\n\n\n\nCode\nnet_gen %&gt;% \n  autoplot(MWH) +\n  labs(title = \"Net Conventional Hydroelectric Power Generation\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\nHere we can observe a general plot of the Hydropower time series. There seems to be no significant trend and high seasonality.\n\n\nCode\nnet_gen |&gt;\n  gg_season(MWH, labels = \"both\")\n\n\n\n\n\nAs we can see here, there are spikes in the same months across all years, indicating high seasonality.\n\n\nCode\nnet_gen |&gt;\n  gg_subseries(MWH)\n\n\n\n\n\nThis plot, where the blue line is the average across the same months, shows that on average there is a spike in the late spring to early summer. The trough is in fall.\n\n\nCode\nnet_gen |&gt;\n  model(stl = STL(MWH)) |&gt;\n  components() |&gt; \n  gg_subseries(season_year) +\n  theme(axis.text.x = element_text(size = 5))\n\n\n\n\n\nIn this plot we can see the average and the trend by month across different years. This plot shows the same spikes and troughs, but we can also observe that the months with the highest values have a downward trend. This means that even though the energy output in those months is still the highest, it diminishes.\n\n\nCode\nnet_gen |&gt;\n  model(stl = STL(MWH)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\nWarning: Ignoring unknown parameters: linewidth\n\n\n\n\n\nHere we see the STL decomposition components: trend, seasonality, and the remainder. There is high seasonality and some downward trend at the end of the series. The gray bars on the left show us that seasonality is very significant, whereas the general trend is less so.\n\n\nCode\nnet_gen |&gt;\n  ACF(MWH) |&gt;\n  autoplot()\n\n\n\n\n\nThe plot of Autocorrelation Figure also indicates high seasonality, since the the values 12 months apart have the highest correlation.\nI estimate Naive, Seasonal Naive, Drift, and two ETS models. There are two ETS models because the optimal ETS() (the default one) seems to be underperforming, since it chooses no trend. I add ETS(A,Ad,M) since it seems to increase forecast accuracy.\nA mable: 1 x 5 Naive Season_Naive Drift ETSopt ETS      1    &lt;ETS(M,N,A)&gt; &lt;ETS(A,Ad,M)&gt;"
  },
  {
    "objectID": "ETS.html#creating-training-and-testing-sets.",
    "href": "ETS.html#creating-training-and-testing-sets.",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US",
    "section": "Creating Training and Testing sets.",
    "text": "Creating Training and Testing sets.\n\n\nCode\n#creating the var with the number of ~80% and ~20% of the observations and then splitting them accordingly.\nround(268*0.8)\n\n\n[1] 214\n\n\nCode\ntotal_obs.net_gen = dim(net_gen)[1] #puts n of obs into total_obs\ntrain_obs = total_obs.net_gen * 0.8\ntest_obs = total_obs.net_gen - train_obs\nnet_gen.train2 = head(net_gen, train_obs)\nnet_gen.test2 = tail(net_gen, test_obs)"
  },
  {
    "objectID": "ETS.html#arima-estimation",
    "href": "ETS.html#arima-estimation",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US",
    "section": "ARIMA Estimation",
    "text": "ARIMA Estimation\n\n\nCode\nnet_gen |&gt; gg_tsdisplay(MWH,\n                     plot_type='partial', lag_max = 24)\n\n\n\n\n\n\n\nCode\nnet_gen |&gt; gg_tsdisplay(difference(MWH, 12),\n                     plot_type='partial', lag_max = 24)\n\n\nWarning: Removed 12 row(s) containing missing values (geom_path).\n\n\nWarning: Removed 12 rows containing missing values (geom_point).\n\n\n\n\n\n\n\nCode\nnet_gen |&gt; gg_tsdisplay(difference(MWH, 12) |&gt; difference(),\n                     plot_type='partial', lag_max = 24)\n\n\nWarning: Removed 13 row(s) containing missing values (geom_path).\n\n\nWarning: Removed 13 rows containing missing values (geom_point).\n\n\n\n\n\n\n\nCode\n#initial ARIMA estimation\nnet_gen.train2 |&gt;\n  model(auto = ARIMA(MWH,\n                     stepwise = FALSE, #gives ARIMA more time to calculate\n                     approx = FALSE)) -&gt; arima.fit\n\n\nWarning in sqrt(diag(best$var.coef)): NaNs produced\n\n\nWhen estimating ARIMA models, it is important to check if the residuals look like white noise and have no patterns.\n\n\nCode\narima.fit |&gt;\n  gg_tsresiduals(lag_max=36)\n\n\n\n\n\nThe graphic of the residuals shows that residuals look pretty random, but there is a spike in acf that is statistically significant (crossed the blue line). I have to perform Ljung-Box test.\n\n\nCode\narima.fit\n\n\n# A mable: 1 x 1\n                       auto\n                    &lt;model&gt;\n1 &lt;ARIMA(2,0,0)(2,1,1)[12]&gt;\n\n\nA mable: 1 x 1 auto  1 &lt;ARIMA(2,0,0)(2,1,1)[12]&gt;\n\n\nCode\n#Ljung-Box test\naugment(arima.fit) |&gt;\n  features(.innov, ljung_box, lag = 36, dof = 5)\n\n\n# A tibble: 1 × 3\n  .model lb_stat lb_pvalue\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 auto      28.9     0.575\n\n\nCode\n#dof=p+q+P+Q from ARIMA(p,d,q)(P,D,Q)\n\n\nThe p-value is very large, which is exactly what we need to proceed and not care about autocorrelation in the residuals."
  },
  {
    "objectID": "ETS.html#seasonal-naive-ts-regression-and-ets",
    "href": "ETS.html#seasonal-naive-ts-regression-and-ets",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US",
    "section": "Seasonal Naive, TS Regression, and ETS",
    "text": "Seasonal Naive, TS Regression, and ETS\n\n\nCode\nnet_gen.fit2 = net_gen.train2 |&gt;\n  model(\n        Season_Naive = SNAIVE(MWH),\n        Regression = TSLM(MWH ~ trend() + season()),\n        ETSopt = ETS(MWH), #automatic\n        ETS = ETS(MWH ~ error(\"A\")\n                      + trend(\"Ad\")\n                      + season(\"M\"))  #industry standard\n        )\n\n\n\n\nCode\nnet_gen.fit2\n\n\n# A mable: 1 x 4\n  Season_Naive Regression        ETSopt           ETS\n       &lt;model&gt;    &lt;model&gt;       &lt;model&gt;       &lt;model&gt;\n1     &lt;SNAIVE&gt;     &lt;TSLM&gt; &lt;ETS(M,Ad,M)&gt; &lt;ETS(A,Ad,M)&gt;\n\n\nA mable: 1 x 4 Season_Naive Regression ETSopt ETS     1   &lt;ETS(M,Ad,M)&gt; &lt;ETS(A,Ad,M)&gt;\n\n\nCode\naccuracy(net_gen.fit2) |&gt;\n  select(.model, RMSE, MAE)\n\n\n# A tibble: 4 × 3\n  .model        RMSE   MAE\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 Season_Naive 3201. 2437.\n2 Regression   2421. 1856.\n3 ETSopt       1687. 1296.\n4 ETS          1685. 1290.\n\n\n\n\nCode\naccuracy(arima.fit) |&gt;\n  select(.model, RMSE, MAE)\n\n\n# A tibble: 1 × 3\n  .model  RMSE   MAE\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 auto   1698. 1243.\n\n\nWe can see how closely the corresponding models predict the data within the Training set.\nNow using these models I predict the next 54 months and compare the predictions to the withheld observations.\n\n\nCode\n#generating forecast\nnet_gen.fc2 = net_gen.fit2 |&gt;\n  forecast(h = 54)\nnet_gen.fc2.arima = arima.fit |&gt;\n  forecast(h = 54)\n\n\n\n\nCode\nnet_gen.fc2 |&gt;\n  autoplot(net_gen.train2, level = NULL) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation Forecast\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal() +\n  theme(plot.caption = element_text(hjust = 6.5))\n\n\n\n\n\n\n\nCode\nnet_gen.fc2 |&gt;\n  #filter(.model = \"\") |&gt;\n  autoplot(net_gen.test2, level = NULL) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation Forecast\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\nCode\n  #theme(plot.caption = element_text(hjust = 5))\n\n\n\n\nCode\nnet_gen.fc2.arima |&gt;\n  autoplot(net_gen.train2) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation\nARIMA Forecast\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\nnet_gen.fc2.arima |&gt;\n  #filter(.model = \"\") |&gt;\n  autoplot(net_gen.test2) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation\nARIMA Forecast\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\nCode\n  #theme(plot.caption = element_text(hjust = 5))\n\n\n\n\nCode\nnet_gen.fc2.arima |&gt;\n  autoplot(net_gen.test2, level = NULL) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation\nARIMA Forecast\",\n       subtitle = \"Nov 2018 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\nIn these plots we can closely observe the ARIMA forecast is quite close, and even though some predicted observations are off, almost all actual values are within the 95% prediction interval."
  },
  {
    "objectID": "ETS.html#arima-with-external-regressors",
    "href": "ETS.html#arima-with-external-regressors",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US",
    "section": "ARIMA with external Regressors",
    "text": "ARIMA with external Regressors\nI import the meteorological data on droughts in the US. I used the Monthly Historical Drought Data, because I assume that the general amount of water affects the aggregate power generated in that month. According to the National Integrated Drought Information System, “SPI captures how observed precipitation (rain, hail, snow) deviates from the climatological average over a given time period—in this case, over the 9 months leading up to the selected date.”\n\n\nCode\nSPI &lt;- read_csv(\"SPI.csv\")\n\n\nRows: 1542 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): DATE\ndbl (12): 0, D0, D1, D2, D3, D4, -9, W0, W1, W2, W3, W4\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nI prepare the meteorological dataset and combine it with the Hydropower one.\n\n\nCode\n#finding the start date\nwhich(SPI$DATE==\"d_20010101\")\n\n\n[1] 1273\n\n\n\n\nCode\n#matching the timeframe with that of Hydropower dataset\nspi = SPI[1273:nrow(SPI), c(-1)]\nspi = spi[1:268,]\n\n\n\n\nCode\nnet_gen = net_gen %&gt;% \n  bind_cols(spi)\n\n\nHere is a preview of the resulting data:\n\n\nCode\nhead(net_gen)\n\n\n# A tsibble: 6 x 15 [1M]\n  Month       MWH     Date DATE     D0    D1    D2    D3    D4  `-9`    W0    W1\n  &lt;chr&gt;     &lt;dbl&gt;    &lt;mth&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Jan 2001 18852. 2001 Jan d_20…  40.8  28.2  12.7   7.3   2.8     0  18.4  11.3\n2 Feb 2001 17473. 2001 Feb d_20…  37.3  27.1  13.9   8.5   4.5     0  23.9  14.5\n3 Mar 2001 20477. 2001 Mar d_20…  37.6  25.9  11.8   7     3.8     0  17.5   8.6\n4 Apr 2001 18013. 2001 Apr d_20…  37.5  26.8  13     7.3   3.1     0  22.8  13.6\n5 May 2001 19176. 2001 May d_20…  36.4  27.6  14.4   8.2   3.4     0  29.7  20.6\n6 Jun 2001 20728. 2001 Jun d_20…  36.8  29.5  16.4   9.3   3.3     0  36.2  25.1\n# … with 3 more variables: W2 &lt;dbl&gt;, W3 &lt;dbl&gt;, W4 &lt;dbl&gt;\n\n\nI again split my data in two parts: training and testing sets.\n\n\nCode\n# Second Split of the data\ntotal_obs.net_gen = dim(net_gen)[1] #puts n of obs into total_obs\ntrain_obs = total_obs.net_gen * 0.8\ntest_obs = total_obs.net_gen - train_obs\nnet_gen.train2 = head(net_gen, train_obs)\nnet_gen.test2 = tail(net_gen, test_obs)\n\n\nHere we can see the percentage of the US territory that had experienced exceptional drought (D4).\n\n\nCode\nnet_gen |&gt; \n  autoplot(D4)\n\n\n\n\n\nI combine this plot with the hydropower time series, to see if there is any visual relationship.\n\n\nCode\nnet_gen |&gt;\n  pivot_longer(c(MWH, D4)) |&gt;\n  ggplot(aes(x = Date, y = value)) +\n  geom_line() +\n  facet_grid(name ~ ., scales = \"free_y\") + ylab(\"\")\n\n\n\n\n\nIt is hard to tell if there is any strong relationship between the two.\n\n\nCode\nnet_gen |&gt;\n  pivot_longer(c(MWH, W4)) |&gt;\n  ggplot(aes(x = Date, y = value)) +\n  geom_line() +\n  facet_grid(name ~ ., scales = \"free_y\") + ylab(\"\")\n\n\n\n\n\nThe same lack of obvious relationship can be said about the exceptionally wet %.\nI look at the correlation between these variables.\n\n\nCode\nlibrary(GGally)\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\n\n\nCode\nnet_gen |&gt;\n  GGally::ggpairs(columns = c(\"MWH\",\"D4\",\"W4\"))\n\n\n\n\n\nAgain, no strong relationship, althought correlation with W4 is statistically significant.\nI include all the independent variables in the regression, as well as the seasonality variable.\n\n\nCode\nnet_gen.fit3 = net_gen.train2 |&gt;\n  model(ARIMA.reg = ARIMA(MWH ~ D0+D1+D2+D3+D4+W0+W1+W2+W3+W4+season()))\n\n\n\n\nCode\nnet_gen.fit3 |&gt; gg_tsresiduals()\n\n\n\n\n\nWe can clearly see here that the residuals look random, which is good.\nI then create the future observations for the independent meteorological observations, by averaging the past values.\n\n\nCode\nnet_gen_future &lt;- new_data(net_gen.train2, 54) |&gt;\n  mutate(D0 = mean(net_gen$D0),\n         D1 = mean(net_gen$D1),\n         D2 = mean(net_gen$D2),\n         D3 = mean(net_gen$D3),\n         D4 = mean(net_gen$D4),\n         W0 = mean(net_gen$W0),\n         W1 = mean(net_gen$W1),\n         W2 = mean(net_gen$W2),\n         W3 = mean(net_gen$W3),\n         W4 = mean(net_gen$W4))\n\n\n\n\nCode\n#forecasting using the newly created observations\nnet_gen.fc3 = net_gen.fit3 |&gt;\n  forecast(new_data = net_gen_future)\n\n\n\n\nCode\nnet_gen.fc3 |&gt;\n  autoplot(net_gen.train2) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation Forecast\nARIMA with Regressors Forecast\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\nnet_gen.fc3 |&gt;\n  autoplot(net_gen.test2) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation\nARIMA with Regressors Forecast\",\n       subtitle = \"Nov 2018 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()"
  },
  {
    "objectID": "ETS.html#accuracy-evaluation",
    "href": "ETS.html#accuracy-evaluation",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US",
    "section": "Accuracy Evaluation",
    "text": "Accuracy Evaluation\n\n\nCode\nnet_gen.fc3 |&gt;\n  accuracy(net_gen) |&gt;\n  select(.model, RMSE, ME, MPE)\n\n\n# A tibble: 1 × 4\n  .model     RMSE    ME   MPE\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 ARIMA.reg 2367. -112. -1.29\n\n\n\n\nCode\nnet_gen.fc2 |&gt;\n  accuracy(net_gen) |&gt;\n  select(.model, RMSE, ME, MPE)\n\n\n# A tibble: 4 × 4\n  .model        RMSE     ME    MPE\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 ETS          3824. -2821. -13.4 \n2 ETSopt       3863. -2877. -13.7 \n3 Regression   2928. -1518.  -7.79\n4 Season_Naive 3323. -1733.  -8.80\n\n\n\n\nCode\nnet_gen.fc2.arima |&gt;\n  accuracy(net_gen) |&gt;\n  select(.model, RMSE, ME, MPE)\n\n\n# A tibble: 1 × 4\n  .model  RMSE    ME   MPE\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 auto   2638. -567. -3.41\n\n\n\n\nCode\nnet_gen.fc2 |&gt;\n  accuracy(net_gen) |&gt;\n  select(.model, RMSE, ME, MPE) |&gt;\n  as.data.frame() -&gt; acc.metric\n\nnet_gen.fc2.arima |&gt;\n  accuracy(net_gen) |&gt;\n  select(.model, RMSE, ME, MPE) |&gt;\n  as.data.frame() |&gt;\n  bind_rows(acc.metric) -&gt; acc.metric\n\nnet_gen.fc3 |&gt;\n  accuracy(net_gen) |&gt;\n  select(.model, RMSE, ME, MPE) |&gt;\n  as.data.frame() |&gt;\n  bind_rows(acc.metric) -&gt; acc.metric\n\nacc.metric[2,1] = \"ARIMA\"\n\n\n\n\nCode\nacc.metric %&gt;% \n  group_by(.model, RMSE) %&gt;% \n  ggplot(aes(x = RMSE, \n             xend = 2300, \n             y = reorder(.model, desc(RMSE)), \n             yend=.model,\n             label=round(RMSE, 0))) +\n  theme_minimal() +\n  geom_segment() +\n  geom_point() +\n  geom_text(nudge_x = -40, nudge_y = 0.2,\n            size = 3) +\n  theme(axis.ticks.y = element_blank(),\n        axis.text.y = element_blank()) +\n  labs(x = \"RMSE\", y = element_blank()) -&gt; rmse.plot\nrmse.plot\n\n\n\n\n\n\n\nCode\nacc.metric %&gt;% \n  group_by(.model, MPE) %&gt;% \n  ggplot(aes(x = MPE, \n             xend = 0, \n             y = reorder(.model, desc(abs(MPE))), \n             yend=.model,\n             label=round(MPE, 2))) +\n  theme_minimal() +\n  geom_segment() +\n  geom_point() +\n  geom_text(nudge_x = 0.5, nudge_y = 0.2,\n            size = 3) +\n  labs(y = \"\", x = \"MPE\") -&gt; mpe.plot\nmpe.plot\n\n\n\n\n\n\n\nCode\nlibrary(ggpubr)\n\n\n\n\nCode\nggarrange(mpe.plot, rmse.plot,\n                    ncol = 2, nrow = 1) |&gt;\n  annotate_figure(top = text_grob(\"Forecast Accuracy Metrics\"))"
  },
  {
    "objectID": "ETS.html#conclusion",
    "href": "ETS.html#conclusion",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nCode\nacc.metric |&gt;\n  arrange(RMSE, MPE)\n\n\n        .model     RMSE         ME        MPE\n1    ARIMA.reg 2366.673  -111.5811  -1.292216\n2        ARIMA 2638.318  -566.9333  -3.406917\n3   Regression 2927.642 -1517.8173  -7.792288\n4 Season_Naive 3322.937 -1733.2903  -8.803675\n5          ETS 3823.632 -2821.2435 -13.413444\n6       ETSopt 3863.003 -2876.8609 -13.679450\n\n\nAs we can see from the evaluation metrics, the ARIMA with external regressors is the best performing model, in terms of both variance (RMSE) and bias (MPE)."
  },
  {
    "objectID": "ml_procurement.html",
    "href": "ml_procurement.html",
    "title": "Procurement Contracts",
    "section": "",
    "text": "I use ML techniques to predict and classify the value of the Procurement Contracts."
  },
  {
    "objectID": "ml_procurement.html#data",
    "href": "ml_procurement.html#data",
    "title": "Procurement Contracts",
    "section": "Data",
    "text": "Data\n\n\nCode\nProcurement &lt;- read.csv(\"Procurement.csv\", header=TRUE, sep=\";\")\nView(Procurement)"
  },
  {
    "objectID": "ml_procurement.html#libraries",
    "href": "ml_procurement.html#libraries",
    "title": "Procurement Contracts",
    "section": "Libraries",
    "text": "Libraries\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ tibble  3.1.8     ✔ purrr   1.0.1\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(Ecdat)\n\n\nLoading required package: Ecfun\n\nAttaching package: 'Ecfun'\n\nThe following object is masked from 'package:base':\n\n    sign\n\n\nAttaching package: 'Ecdat'\n\nThe following object is masked from 'package:datasets':\n\n    Orange\n\n\nCode\nlibrary(glmnet)\n\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-6\n\n\nCode\nlibrary(Matrix)"
  },
  {
    "objectID": "ml_procurement.html#stats",
    "href": "ml_procurement.html#stats",
    "title": "Procurement Contracts",
    "section": "Stats",
    "text": "Stats\n\n\nCode\nsummary(Procurement$cri_wb) #NA's 86,535\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    0.00    0.25    0.34    0.50    1.00   86535 \n\n\nCode\nprint(86535/248000) #percent of total obs. Will impute later\n\n\n[1] 0.3489315\n\n\nCode\nprint(248000-86535) #n of total obs\n\n\n[1] 161465"
  },
  {
    "objectID": "ml_procurement.html#cleaning",
    "href": "ml_procurement.html#cleaning",
    "title": "Procurement Contracts",
    "section": "Cleaning",
    "text": "Cleaning\n\n\nCode\nProcurement2 = Procurement %&gt;% \n  filter(!is.na(cri_wb))\nsummary(Procurement2)\n\n\n country_wb_string    country             WBCode          country_iso3      \n Length:161465      Length:161465      Length:161465      Length:161465     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n country_iso2           fyear           year         region         \n Length:161465      Min.   :1998   Min.   :1997   Length:161465     \n Class :character   1st Qu.:2000   1st Qu.:1999   Class :character  \n Mode  :character   Median :2002   Median :2001   Mode  :character  \n                    Mean   :2002   Mean   :2002                     \n                    3rd Qu.:2004   3rd Qu.:2004                     \n                    Max.   :2009   Max.   :2009                     \n                                                                    \n    pr_id             pr_name          pr_loannumber      cft_methodtype    \n Length:161465      Length:161465      Length:161465      Length:161465     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  cft_sector          ca_type            ca_title            ca_id          \n Length:161465      Length:161465      Length:161465      Length:161465     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  ca_nrbidsrec    ca_nrbidscons      ca_contract_value_original\n Min.   :  1.00   Length:161465      Length:161465             \n 1st Qu.:  2.00   Class :character   Class :character          \n Median :  3.00   Mode  :character   Mode  :character          \n Mean   :  5.02                                                \n 3rd Qu.:  6.00                                                \n Max.   :824.00                                                \n NA's   :55151                                                 \n ca_contract_value   lca_contract_value ca_contract_valuec ca_signdate       \n Min.   :0.000e+00   Min.   :-3.632     Length:161465      Length:161465     \n 1st Qu.:5.608e+04   1st Qu.:10.935     Class :character   Class :character  \n Median :2.297e+05   Median :12.345     Mode  :character   Mode  :character  \n Mean   :1.876e+06   Mean   :12.287                                          \n 3rd Qu.:8.755e+05   3rd Qu.:13.683                                          \n Max.   :2.132e+09   Max.   :21.480                                          \n NA's   :13          NA's   :13                                              \n   ca_date           ca_bids_all         ca_bids       ca_procedure      \n Length:161465      Min.   :  1.000   Min.   : 1.000   Length:161465     \n Class :character   1st Qu.:  1.000   1st Qu.: 1.000   Class :character  \n Mode  :character   Median :  2.000   Median : 2.000   Mode  :character  \n                    Mean   :  3.918   Mean   : 3.701                     \n                    3rd Qu.:  4.000   3rd Qu.: 4.000                     \n                    Max.   :993.000   Max.   :50.000                     \n                                                                         \n  ca_signper        ca_signper_corr  ca_signper_corr5   ca_signper_corr25\n Length:161465      Min.   :  0.00   Length:161465      Min.   : 1.00    \n Class :character   1st Qu.: 10.00   Class :character   1st Qu.: 8.00    \n Mode  :character   Median : 27.00   Mode  :character   Median :16.00    \n                    Mean   : 48.92                      Mean   :15.13    \n                    3rd Qu.: 57.00                      3rd Qu.:22.00    \n                    Max.   :728.00                      Max.   :26.00    \n                    NA's   :19568                                        \n    w_name           w_country             w_id             anb_name        \n Length:161465      Length:161465      Length:161465      Length:161465     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n    SuppAwd       b2_country         b3_country         b4_country       \n Min.   :1.000   Length:161465      Length:161465      Length:161465     \n 1st Qu.:1.000   Class :character   Class :character   Class :character  \n Median :1.000   Mode  :character   Mode  :character   Mode  :character  \n Mean   :1.049                                                           \n 3rd Qu.:1.000                                                           \n Max.   :7.000                                                           \n                                                                         \n  ppp_original        ppp          country_st            anb_id     \n Min.   :0.108   Min.   :0.1077   Length:161465      Min.   :    1  \n 1st Qu.:0.236   1st Qu.:0.2363   Class :character   1st Qu.: 3775  \n Median :0.306   Median :0.3107   Mode  :character   Median : 6902  \n Mean   :0.330   Mean   :0.3310                      Mean   : 7556  \n 3rd Qu.:0.386   3rd Qu.:0.3865                      3rd Qu.:11454  \n Max.   :1.086   Max.   :1.0864                      Max.   :15600  \n NA's   :6868    NA's   :11                          NA's   :366    \n procedure_type        singleb         prop_bidnr      corr_proc        \n Length:161465      Min.   :0.0000   Min.   :0.0004   Length:161465     \n Class :character   1st Qu.:0.0000   1st Qu.:0.0625   Class :character  \n Mode  :character   Median :0.0000   Median :0.2500   Mode  :character  \n                    Mean   :0.4147   Mean   :0.4676                     \n                    3rd Qu.:1.0000   3rd Qu.:1.0000                     \n                    Max.   :1.0000   Max.   :1.0000                     \n                                                                        \n  corr_proc3         corr_signp        corr_signp1         corr_signp2    \n Length:161465      Length:161465      Length:161465      Min.   :0.0000  \n Class :character   Class :character   Class :character   1st Qu.:0.0000  \n Mode  :character   Mode  :character   Mode  :character   Median :0.0000  \n                                                          Mean   :0.2533  \n                                                          3rd Qu.:1.0000  \n                                                          Max.   :1.0000  \n                                                                          \n  corr_signp3      corr_signp4      corr_proc31     corr_proc32   \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.000  \n Median :0.0000   Median :0.0000   Median :1.000   Median :0.000  \n Mean   :0.1148   Mean   :0.1212   Mean   :0.556   Mean   :0.228  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:1.000   3rd Qu.:0.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.000   Max.   :1.000  \n                                                                  \n  corr_proc33         nrc           w_iso             sec_score    \n Min.   :0.000   Min.   :    2   Length:161465      Min.   :31.38  \n 1st Qu.:0.000   1st Qu.: 1707   Class :character   1st Qu.:50.11  \n Median :0.000   Median : 2752   Mode  :character   Median :53.92  \n Mean   :0.216   Mean   : 4989                      Mean   :55.61  \n 3rd Qu.:0.000   3rd Qu.: 6202                      3rd Qu.:58.00  \n Max.   :1.000   Max.   :15701                      Max.   :92.00  \n                                                    NA's   :79897  \n sec_score_max      taxhav          taxhav_fixed         taxhav3         \n Min.   :31.38   Length:161465      Length:161465      Length:161465     \n 1st Qu.:51.00   Class :character   Class :character   Class :character  \n Median :54.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :56.01                                                           \n 3rd Qu.:60.00                                                           \n Max.   :92.00                                                           \n NA's   :84096                                                           \n  taxhav3bi             fsuppl          taxhav31        taxhav32     \n Length:161465      Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n Class :character   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Mode  :character   Median :0.0000   Median :0.000   Median :0.0000  \n                    Mean   :0.2568   Mean   :0.237   Mean   :0.0148  \n                    3rd Qu.:1.0000   3rd Qu.:0.000   3rd Qu.:0.0000  \n                    Max.   :1.0000   Max.   :1.000   Max.   :1.0000  \n                                                                     \n    taxhav33           taxhav34          cri_wb      \n Min.   :0.000000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.000000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.000000   Median :1.0000   Median :0.2500  \n Mean   :0.005017   Mean   :0.7432   Mean   :0.3419  \n 3rd Qu.:0.000000   3rd Qu.:1.0000   3rd Qu.:0.5000  \n Max.   :1.000000   Max.   :1.0000   Max.   :1.0000"
  },
  {
    "objectID": "ml_procurement.html#analysis-of-variables",
    "href": "ml_procurement.html#analysis-of-variables",
    "title": "Procurement Contracts",
    "section": "Analysis of Variables",
    "text": "Analysis of Variables\nSeeing unique values of categorical vars and descriptive stats of numerical ones.\n\n\nCode\nunique(Procurement2$cft_methodtype)\n\n\n [1] \"National Competitive Bidding\"                 \n [2] \"Single Source Selection\"                      \n [3] \"International Competitive Bidding\"            \n [4] \"Direct Contracting\"                           \n [5] \"Quality And Cost-Based Selection\"             \n [6] \"Limited International Bidding\"                \n [7] \"National Shopping\"                            \n [8] \"International Shopping\"                       \n [9] \"Selection Based On Consultant's Qualification\"\n[10] \"Least Cost Selection\"                         \n[11] \"Quality Based Selection\"                      \n[12] \"Individual\"                                   \n[13] \"Selection Under a Fixed Budget\"               \n[14] \"CQS\"                                          \n[15] \"SHOP\"                                         \n\n\n\n\nCode\nunique(Procurement2$cft_sector)\n\n\n [1] \"Public admin, Law\"    \"Health & social serv\" \"Education\"           \n [4] \"Water/sanit/fld prot\" \"Industry and trade\"   \"Transportation\"      \n [7] \"Finance\"              \"Agriculture\"          \"Energy & mining\"     \n[10] \"Info & communication\" \"(H)\"                  \"(H)Multisector\"      \n[13] \"\"                     \"(H)Priv Sector Dev\"  \n\n\nCode\ntable(Procurement2$cft_sector)\n\n\n\n                                      (H)       (H)Multisector \n                  61                   66                   28 \n  (H)Priv Sector Dev          Agriculture            Education \n                   1                13494                15424 \n     Energy & mining              Finance Health & social serv \n               11106                 4878                25301 \n  Industry and trade Info & communication    Public admin, Law \n                7501                 2067                46307 \n      Transportation Water/sanit/fld prot \n               19215                16016 \n\n\nSector can be divided into dummies later\n\n\nCode\nunique(Procurement2$ca_type)\n\n\n[1] \"Civil Works\"         \"Consultant Services\" \"Goods\"              \n\n\n\n\nCode\n#N of bids recieved\nunique(Procurement2$ca_nrbidsrec)\n\n\n [1]   6   3   1   5   2   4  NA   7  17  19  10  12   8  11  16   9  21  13  14\n[20]  15  23  27  18 250  25  26  20  22  28  24  40 109  35  70  50  38  36 100\n[39]  56  33  31  58  57  48  45  30  41  79  39  67  65  32  47  54  29  76  34\n[58] 133  64  52  55  78  80 105  44  71 230  43  87  82  69 824  37  46 101 170\n[77] 149\n\n\nCode\nsummary(Procurement2$ca_nrbidsrec)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1.00    2.00    3.00    5.02    6.00  824.00   55151 \n\n\n\n\nCode\n#N of bids considered\n#Was a character class, so transformed into a numerical one\n#Empty spaces treated as if 0\nunique(Procurement2$ca_nrbidscons)\n\n\n [1] \"\"    \"1\"   \"2\"   \"3\"   \"5\"   \"4\"   \"7\"   \"6\"   \"88\"  \"13\"  \"8\"   \"12\" \n[13] \"11\"  \"9\"   \"23\"  \"15\"  \"113\" \"143\" \"22\"  \"51\"  \"101\" \"37\"  \"993\" \"102\"\n[25] \"14\"  \"16\"  \"10\"  \"141\" \"886\" \"682\" \"986\" \"20\"  \"21\"  \"31\"  \"26\"  \"17\" \n[37] \"60\" \n\n\nCode\nsummary(Procurement2$ca_nrbidscons)\n\n\n   Length     Class      Mode \n   161465 character character \n\n\nCode\nProcurement2 = Procurement2 %&gt;% \n  mutate(bidscons = as.numeric(ca_nrbidscons)) %&gt;% \n  mutate(bidscons = ifelse(is.na(bidscons), 0, bidscons))\n\n\n\n\nCode\n#signature period from 14 to 93 days\nProcurement2 = Procurement2 %&gt;% \n  mutate(corr_signp1 = as.numeric(corr_signp1))\n\nsummary(Procurement2$corr_signp1)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  1.0000  0.5107  1.0000  1.0000 \n\n\n\n\nCode\n#Value of the contract\nProcurement2 = Procurement2 %&gt;% \n  mutate(cntrct_val = as.numeric(ca_contract_value))\n\nsummary(Procurement2$ca_contract_value)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.      NA's \n0.000e+00 5.608e+04 2.297e+05 1.876e+06 8.755e+05 2.132e+09        13 \n\n\nCode\nsummary(Procurement2$cntrct_val)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.      NA's \n0.000e+00 5.608e+04 2.297e+05 1.876e+06 8.755e+05 2.132e+09        13 \n\n\n\n\nCode\n#Value of the contract logged\nsummary(Procurement2$lca_contract_value)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n -3.632  10.935  12.345  12.287  13.683  21.480      13 \n\n\n\n\nCode\n#Contract Value Categories\nunique(Procurement2$ca_contract_valuec)\n\n\n[1] \"50.000-199.999\" \"200.000-\"       \"0-24.999\"       \"25.000-49.999\" \n[5] \"\"              \n\n\nCode\nsummary(Procurement2$ca_contract_valuec)\n\n\n   Length     Class      Mode \n   161465 character character \n\n\nCode\ntable(Procurement2$ca_contract_valuec)\n\n\n\n                     0-24.999       200.000-  25.000-49.999 50.000-199.999 \n            13          23693          85200          14064          38495 \n\n\n\n\nCode\n#Main procurement type category\nunique(Procurement2$ca_procedure)\n\n\n[1] \"open\"                \"single source\"       \"consultancy,cost\"   \n[4] \"restricted\"          \"consultancy,quality\"\n\n\nCode\ntable(Procurement2$ca_procedure)\n\n\n\n   consultancy,cost consultancy,quality                open          restricted \n              25508                9372               89770                7637 \n      single source \n              29178 \n\n\n\n\nCode\n#Signature Period (probably in days)\n#Was character class\nProcurement2 = Procurement2 %&gt;% \n  mutate(signper = as.numeric(ca_signper))\n\nsummary(Procurement2$signper)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.      NA's \n-29188.00      6.00     23.00     34.83     53.00   2076.00      4436 \n\n\n\n\nCode\n#Unclear\nunique(Procurement2$SuppAwd)\n\n\n[1] 1 2 3 4 6 5 7\n\n\nCode\nsummary(Procurement2$SuppAwd)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.000   1.049   1.000   7.000 \n\n\nCode\ntable(Procurement2$SuppAwd)\n\n\n\n     1      2      3      4      5      6      7 \n155524   4462   1179    199     46     45     10 \n\n\n\n\nCode\n#Supplier country secrecy score\nsummary(Procurement2$sec_score)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  31.38   50.11   53.92   55.61   58.00   92.00   79897 \n\n\n\n\nCode\nunique(Procurement2$procedure_type)\n\n\n [1] \"National Competitive Bidding\"                 \n [2] \"Single Source Selection\"                      \n [3] \"International Competitive Bidding\"            \n [4] \"Direct Contracting\"                           \n [5] \"Quality And Cost-Based Selection\"             \n [6] \"Limited International Bidding\"                \n [7] \"National Shopping\"                            \n [8] \"International Shopping\"                       \n [9] \"Selection Based On Consultant's Qualification\"\n[10] \"Least Cost Selection\"                         \n[11] \"Quality Based Selection\"                      \n[12] \"Individual\"                                   \n[13] \"Selection Under a Fixed Budget\"               \n\n\nCode\ntable(Procurement2$procedure_type)\n\n\n\n                           Direct Contracting \n                                         5205 \n                                   Individual \n                                         9782 \n            International Competitive Bidding \n                                        48500 \n                       International Shopping \n                                         4004 \n                         Least Cost Selection \n                                          796 \n                Limited International Bidding \n                                         1896 \n                 National Competitive Bidding \n                                        41270 \n                            National Shopping \n                                         1737 \n             Quality And Cost-Based Selection \n                                        23916 \n                      Quality Based Selection \n                                         2354 \nSelection Based On Consultant's Qualification \n                                         7018 \n               Selection Under a Fixed Budget \n                                          796 \n                      Single Source Selection \n                                        14191 \n\n\n\n\nCode\n#Whether tax haven or not\nunique(Procurement2$taxhav)\n\n\n[1] \"domestic supplier\" \"NO tax haven\"      \"tax haven\"        \n\n\nCode\ntable(Procurement2$taxhav)\n\n\n\ndomestic supplier      NO tax haven         tax haven \n           119994             38272              3199 \n\n\n\n\nCode\nunique(Procurement2$taxhav_fixed)\n\n\n[1] \"domestic supplier\" \"NO tax haven\"      \"tax haven\"        \n\n\n\n\nCode\nunique(Procurement2$taxhav3bi)\n\n\n[1] \"domestic supplier\"                   \n[2] \"NO tax haven & tax haven,large state\"\n[3] \"tax haven, small state\"              \n\n\n\n\nCode\n#Corruption Risk Index, according to GTI\nsummary(Procurement2$cri_wb)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.2500  0.3419  0.5000  1.0000 \n\n\nCode\nunique(Procurement2$cri_wb)\n\n\n[1] 0.00 0.25 0.75 0.50 1.00\n\n\nCode\ntable(Procurement2$cri_wb)\n\n\n\n    0  0.25   0.5  0.75     1 \n42239 50405 36622 31594   605 \n\n\n\n\nCode\n#creating cri_wb character var for plotting\nProcurement2 = Procurement2 %&gt;% \n  mutate(cri_wb.ch = as.character(cri_wb))"
  },
  {
    "objectID": "ml_procurement.html#plot",
    "href": "ml_procurement.html#plot",
    "title": "Procurement Contracts",
    "section": "Plot",
    "text": "Plot\n\n\nCode\n# plot cri_wb\nplot(density(Procurement2$cri_wb))\n\n\n\n\n\nCode\nplot(hist(Procurement2$cri_wb))\n\n\n\n\n\n\n\n\n\n\nCode\n# Plot distribution\nProcurement2 %&gt;% \n  ggplot(aes(x=cri_wb, colour=region)) + \n  geom_density()\n\n\n\n\n\n\n\nCode\n#Plot by cri_wb and region\nProcurement2 %&gt;%  \n  group_by(cri_wb.ch, region) %&gt;%  \n  summarize(Count = n()) %&gt;% \n  ggplot(aes(x=region, y=Count, fill=cri_wb.ch)) + \n  geom_bar(stat='identity', position= \"dodge\")\n\n\n`summarise()` has grouped output by 'cri_wb.ch'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\nCode\n#Plot of lca_contract_value\nplot(density(na.omit(Procurement2$lca_contract_value)))\n\n\n\n\n\n\n\nCode\n#Plot lca_contract_value by region\nProcurement2 %&gt;% \n  ggplot(aes(x=lca_contract_value, color=region)) + \n  geom_density()\n\n\nWarning: Removed 13 rows containing non-finite values (stat_density).\n\n\n\n\n\nWe can observe some differences, for example between regions like Sub-Saharan Africa and Latin America.\n\n\nCode\n#Plot lca_contract_value by sector\nProcurement2 %&gt;% \n  ggplot(aes(x=lca_contract_value, color=cft_sector)) + \n  geom_density()\n\n\nWarning: Removed 13 rows containing non-finite values (stat_density).\n\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\nAgain some differences are seen between for example Energy&Mining and Law, which makes sense, since Mining requires more costly equipment. The sector without a name seems to be an outlier almost.\n\n\nCode\n#Plot average lca_contract_value over time\nProcurement2 %&gt;% \n  group_by(year, lca_contract_value) %&gt;% \n  ggplot(aes(y=lca_contract_value, x=year)) +\n  stat_summary(fun = \"mean\", geom = \"line\")\n\n\nWarning: Removed 13 rows containing non-finite values (stat_summary).\n\n\n\n\n\nGenerally increasing trend of average value per contract."
  },
  {
    "objectID": "ml_procurement.html#regression",
    "href": "ml_procurement.html#regression",
    "title": "Procurement Contracts",
    "section": "Regression",
    "text": "Regression\nI run a simple regression with variables of interest to see if anything interesting happens\n\n\nCode\n#Simple Regression for the logged Value\nlr1 = lm(lca_contract_value ~ region + ca_type + ca_bids_all + ca_procedure + singleb + corr_signp2 + corr_signp3 + corr_signp4 + nrc + taxhav_fixed + cri_wb, data = Procurement2)\nsummary(lr1)\n\n\n\nCall:\nlm(formula = lca_contract_value ~ region + ca_type + ca_bids_all + \n    ca_procedure + singleb + corr_signp2 + corr_signp3 + corr_signp4 + \n    nrc + taxhav_fixed + cri_wb, data = Procurement2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.4957  -1.2318   0.0867   1.2818   8.4553 \n\nCoefficients: (1 not defined because of singularities)\n                                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      1.358e+01  3.544e-02 383.279  &lt; 2e-16 ***\nregionEAP                       -5.604e-02  1.851e-02  -3.028  0.00247 ** \nregionECA                       -2.158e-01  1.546e-02 -13.959  &lt; 2e-16 ***\nregionLCR                       -5.246e-01  1.536e-02 -34.153  &lt; 2e-16 ***\nregionMNA                        8.887e-02  2.102e-02   4.227 2.37e-05 ***\nregionSAR                       -5.903e-02  2.073e-02  -2.848  0.00441 ** \nca_typeConsultant Services      -1.443e+00  3.071e-02 -46.986  &lt; 2e-16 ***\nca_typeGoods                    -7.299e-01  1.279e-02 -57.051  &lt; 2e-16 ***\nca_bids_all                      6.982e-04  3.591e-04   1.945  0.05184 .  \nca_procedureconsultancy,quality -9.478e-01  2.372e-02 -39.965  &lt; 2e-16 ***\nca_procedureopen                -6.071e-01  3.322e-02 -18.277  &lt; 2e-16 ***\nca_procedurerestricted          -1.573e+00  3.941e-02 -39.906  &lt; 2e-16 ***\nca_proceduresingle source       -7.534e-01  1.824e-02 -41.302  &lt; 2e-16 ***\nsingleb                         -1.965e-01  1.232e-02 -15.949  &lt; 2e-16 ***\ncorr_signp2                     -2.161e-01  1.201e-02 -17.986  &lt; 2e-16 ***\ncorr_signp3                      2.122e-01  1.592e-02  13.331  &lt; 2e-16 ***\ncorr_signp4                     -5.635e-01  1.590e-02 -35.452  &lt; 2e-16 ***\nnrc                              4.125e-05  1.467e-06  28.117  &lt; 2e-16 ***\ntaxhav_fixedNO tax haven         8.181e-01  1.238e-02  66.078  &lt; 2e-16 ***\ntaxhav_fixedtax haven            8.470e-01  3.503e-02  24.176  &lt; 2e-16 ***\ncri_wb                                  NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.935 on 161432 degrees of freedom\n  (13 observations deleted due to missingness)\nMultiple R-squared:  0.1659,    Adjusted R-squared:  0.1658 \nF-statistic:  1690 on 19 and 161432 DF,  p-value: &lt; 2.2e-16\n\n\nAdjusted R-squared is about 0.17, whereas cri_wb is dropped because of singularity.\n\n\nCode\ndeviance(lr1)\n\n\n[1] 604717.7\n\n\nRSS is quite big.\nThe variable for cri_wb was omitted due to singularities. Perhaps creating a dummy might change it.\n\n\nCode\n#Creating cri_wb dummy\n#0.00 0.25 0.75 0.50 1.00\nProcurement2 = Procurement2 %&gt;% \n  mutate(cri_wb.d0 = ifelse(cri_wb==0, 1, 0)) %&gt;% \n  mutate(cri_wb.d25 = ifelse(cri_wb==0.25, 1, 0)) %&gt;%\n  mutate(cri_wb.d50 = ifelse(cri_wb==0.5, 1, 0)) %&gt;%\n  mutate(cri_wb.d75 = ifelse(cri_wb==0.75, 1, 0))\n\n\nI am creating several dummies for categorical vars.\n\n\nCode\n#Creating cft_sector dummmies, omitting \"\"\nProcurement2 = Procurement2 %&gt;% \n  mutate(P = ifelse(cft_sector==\"Public admin, Law\", 1, 0)) %&gt;% \n  mutate(Health = ifelse(cft_sector==\"Health & social serv\", 1, 0)) %&gt;%\n  mutate(Edu = ifelse(cft_sector==\"Education\", 1, 0)) %&gt;%\n  mutate(W = ifelse(cft_sector==\"Water/sanit/fld prot\", 1, 0)) %&gt;%\n  mutate(I = ifelse(cft_sector==\"Industry and trade\", 1, 0)) %&gt;%\n  mutate(Tran = ifelse(cft_sector==\"Transportation\", 1, 0)) %&gt;% \n  mutate(Fin = ifelse(cft_sector==\"Finance\", 1, 0)) %&gt;% \n  mutate(A = ifelse(cft_sector==\"Agriculture\", 1, 0)) %&gt;%\n  mutate(Ener = ifelse(cft_sector==\"Energy & mining\", 1, 0)) %&gt;%\n  mutate(Info = ifelse(cft_sector==\"Info & communication\", 1, 0)) %&gt;%\n  mutate(H = ifelse(cft_sector==\"(H)\", 1, 0)) %&gt;%\n  mutate(Multi = ifelse(cft_sector==\"(H)Multisector\", 1, 0)) %&gt;% \n  mutate(Priv = ifelse(cft_sector==\"(H)Priv Sector Dev\", 1, 0))\nunique(Procurement2$cft_sector)\n\n\n [1] \"Public admin, Law\"    \"Health & social serv\" \"Education\"           \n [4] \"Water/sanit/fld prot\" \"Industry and trade\"   \"Transportation\"      \n [7] \"Finance\"              \"Agriculture\"          \"Energy & mining\"     \n[10] \"Info & communication\" \"(H)\"                  \"(H)Multisector\"      \n[13] \"\"                     \"(H)Priv Sector Dev\"  \n\n\n\n\nCode\nProcurement2 = Procurement2 %&gt;% \n  mutate(AFR = ifelse(region==\"AFR\", 1, 0)) %&gt;% \n  mutate(EAP = ifelse(region==\"EAP\", 1, 0)) %&gt;%\n  mutate(ECA = ifelse(region==\"ECA\", 1, 0)) %&gt;%\n  mutate(LCR = ifelse(region==\"LCR\", 1, 0)) %&gt;%\n  mutate(MNA = ifelse(region==\"MNA\", 1, 0)) %&gt;%\n  mutate(SAR = ifelse(region==\"SAR\", 1, 0))\n\n\n\n\nCode\n#Creating ca_type dummies, omitting Civil Works\nProcurement2 = Procurement2 %&gt;% \n  mutate(ca_Consul.Service = ifelse(ca_type==\"Consultant Services\", 1, 0)) %&gt;% \n  mutate(ca_Goods = ifelse(ca_type==\"Goods\", 1, 0))\n\n\n\n\nCode\n#Creating taxhav_fixed dummies, omitting domestric supplier\nProcurement2 = Procurement2 %&gt;% \n  mutate(not.taxhaven = ifelse(taxhav_fixed==\"NO tax haven\", 1, 0)) %&gt;% \n  mutate(taxhaven = ifelse(taxhav_fixed==\"tax haven\", 1, 0))\n\n\n\n\nCode\n#Creating ca_procedure dummies, omitting cost\nProcurement2 = Procurement2 %&gt;% \n  mutate(ca_proc_open = ifelse(ca_procedure==\"open\", 1, 0)) %&gt;% \n  mutate(ca_proc_quality = ifelse(ca_procedure==\"consultancy,quality\", 1, 0)) %&gt;%\n  mutate(ca_proc_source = ifelse(ca_procedure==\"single source\", 1, 0)) %&gt;%\n  mutate(ca_proc_restricted = ifelse(ca_procedure==\"restricted\", 1, 0))\n\n\nFor regression of the value per contract I am selecting the following variables: 1. Dummy variables for region, because there was a slight difference between the distributions of value by regions. 4. Procurement Sector, thinking some sectors tend to attract more valuable tender contracts. 2. Dummy for ca_type (Procurement Category) since the type of services can affect the value 3. Dummy for for n of bidders since higher n of bidders should indicate a higher price of the contract. 4. Dummy for contract award procedure associated with corruption. 5. Dummy for single bidder might indicate corrupted case, which can imply high value of the contract. 6. Period between when the contract was awarded and signed. 7. Number of contracts per country. 8. Whether the supplier is a domestic one or a tax haven. Foreign tax havens might indicate corruption and thus higher price. 9. The Corruption Index might be associated with the higher value of the contract.\n\n\nCode\n#Creating only predictor and response dataset Procurement3\nProcurement3 = Procurement2 %&gt;% \n  select(\"ca_contract_valuec\", \"lca_contract_value\", \"AFR\", \"EAP\", \"ECA\", \"LCR\", \"MNA\", \"SAR\", \"ca_Consul.Service\", \"ca_Goods\", \"ca_bids_all\", \"ca_proc_open\", \"ca_proc_quality\", \"ca_proc_source\", \"ca_proc_restricted\", \"singleb\", \"corr_signp1\", \"corr_signp2\", \"corr_signp3\", \"nrc\", \"taxhaven\", \"not.taxhaven\", \"cri_wb.d0\", \"cri_wb.d25\", \"cri_wb.d50\", \"cri_wb.d75\", \"P\", \"Health\", \"Edu\", \"W\", \"I\", \"Tran\", \"Fin\", \"A\", \"Ener\", \"Info\", \"H\", \"Multi\", \"Priv\")\n\n\n\n\nCode\n#Saving Procurement3 as an excel dataset\nlibrary(writexl)\nwrite_xlsx(Procurement3, \"~/Desktop/BC_courses/BigDataEconometrics/Procurement3.xlsx\")\n\n\n\n\nCode\n#Omitting any missing values\nProcurement3 = na.omit(Procurement3)\n\n\n\n\nCode\n#Linear Regression with Procurement3\nlr2 = lm(lca_contract_value ~ ., data = Procurement3)\nsummary(lr2)\n\n\n\nCall:\nlm(formula = lca_contract_value ~ ., data = Procurement3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.7818  -0.6101  -0.0397   0.5108   7.0744 \n\nCoefficients: (2 not defined because of singularities)\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                       9.252e+00  1.382e-01  66.933  &lt; 2e-16 ***\nca_contract_valuec200.000-        4.692e+00  7.916e-03 592.686  &lt; 2e-16 ***\nca_contract_valuec25.000-49.999   1.543e+00  1.066e-02 144.820  &lt; 2e-16 ***\nca_contract_valuec50.000-199.999  2.570e+00  8.370e-03 307.066  &lt; 2e-16 ***\nAFR                               5.303e-02  1.071e-02   4.951 7.40e-07 ***\nEAP                               1.428e-01  9.106e-03  15.681  &lt; 2e-16 ***\nECA                               6.290e-02  1.089e-02   5.778 7.57e-09 ***\nLCR                               1.050e-01  1.050e-02  10.000  &lt; 2e-16 ***\nMNA                               1.890e-01  1.336e-02  14.146  &lt; 2e-16 ***\nSAR                                      NA         NA      NA       NA    \nca_Consul.Service                -3.435e-01  1.593e-02 -21.562  &lt; 2e-16 ***\nca_Goods                         -2.349e-01  6.761e-03 -34.744  &lt; 2e-16 ***\nca_bids_all                       1.069e-03  1.850e-04   5.782 7.40e-09 ***\nca_proc_open                      2.515e-01  4.851e-02   5.184 2.17e-07 ***\nca_proc_quality                  -1.505e-01  1.232e-02 -12.215  &lt; 2e-16 ***\nca_proc_source                   -6.125e-02  9.609e-03  -6.374 1.85e-10 ***\nca_proc_restricted               -2.147e-01  2.046e-02 -10.492  &lt; 2e-16 ***\nsingleb                          -2.465e-01  4.567e-02  -5.398 6.74e-08 ***\ncorr_signp1                       3.224e-01  4.605e-02   7.001 2.55e-12 ***\ncorr_signp2                       4.846e-02  8.781e-03   5.519 3.42e-08 ***\ncorr_signp3                       2.146e-01  1.054e-02  20.355  &lt; 2e-16 ***\nnrc                               1.587e-05  7.593e-07  20.903  &lt; 2e-16 ***\ntaxhaven                          1.119e-01  4.103e-02   2.728 0.006376 ** \nnot.taxhaven                      2.798e-01  6.462e-03  43.304  &lt; 2e-16 ***\ncri_wb.d0                        -6.546e-01  1.366e-01  -4.791 1.66e-06 ***\ncri_wb.d25                       -5.129e-01  9.156e-02  -5.602 2.12e-08 ***\ncri_wb.d50                       -2.489e-01  4.666e-02  -5.334 9.61e-08 ***\ncri_wb.d75                               NA         NA      NA       NA    \nP                                -2.145e-01  1.278e-01  -1.678 0.093383 .  \nHealth                           -9.958e-02  1.279e-01  -0.779 0.436187    \nEdu                              -9.031e-02  1.280e-01  -0.706 0.480466    \nW                                 2.929e-02  1.280e-01   0.229 0.819010    \nI                                -1.220e-01  1.282e-01  -0.952 0.341306    \nTran                              3.261e-01  1.280e-01   2.549 0.010814 *  \nFin                              -9.101e-02  1.285e-01  -0.708 0.478748    \nA                                -4.242e-02  1.280e-01  -0.331 0.740364    \nEner                              3.031e-01  1.281e-01   2.366 0.017991 *  \nInfo                             -1.693e-01  1.296e-01  -1.306 0.191443    \nH                                -6.562e-02  1.771e-01  -0.371 0.710976    \nMulti                            -8.459e-01  2.276e-01  -3.717 0.000202 ***\nPriv                              5.614e-01  1.005e+00   0.559 0.576349    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9966 on 161413 degrees of freedom\nMultiple R-squared:  0.7789,    Adjusted R-squared:  0.7788 \nF-statistic: 1.496e+04 on 38 and 161413 DF,  p-value: &lt; 2.2e-16\n\n\nTwo variables were dropped because of singularity still. Most of the remaining vars have high statistical significance."
  },
  {
    "objectID": "ml_procurement.html#ridge-regression-with-grid-search",
    "href": "ml_procurement.html#ridge-regression-with-grid-search",
    "title": "Procurement Contracts",
    "section": "Ridge Regression with Grid Search",
    "text": "Ridge Regression with Grid Search\n\n\nCode\n#Last check of the predictor dataset\nsummary(Procurement3)\n\n\n ca_contract_valuec lca_contract_value      AFR             EAP        \n Length:161452      Min.   :-3.632     Min.   :0.000   Min.   :0.0000  \n Class :character   1st Qu.:10.935     1st Qu.:0.000   1st Qu.:0.0000  \n Mode  :character   Median :12.345     Median :0.000   Median :0.0000  \n                    Mean   :12.287     Mean   :0.232   Mean   :0.1787  \n                    3rd Qu.:13.683     3rd Qu.:0.000   3rd Qu.:0.0000  \n                    Max.   :21.480     Max.   :1.000   Max.   :1.0000  \n      ECA              LCR              MNA               SAR        \n Min.   :0.0000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.00000   Median :0.0000  \n Mean   :0.1791   Mean   :0.1975   Mean   :0.07035   Mean   :0.1424  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n ca_Consul.Service    ca_Goods       ca_bids_all       ca_proc_open  \n Min.   :0.0000    Min.   :0.0000   Min.   :  1.000   Min.   :0.000  \n 1st Qu.:0.0000    1st Qu.:0.0000   1st Qu.:  1.000   1st Qu.:0.000  \n Median :0.0000    Median :0.0000   Median :  2.000   Median :1.000  \n Mean   :0.3645    Mean   :0.3459   Mean   :  3.918   Mean   :0.556  \n 3rd Qu.:1.0000    3rd Qu.:1.0000   3rd Qu.:  4.000   3rd Qu.:1.000  \n Max.   :1.0000    Max.   :1.0000   Max.   :993.000   Max.   :1.000  \n ca_proc_quality   ca_proc_source   ca_proc_restricted    singleb      \n Min.   :0.00000   Min.   :0.0000   Min.   :0.0000     Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000     1st Qu.:0.0000  \n Median :0.00000   Median :0.0000   Median :0.0000     Median :0.0000  \n Mean   :0.05805   Mean   :0.1806   Mean   :0.0473     Mean   :0.4147  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.0000     3rd Qu.:1.0000  \n Max.   :1.00000   Max.   :1.0000   Max.   :1.0000     Max.   :1.0000  \n  corr_signp1      corr_signp2      corr_signp3          nrc       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :    2  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 1707  \n Median :1.0000   Median :0.0000   Median :0.0000   Median : 2752  \n Mean   :0.5108   Mean   :0.2532   Mean   :0.1148   Mean   : 4990  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.: 6202  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :15701  \n    taxhaven       not.taxhaven     cri_wb.d0        cri_wb.d25    \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.000   Median :0.0000   Median :0.0000  \n Mean   :0.0198   Mean   :0.237   Mean   :0.2616   Mean   :0.3122  \n 3rd Qu.:0.0000   3rd Qu.:0.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n   cri_wb.d50       cri_wb.d75           P              Health      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.2268   Mean   :0.1956   Mean   :0.2868   Mean   :0.1567  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n      Edu                W                 I                Tran      \n Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.000  \n Median :0.00000   Median :0.00000   Median :0.00000   Median :0.000  \n Mean   :0.09553   Mean   :0.09919   Mean   :0.04646   Mean   :0.119  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.000  \n Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   Max.   :1.000  \n      Fin                A               Ener              Info       \n Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.00000   Median :0.0000   Median :0.00000   Median :0.0000  \n Mean   :0.03021   Mean   :0.0835   Mean   :0.06879   Mean   :0.0128  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.0000  \n Max.   :1.00000   Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n       H                 Multi                Priv        \n Min.   :0.0000000   Min.   :0.0000000   Min.   :0.0e+00  \n 1st Qu.:0.0000000   1st Qu.:0.0000000   1st Qu.:0.0e+00  \n Median :0.0000000   Median :0.0000000   Median :0.0e+00  \n Mean   :0.0004088   Mean   :0.0001734   Mean   :6.2e-06  \n 3rd Qu.:0.0000000   3rd Qu.:0.0000000   3rd Qu.:0.0e+00  \n Max.   :1.0000000   Max.   :1.0000000   Max.   :1.0e+00  \n\n\n\n\nCode\n#assigning index to each row\nindex = 1:nrow(Procurement3)\n\n\n\n\nCode\n#randomly taking 90% of the indices for training subset\nset.seed(12345) #for random\ntrain_index = sample(index, round(0.90*nrow(Procurement3)), replace = FALSE) #takes sample of 90% of rows without replacement\n\n\n\n\nCode\n#taking the remaining indices for test subset\ntest_index = setdiff(index, train_index) #takes untaken rows\n\n\n\n\nCode\n#creating train and test subsets of obs\ntrain_x = Procurement3[train_index, ] %&gt;% \n  select(-lca_contract_value, -ca_contract_valuec, -starts_with(\"cri_wb.d\")) #Procurement3[train_index, ] expression to use only training subset %&gt;% everything except price, price category, and corruption index\n\ntrain_y = Procurement3[train_index, ] %&gt;% \n  pull(lca_contract_value)\n\ntest_x = Procurement3[test_index, ] %&gt;% \n  select(-lca_contract_value, -ca_contract_valuec, -starts_with(\"cri_wb.d\"))\n\ntest_y = Procurement3[test_index, ] %&gt;% \n  pull(lca_contract_value)\n\n\n\n\nCode\n#putting subsets into matrices for the models\n#creates a model) matrix, e.g., by expanding factors to a set of dummy variables (depending on the contrasts) and expanding interactions similarly.\ntrain_matrix = model.matrix(train_y ~ ., data = train_x)\n\ntest_matrix = model.matrix(test_y ~ ., data = test_x)\n\n\n\n\nCode\n#Linear Regression\n#To run an unpenalized linear regression penalty, lambda is set to 0\nmodel_lm_ridge = glmnet(y = train_y, x = train_matrix, alpha = 0, lambda = 0)\n\n\n\n\nCode\n#Coefficients\ncoef(model_lm_ridge)\n\n\n35 x 1 sparse Matrix of class \"dgCMatrix\"\n                              s0\n(Intercept)         1.295066e+01\n(Intercept)         .           \nAFR                 1.740151e-01\nEAP                 5.540119e-02\nECA                -5.063949e-03\nLCR                -2.871303e-01\nMNA                 3.080603e-01\nSAR                 1.182342e-01\nca_Consul.Service  -1.243931e+00\nca_Goods           -6.270021e-01\nca_bids_all         9.673877e-04\nca_proc_open       -4.830663e-01\nca_proc_quality    -8.910647e-01\nca_proc_source     -7.194468e-01\nca_proc_restricted -1.445686e+00\nsingleb            -1.378996e-01\ncorr_signp1         5.353419e-01\ncorr_signp2         3.257312e-01\ncorr_signp3         7.423295e-01\nnrc                 3.892206e-05\ntaxhaven            8.405527e-01\nnot.taxhaven        8.169726e-01\nP                  -9.535187e-01\nHealth             -3.615957e-01\nEdu                -7.520389e-02\nW                   9.985832e-03\nI                  -6.340170e-01\nTran                4.970437e-01\nFin                -6.351149e-01\nA                   3.418690e-02\nEner                5.531834e-01\nInfo               -8.652731e-01\nH                  -1.507041e-01\nMulti              -5.725173e-01\nPriv                1.060161e+00\n\n\nCoefficients of this regression did not drop the two from the lr2, which might be because the estimates in glmnet and lm will never be exactly the same and one is the subset of the other.\n\n\nCode\n#RMSE for lm\n#compute the RMSE\npreds_lm = predict(model_lm_ridge, test_matrix)\n\nrmse_lm = sqrt(mean(preds_lm - test_y)^2)\n\n\n\n\nCode\n#Ridge regression with lambda 10\nmodel_ridge = glmnet(y = train_y, x = train_matrix, alpha = 0, lambda = 10)\n\n\n\n\nCode\n#compute the RMSE for Ridge\npreds = predict(model_ridge, test_matrix)\n\nrmse = sqrt(mean(preds - test_y)^2)\n\n\nRMSE of the first Ridge regression is bigger than that of the unregularized linear regression. Seems that the optimal lambda should be small.\n\n\nCode\n#finding optimal lambda\n#Function for picking the lowest RMSE\n#cv.glmnet: Does k-fold cross-validation for glmnet, produces a plot, and returns a value for lambda\nbest_model = cv.glmnet(train_matrix, train_y, alpha=0)\n# lambda that minimizes the MSE\nbest_model$lambda.min\n\n\n[1] 0.05474761\n\n\nThe optimal lambda is very close to 0.\n\n\nCode\n#Ridge regression with optimal lambda\n#use the new lambda\nnew_lambda = best_model$lambda.min\nnew_model_ridge = glmnet(y = train_y, x = train_matrix, alpha = 0, lambda = new_lambda)\n\n\n\n\nCode\n#new RMSE\n#check the RMSE\nnew_preds = predict(new_model_ridge, test_matrix)\n\nnew_rmse = sqrt(mean(new_preds - test_y)^2)\n\n\n\n\nCode\n#Comparing RMSE\nprint(rmse)\n\n\n[1] 0.01582965\n\n\nCode\nprint(rmse_lm)\n\n\n[1] 0.01447194\n\n\nCode\nprint(new_rmse)\n\n\n[1] 0.01409955\n\n\nThe RMSE for Ridge regression with the optimal parameter is smaller than that with lambda 0 just a little bit. Which means regularized regression is just a little more efficient than the simple linear regression.\n\n\nCode\n#Comparing RSS\nsse_lm = sum((preds_lm - test_y)^2)\nsse_ridge = sum((new_preds - test_y)^2)\n\nprint(sse_lm)\n\n\n[1] 56996.77\n\n\nCode\nprint(sse_ridge)\n\n\n[1] 57044.26\n\n\n\n\nCode\nlr3 = lm(train_y ~ ., data = train_x)\nsummary(lr3)\n\n\n\nCall:\nlm(formula = train_y ~ ., data = train_x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.9811  -1.1631   0.0784   1.2355   8.4591 \n\nCoefficients: (1 not defined because of singularities)\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         1.231e+01  2.672e-01  46.051  &lt; 2e-16 ***\nAFR                 5.563e-02  2.107e-02   2.640 0.008280 ** \nEAP                -6.235e-02  1.788e-02  -3.488 0.000488 ***\nECA                -1.228e-01  2.140e-02  -5.735 9.75e-09 ***\nLCR                -4.057e-01  2.058e-02 -19.717  &lt; 2e-16 ***\nMNA                 1.897e-01  2.630e-02   7.215 5.40e-13 ***\nSAR                        NA         NA      NA       NA    \nca_Consul.Service  -1.254e+00  3.123e-02 -40.153  &lt; 2e-16 ***\nca_Goods           -6.273e-01  1.324e-02 -47.388  &lt; 2e-16 ***\nca_bids_all         9.722e-04  3.647e-04   2.666 0.007686 ** \nca_proc_open       -4.938e-01  3.371e-02 -14.650  &lt; 2e-16 ***\nca_proc_quality    -8.915e-01  2.411e-02 -36.982  &lt; 2e-16 ***\nca_proc_source     -7.207e-01  1.854e-02 -38.863  &lt; 2e-16 ***\nca_proc_restricted -1.456e+00  3.999e-02 -36.409  &lt; 2e-16 ***\nsingleb            -1.378e-01  1.251e-02 -11.009  &lt; 2e-16 ***\ncorr_signp1         5.356e-01  1.612e-02  33.230  &lt; 2e-16 ***\ncorr_signp2         3.258e-01  1.724e-02  18.900  &lt; 2e-16 ***\ncorr_signp3         7.424e-01  2.055e-02  36.121  &lt; 2e-16 ***\nnrc                 3.891e-05  1.493e-06  26.061  &lt; 2e-16 ***\ntaxhaven            8.407e-01  3.551e-02  23.672  &lt; 2e-16 ***\nnot.taxhaven        8.168e-01  1.261e-02  64.770  &lt; 2e-16 ***\nP                  -1.791e-01  2.635e-01  -0.679 0.496886    \nHealth              4.127e-01  2.637e-01   1.565 0.117569    \nEdu                 6.990e-01  2.639e-01   2.649 0.008080 ** \nW                   7.842e-01  2.639e-01   2.972 0.002962 ** \nI                   1.401e-01  2.643e-01   0.530 0.596089    \nTran                1.271e+00  2.638e-01   4.818 1.45e-06 ***\nFin                 1.388e-01  2.648e-01   0.524 0.600193    \nA                   8.082e-01  2.639e-01   3.062 0.002199 ** \nEner                1.327e+00  2.641e-01   5.026 5.03e-07 ***\nInfo               -9.151e-02  2.669e-01  -0.343 0.731706    \nH                   6.233e-01  3.608e-01   1.728 0.084042 .  \nMulti               2.018e-01  4.622e-01   0.437 0.662471    \nPriv                1.833e+00  1.879e+00   0.976 0.329313    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.861 on 145274 degrees of freedom\nMultiple R-squared:  0.2276,    Adjusted R-squared:  0.2275 \nF-statistic:  1338 on 32 and 145274 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\npreds_lr3 = predict(lr3, test_x)\n\n\nWarning in predict.lm(lr3, test_x): prediction from a rank-deficient fit may be\nmisleading\n\n\nCode\nrmse_lr3 = sqrt(mean(preds_lr3 - test_y)^2)\nsum((preds_lr3 - test_y)^2)\n\n\n[1] 56998.57\n\n\nResidual Sums Squared are almost identical for both regressions. Given that, I’d suggest sticking to a simpler model, i.e. unpenalized Linear Regression Model."
  },
  {
    "objectID": "ml_procurement.html#corrected-ridge-without-cri_wb",
    "href": "ml_procurement.html#corrected-ridge-without-cri_wb",
    "title": "Procurement Contracts",
    "section": "Corrected Ridge without cri_wb",
    "text": "Corrected Ridge without cri_wb\nLater in the research I discovered that cri_wb caused multicollinearity, so I removed it and performed Ridge regression again\n\n\nCode\n#subsetting the data\n\nindex.r = 1:nrow(Procurement3) #indexing each row\n\nset.seed(12345) #for random\ntrain_index.r = sample(index.r, round(0.90*nrow(Procurement3)), replace = FALSE) #takes sample of 90% of rows without replacement\n\ntest_index.r = setdiff(index.r, train_index.r) #takes untaken rows\n\ntrain_x.r = Procurement3[train_index.r, ] %&gt;% \n  select(-lca_contract_value, -ca_contract_valuec, -starts_with(\"cri_wb.d\")) #Procurement3[train_index, ] expression to use only training subset %&gt;% everything except price, price category, and corruption index\n\ntrain_y.r = Procurement3[train_index.r, ] %&gt;% \n  pull(lca_contract_value)\n\ntest_x.r = Procurement3[test_index.r, ] %&gt;% \n  select(-lca_contract_value, -ca_contract_valuec, -starts_with(\"cri_wb.d\"))\n\ntest_y.r = Procurement3[test_index.r, ] %&gt;% \n  pull(lca_contract_value)\n\n#creates a model) matrix, e.g., by expanding factors to a set of dummy variables (depending on the contrasts) and expanding interactions similarly. The matrices are for glmnet.\ntrain_matrix.r = model.matrix(train_y.r ~ ., data = train_x.r)\n\ntest_matrix.r = model.matrix(test_y.r ~ ., data = test_x.r)\n\n\n\n\nCode\n#CORRECTED Linear Regression with glmnet\n#To run an unpenalized linear regression penalty, lambda is set to 0\nlm_ridge.c = glmnet(y = train_y.r, x = train_matrix.r, alpha = 0, lambda = 0)\n\n\n\n\nCode\n#RMSE and SSE for corrected lm glmnet\npreds_r = predict(lm_ridge.c, test_matrix.r)\n\nrmse_r = sqrt(mean(preds_r - test_y.r)^2)\nsse_r = sum((preds_r - test_y.r)^2)\n\nprint(rmse_r)\n\n\n[1] 0.01447194\n\n\nCode\nprint(sse_r)\n\n\n[1] 56996.77\n\n\n\n\nCode\n#finding optimal lambda\n\n#Function for picking the lowest RMSE\n#cv.glmnet: Does k-fold cross-validation for glmnet, produces a plot, and returns a value for lambda\nbest_model.r = cv.glmnet(train_matrix.r, train_y.r, alpha=0)\n# lambda that minimizes the MSE\nbest_model.r$lambda.min\n\n\n[1] 0.05474761\n\n\nThe optimal lambda is very close to 0.\n\n\nCode\n#Ridge regression with optimal lambda\n#use the new lambda\nnew_lambda.r = best_model.r$lambda.min\nlm_ridge.c2 = glmnet(y = train_y.r, x = train_matrix.r, alpha = 0, lambda = new_lambda.r)\n\n\n\n\nCode\n#new RMSE\n#check the RMSE\nnew_preds.r = predict(lm_ridge.c2, test_matrix.r)\n\nnew_rmse.r = sqrt(mean(new_preds.r - test_y.r)^2)\nnew_sse.r = sum((new_preds.r - test_y.r)^2)\n\nprint(new_rmse.r)\n\n\n[1] 0.01409955\n\n\nCode\nprint(new_sse.r)\n\n\n[1] 57044.26"
  },
  {
    "objectID": "ml_procurement.html#logit-for-contract-value-categories",
    "href": "ml_procurement.html#logit-for-contract-value-categories",
    "title": "Procurement Contracts",
    "section": "Logit for contract value categories",
    "text": "Logit for contract value categories\n\n\nCode\n#Creating contract_valuec dummy and Procurement4\nProcurement4 = Procurement3 %&gt;% \n  mutate(value.cat = ifelse(ca_contract_valuec==\"200.000-\", 1, 0)) %&gt;% \n  select(-c(ca_contract_valuec, lca_contract_value)) #I drop cols for value logged and category\n\n\n\n\nCode\n#simple logit model\nlog_model = glm(value.cat ~ ., family = binomial(link = \"logit\"), Procurement4)\nsummary(log_model)\n\n\n\nCall:\nglm(formula = value.cat ~ ., family = binomial(link = \"logit\"), \n    data = Procurement4)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1680  -1.0435   0.5946   0.9686   2.4301  \n\nCoefficients: (2 not defined because of singularities)\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         2.173e-01  3.725e-01   0.583 0.559751    \nAFR                -1.557e-01  2.348e-02  -6.632 3.32e-11 ***\nEAP                -2.572e-01  2.025e-02 -12.697  &lt; 2e-16 ***\nECA                -2.847e-01  2.384e-02 -11.941  &lt; 2e-16 ***\nLCR                -6.262e-01  2.302e-02 -27.201  &lt; 2e-16 ***\nMNA                -8.363e-02  2.911e-02  -2.873 0.004065 ** \nSAR                        NA         NA      NA       NA    \nca_Consul.Service  -1.132e+00  3.419e-02 -33.102  &lt; 2e-16 ***\nca_Goods           -4.248e-01  1.462e-02 -29.046  &lt; 2e-16 ***\nca_bids_all         3.636e-04  4.081e-04   0.891 0.372981    \nca_proc_open       -2.588e-01  1.072e-01  -2.414 0.015777 *  \nca_proc_quality    -7.728e-01  2.734e-02 -28.270  &lt; 2e-16 ***\nca_proc_source     -8.616e-01  2.107e-02 -40.888  &lt; 2e-16 ***\nca_proc_restricted -1.399e+00  4.401e-02 -31.799  &lt; 2e-16 ***\nsingleb            -4.436e-01  1.012e-01  -4.381 1.18e-05 ***\ncorr_signp1         8.260e-01  1.021e-01   8.090 5.98e-16 ***\ncorr_signp2         3.143e-01  1.952e-02  16.099  &lt; 2e-16 ***\ncorr_signp3         6.527e-01  2.323e-02  28.100  &lt; 2e-16 ***\nnrc                 2.967e-05  1.669e-06  17.774  &lt; 2e-16 ***\ntaxhaven            4.303e-01  9.095e-02   4.731 2.23e-06 ***\nnot.taxhaven        6.952e-01  1.412e-02  49.228  &lt; 2e-16 ***\ncri_wb.d0          -9.895e-01  3.031e-01  -3.265 0.001095 ** \ncri_wb.d25         -7.253e-01  2.031e-01  -3.572 0.000354 ***\ncri_wb.d50         -3.457e-01  1.035e-01  -3.341 0.000834 ***\ncri_wb.d75                 NA         NA      NA       NA    \nP                   3.103e-01  3.543e-01   0.876 0.381088    \nHealth              9.720e-01  3.544e-01   2.743 0.006092 ** \nEdu                 1.355e+00  3.545e-01   3.822 0.000132 ***\nW                   1.275e+00  3.545e-01   3.596 0.000323 ***\nI                   6.389e-01  3.550e-01   1.800 0.071878 .  \nTran                1.641e+00  3.545e-01   4.629 3.68e-06 ***\nFin                 7.281e-01  3.554e-01   2.049 0.040501 *  \nA                   1.378e+00  3.546e-01   3.885 0.000102 ***\nEner                1.716e+00  3.548e-01   4.837 1.32e-06 ***\nInfo                3.506e-01  3.573e-01   0.981 0.326468    \nH                   1.121e+00  4.383e-01   2.557 0.010551 *  \nMulti               2.200e+00  5.811e-01   3.786 0.000153 ***\nPriv                7.657e+00  2.667e+01   0.287 0.774001    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 223324  on 161451  degrees of freedom\nResidual deviance: 195143  on 161416  degrees of freedom\nAIC: 195215\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\nCode\n#creating train and test dataset\n\nindex.log = 1:nrow(Procurement4)\n\nset.seed(12345) #for random\ntrain_index.log = sample(index.log, round(0.90*nrow(Procurement4)), replace = FALSE) #takes sample of 90% of rows without replacement\n\ntest_index.log = setdiff(index.log, train_index.log) #takes remaining rows for test\n\n#creating train and test subsets of obs\ntrain_x.log = Procurement4[train_index.log, ] %&gt;% \n  select(-value.cat) #Procurement3[train_index.log, ] expression to use only training subset %&gt;% everything except price category\n\ntrain_y.log = Procurement4[train_index.log, ] %&gt;% \n  pull(value.cat) #pulls only value.cat\n\ntest_x.log = Procurement4[test_index.log, ] %&gt;% \n  select(-value.cat)\n\ntest_y.log = Procurement4[test_index.log, ] %&gt;% \n  pull(value.cat)\n\n\n\n\nCode\n#logit model training\n\nlog_model_train = glm(train_y.log ~ ., family = binomial(link = \"logit\"), train_x.log)\nsummary(log_model_train)\n\n\n\nCall:\nglm(formula = train_y.log ~ ., family = binomial(link = \"logit\"), \n    data = train_x.log)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1717  -1.0427   0.5951   0.9680   2.4297  \n\nCoefficients: (2 not defined because of singularities)\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         1.446e-01  4.138e-01   0.349 0.726831    \nAFR                -1.496e-01  2.475e-02  -6.045 1.49e-09 ***\nEAP                -2.556e-01  2.132e-02 -11.987  &lt; 2e-16 ***\nECA                -2.766e-01  2.514e-02 -11.005  &lt; 2e-16 ***\nLCR                -6.208e-01  2.426e-02 -25.591  &lt; 2e-16 ***\nMNA                -6.703e-02  3.071e-02  -2.183 0.029056 *  \nSAR                        NA         NA      NA       NA    \nca_Consul.Service  -1.121e+00  3.604e-02 -31.096  &lt; 2e-16 ***\nca_Goods           -4.294e-01  1.541e-02 -27.856  &lt; 2e-16 ***\nca_bids_all         3.483e-04  4.278e-04   0.814 0.415608    \nca_proc_open       -2.637e-01  1.128e-01  -2.338 0.019403 *  \nca_proc_quality    -7.794e-01  2.887e-02 -26.995  &lt; 2e-16 ***\nca_proc_source     -8.612e-01  2.224e-02 -38.728  &lt; 2e-16 ***\nca_proc_restricted -1.396e+00  4.643e-02 -30.069  &lt; 2e-16 ***\nsingleb            -4.285e-01  1.065e-01  -4.025 5.70e-05 ***\ncorr_signp1         8.177e-01  1.073e-01   7.620 2.54e-14 ***\ncorr_signp2         3.158e-01  2.057e-02  15.352  &lt; 2e-16 ***\ncorr_signp3         6.552e-01  2.447e-02  26.772  &lt; 2e-16 ***\nnrc                 2.977e-05  1.760e-06  16.917  &lt; 2e-16 ***\ntaxhaven            4.361e-01  9.553e-02   4.565 5.00e-06 ***\nnot.taxhaven        6.971e-01  1.491e-02  46.765  &lt; 2e-16 ***\ncri_wb.d0          -9.462e-01  3.186e-01  -2.970 0.002982 ** \ncri_wb.d25         -7.003e-01  2.135e-01  -3.280 0.001037 ** \ncri_wb.d50         -3.246e-01  1.088e-01  -2.983 0.002850 ** \ncri_wb.d75                 NA         NA      NA       NA    \nP                   3.490e-01  3.955e-01   0.882 0.377560    \nHealth              1.020e+00  3.956e-01   2.579 0.009897 ** \nEdu                 1.398e+00  3.958e-01   3.533 0.000411 ***\nW                   1.311e+00  3.958e-01   3.313 0.000924 ***\nI                   6.749e-01  3.962e-01   1.704 0.088465 .  \nTran                1.680e+00  3.957e-01   4.245 2.19e-05 ***\nFin                 7.710e-01  3.966e-01   1.944 0.051913 .  \nA                   1.418e+00  3.958e-01   3.582 0.000341 ***\nEner                1.769e+00  3.960e-01   4.467 7.94e-06 ***\nInfo                3.904e-01  3.985e-01   0.979 0.327335    \nH                   1.155e+00  4.832e-01   2.391 0.016820 *  \nMulti               2.246e+00  6.367e-01   3.527 0.000420 ***\nPriv                8.699e+00  4.396e+01   0.198 0.843127    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 200979  on 145306  degrees of freedom\nResidual deviance: 175595  on 145271  degrees of freedom\nAIC: 175667\n\nNumber of Fisher Scoring iterations: 7\n\n\n\n\nCode\n#predict probabilities for test subset (Warning)\nprobabilities1 &lt;- log_model_train %&gt;% predict(newdata=test_x.log, type = \"response\")\n\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\n\nCode\nhead(probabilities1) #the warning might be bc there is collinearity or too many predictors. \n\n\n        8        14        34        51        55        63 \n0.6394443 0.6120216 0.4675069 0.3935432 0.4991478 0.7957945 \n\n\n\n\nCode\n#create predicted.classes1\npredicted.classes1 &lt;- ifelse(probabilities1 &gt; 0.5, 1, 0)\nhead(predicted.classes1)\n\n\n 8 14 34 51 55 63 \n 1  1  0  0  0  1 \n\n\n\n\nCode\n#Comparison between actual and predicted values\nmean(predicted.classes1 == test_y.log)\n\n\n[1] 0.6716011\n\n\n\n\nCode\n#Check for aliased coeffs in the model\n\nalias(log_model_train)\n\n\nModel :\ntrain_y.log ~ AFR + EAP + ECA + LCR + MNA + SAR + ca_Consul.Service + \n    ca_Goods + ca_bids_all + ca_proc_open + ca_proc_quality + \n    ca_proc_source + ca_proc_restricted + singleb + corr_signp1 + \n    corr_signp2 + corr_signp3 + nrc + taxhaven + not.taxhaven + \n    cri_wb.d0 + cri_wb.d25 + cri_wb.d50 + cri_wb.d75 + P + Health + \n    Edu + W + I + Tran + Fin + A + Ener + Info + H + Multi + \n    Priv\n\nComplete :\n           (Intercept) AFR EAP ECA LCR MNA ca_Consul.Service ca_Goods\nSAR         1          -1  -1  -1  -1  -1   0                 0      \ncri_wb.d75  2           0   0   0   0   0   0                 0      \n           ca_bids_all ca_proc_open ca_proc_quality ca_proc_source\nSAR         0           0            0               0            \ncri_wb.d75  0           1            0               0            \n           ca_proc_restricted singleb corr_signp1 corr_signp2 corr_signp3 nrc\nSAR         0                  0       0           0           0           0 \ncri_wb.d75  0                 -1       1           0           0           0 \n           taxhaven not.taxhaven cri_wb.d0 cri_wb.d25 cri_wb.d50 P  Health Edu\nSAR         0        0            0         0          0          0  0      0 \ncri_wb.d75 -1        0           -4        -3         -2          0  0      0 \n           W  I  Tran Fin A  Ener Info H  Multi Priv\nSAR         0  0  0    0   0  0    0    0  0     0  \ncri_wb.d75  0  0  0    0   0  0    0    0  0     0  \n\n\nCode\n#aliased coeffs indicate collinearity problem in the model (some variables are linearly dependent on others) and should be removed or transformed.\n\n\n\n\nCode\nalias(log_model_train)$reduced\n\n\nNULL\n\n\n\n\nCode\n#Remove cri_wb.d and other superfluous vars\n#Most probably cri_wb was calculated based on the other vars in the dataset, so it is linearly dependent on them and should be removed\ntrain_x.log2 = train_x.log %&gt;% \n  select( -starts_with(\"cri_wb.d\"), -c(AFR, EAP, ECA, LCR, MNA, SAR, ca_bids_all)) #starts_with() removes all cri_wb dummies \n\ntest_x.log2 = test_x.log %&gt;% \n  select( -starts_with(\"cri_wb.d\"), -c(AFR, EAP, ECA, LCR, MNA, SAR, ca_bids_all))\n\n\n\n\nCode\n#2 logit model training\nlog_model_train.2 = glm(train_y.log ~ ., family = binomial(link = \"logit\"), train_x.log2)\nsummary(log_model_train.2)\n\n\n\nCall:\nglm(formula = train_y.log ~ ., family = binomial(link = \"logit\"), \n    data = train_x.log2)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1247  -1.0546   0.5972   0.9890   2.3543  \n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        -4.547e-01  3.980e-01  -1.142 0.253301    \nca_Consul.Service  -1.164e+00  3.587e-02 -32.450  &lt; 2e-16 ***\nca_Goods           -4.829e-01  1.515e-02 -31.880  &lt; 2e-16 ***\nca_proc_open       -5.625e-01  3.864e-02 -14.559  &lt; 2e-16 ***\nca_proc_quality    -7.681e-01  2.861e-02 -26.849  &lt; 2e-16 ***\nca_proc_source     -8.259e-01  2.176e-02 -37.948  &lt; 2e-16 ***\nca_proc_restricted -1.451e+00  4.600e-02 -31.536  &lt; 2e-16 ***\nsingleb            -1.536e-01  1.412e-02 -10.880  &lt; 2e-16 ***\ncorr_signp1         5.438e-01  1.899e-02  28.637  &lt; 2e-16 ***\ncorr_signp2         3.407e-01  2.035e-02  16.740  &lt; 2e-16 ***\ncorr_signp3         6.837e-01  2.421e-02  28.246  &lt; 2e-16 ***\nnrc                 3.793e-05  1.264e-06  30.010  &lt; 2e-16 ***\ntaxhaven            7.224e-01  4.179e-02  17.286  &lt; 2e-16 ***\nnot.taxhaven        7.763e-01  1.460e-02  53.159  &lt; 2e-16 ***\nP                   2.943e-01  3.954e-01   0.744 0.456650    \nHealth              9.874e-01  3.955e-01   2.496 0.012545 *  \nEdu                 1.335e+00  3.957e-01   3.373 0.000743 ***\nW                   1.257e+00  3.957e-01   3.176 0.001496 ** \nI                   6.348e-01  3.961e-01   1.603 0.109024    \nTran                1.625e+00  3.957e-01   4.108 3.99e-05 ***\nFin                 7.117e-01  3.966e-01   1.795 0.072714 .  \nA                   1.386e+00  3.958e-01   3.502 0.000461 ***\nEner                1.728e+00  3.960e-01   4.364 1.28e-05 ***\nInfo                3.181e-01  3.984e-01   0.798 0.424710    \nH                   1.137e+00  4.840e-01   2.349 0.018802 *  \nMulti               2.320e+00  6.344e-01   3.657 0.000255 ***\nPriv                8.504e+00  4.396e+01   0.193 0.846597    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 200979  on 145306  degrees of freedom\nResidual deviance: 176745  on 145280  degrees of freedom\nAIC: 176799\n\nNumber of Fisher Scoring iterations: 7\n\n\n\n\nCode\nalias(log_model_train.2)$complete\n\n\nNULL\n\n\nCode\n#no aliased coeffs\n\n\n\n\nCode\n#Predict probabilities2\nprobabilities2 &lt;- log_model_train.2 %&gt;% predict(newdata=test_x.log2, type = \"response\")\nhead(probabilities2)\n\n\n        8        14        34        51        55        63 \n0.6194700 0.5866468 0.4000703 0.3530457 0.4693526 0.7737040 \n\n\nCode\n#No warning this time\n\n\n\n\nCode\n#Predcited classes based on probabilities\npredicted.classes2 &lt;- ifelse(probabilities2 &gt; 0.5, 1, 0)\nhead(predicted.classes2)\n\n\n 8 14 34 51 55 63 \n 1  1  0  0  0  1 \n\n\n\n\nCode\n#Compare observed and predicted values\npred.rate.log = mean(predicted.classes2 == test_y.log)\n\nmean(predicted.classes2 == test_y.log)\n\n\n[1] 0.6688758"
  },
  {
    "objectID": "ml_procurement.html#knn",
    "href": "ml_procurement.html#knn",
    "title": "Procurement Contracts",
    "section": "KNN",
    "text": "KNN\n\n\nCode\nstart_time = Sys.time()\n\n#KNN classification\n\nlibrary(class)\n\npredicted_labels = knn(train_x.log, test_x.log, train_y.log, k = 3)\n#Here I used the subsets with cri_wb and other coefficients because the model doesn't care about collinearity and because without them there were too many ties.\n\nprint(Sys.time() - start_time)\n\n\nTime difference of 1.150231 mins\n\n\n\n\nCode\n#Comparison between obs and predictions through KNN\npred.rate.knn = mean(predicted_labels == test_y.log)\n\nprint(pred.rate.knn)\n\n\n[1] 0.7562713\n\n\n\n\nCode\npredicted_labels2 = knn(train_x.log, test_x.log, train_y.log, k = 5)\n\npred.rate.knn2 = mean(predicted_labels2 == test_y.log)\nprint(pred.rate.knn2)\n\n\n[1] 0.7529266\n\n\n\n\nCode\nstart_time = Sys.time()\n\n#KNN without cri_wb\ntrain_x.log3 = train_x.log %&gt;% \n  select( -starts_with(\"cri_wb.d\")) #-starts_with() removes all cri_wb dummies \n\ntest_x.log3 = test_x.log %&gt;% \n  select( -starts_with(\"cri_wb.d\"))\n\npredicted_labels3 = knn(train_x.log3, test_x.log3, train_y.log, k = 3)\n\npred.rate.knn3 = mean(predicted_labels3 == test_y.log)\nprint(pred.rate.knn3)\n\n\n[1] 0.7591205\n\n\nCode\nprint(Sys.time() - start_time)\n\n\nTime difference of 56.19445 secs"
  },
  {
    "objectID": "ml_procurement.html#beyond-linearity-splines",
    "href": "ml_procurement.html#beyond-linearity-splines",
    "title": "Procurement Contracts",
    "section": "Beyond Linearity: Splines",
    "text": "Beyond Linearity: Splines\n\n\nCode\nlibrary(splines)\n\n\n\n\nCode\n#modelling basic spline without bids\n# Fit a regression model with cubic splines for multiple predictors\nspline_model &lt;- lm(train_y ~ bs(nrc, degree = 3) + Edu + Ener + Multi, data = train_x)\nsummary(spline_model)\n\n\n\nCall:\nlm(formula = train_y ~ bs(nrc, degree = 3) + Edu + Ener + Multi, \n    data = train_x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.3907  -1.3039   0.0776   1.3431   9.4819 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          11.99366    0.01865 643.154   &lt;2e-16 ***\nbs(nrc, degree = 3)1 -0.03461    0.06848  -0.505    0.613    \nbs(nrc, degree = 3)2  0.62123    0.06629   9.371   &lt;2e-16 ***\nbs(nrc, degree = 3)3  0.87160    0.02672  32.625   &lt;2e-16 ***\nEdu                   0.27344    0.01862  14.682   &lt;2e-16 ***\nEner                  1.03912    0.02175  47.769   &lt;2e-16 ***\nMulti                -0.21348    0.42393  -0.504    0.615    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.077 on 145300 degrees of freedom\nMultiple R-squared:  0.03805,   Adjusted R-squared:  0.03801 \nF-statistic: 957.9 on 6 and 145300 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\n#rmse and sse for basic spline\n\npreds_spline = predict(spline_model, test_x)\n\nrmse_spline = sqrt(mean(preds_spline - test_y)^2)\nsse_spline = sum((preds_spline - test_y)^2)\nhead(preds_spline)\n\n\n       8       14       34       51       55       63 \n11.99398 12.26742 12.26742 12.00141 12.00141 12.00141 \n\n\n\n\nCode\nhead(test_y)\n\n\n[1]  8.978580 12.802028 13.114779  9.409601 12.325201 14.210191\n\n\nCode\nsqrt(mean(preds_spline - test_y)^2)\n\n\n[1] 0.01513323\n\n\nCode\nsum((preds_spline - test_y)^2)\n\n\n[1] 70996.55\n\n\n\n\nCode\nsummary(test_y)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6446 10.9005 12.3221 12.2694 13.6952 20.3544 \n\n\n\n\nCode\n#spline model with bids included\nspline_model.bids &lt;- lm(train_y ~ bs(nrc, degree = 3) + bs(ca_bids_all, degree = 3) + Edu + Ener + Multi, data = train_x)\nsummary(spline_model.bids)\n\n\n\nCall:\nlm(formula = train_y ~ bs(nrc, degree = 3) + bs(ca_bids_all, \n    degree = 3) + Edu + Ener + Multi, data = train_x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.3182  -1.2970   0.0857   1.3408   9.3605 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  11.90826    0.01882 632.741  &lt; 2e-16 ***\nbs(nrc, degree = 3)1          0.01477    0.06829   0.216  0.82881    \nbs(nrc, degree = 3)2          0.53280    0.06617   8.052  8.2e-16 ***\nbs(nrc, degree = 3)3          0.84187    0.02665  31.586  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)1 11.07475    0.38349  28.879  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)2 -2.73186    1.81815  -1.503  0.13296    \nbs(ca_bids_all, degree = 3)3  2.58227    0.83824   3.081  0.00207 ** \nEdu                           0.24309    0.01860  13.072  &lt; 2e-16 ***\nEner                          1.04128    0.02169  48.014  &lt; 2e-16 ***\nMulti                        -0.20574    0.42263  -0.487  0.62639    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.07 on 145297 degrees of freedom\nMultiple R-squared:  0.04394,   Adjusted R-squared:  0.04388 \nF-statistic:   742 on 9 and 145297 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\n#spline rmse and sse including bids variable\npreds_spline.bids = predict(spline_model.bids, test_x)\n\nrmse_spline.bids = sqrt(mean(preds_spline.bids - test_y)^2)\nsse_spline.bids = sum((preds_spline.bids - test_y)^2)\n\nprint(sse_spline.bids)\n\n\n[1] 70454.6\n\n\nCode\nprint(rmse_spline.bids)\n\n\n[1] 0.01356418\n\n\n\n\nCode\n#Plot nrc vs value\nProcurement3 %&gt;% \n  ggplot(aes(x=nrc, y=lca_contract_value)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\nCode\n#Plot ca_bids_all vs value 1\nProcurement3 %&gt;% \n  ggplot(aes(x=ca_bids_all, y=lca_contract_value)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\nCode\nsum(train_x$nrc &gt; 10000)\n\n\n[1] 23225\n\n\nCode\nsum(train_x$ca_bids_all &gt;= 50)\n\n\n[1] 294\n\n\nCode\n293/145218\n\n\n[1] 0.002017656\n\n\nCode\n23232/145218\n\n\n[1] 0.1599802\n\n\n\n\nCode\n#Plot ca_bids_all vs value 2\nProcurement3 %&gt;%\n  filter(!(ca_bids_all &gt;= 50)) %&gt;% \n  ggplot(aes(x=ca_bids_all, y=lca_contract_value)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\nCode\n#Plot for ca_bids_all\nProcurement3 %&gt;% \n  filter(!(ca_bids_all &gt;= 50)) %&gt;%\n  ggplot(aes(x=ca_bids_all)) + \n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\nCode\n#scaling ca_bids_all & Creating Procure_scaled\nProcure_scaled = Procurement3 %&gt;% \n  filter(!(ca_bids_all &gt;= 50)) %&gt;%\n  mutate_at(vars(ca_bids_all), scale) %&gt;% \n  mutate_at(vars(ca_bids_all), as.numeric) #scale returns a matrix, so we need to save as.numeric\n\n\n\n\nCode\n#Plot for nrc\nProcurement3 %&gt;% \n  ggplot(aes(x=nrc)) + \n  geom_density()\n\n\n\n\n\n\n\nCode\n#scaling nrc\nProcure_scaled = Procure_scaled %&gt;% \n  mutate_at(vars(nrc), scale) %&gt;% \n  mutate_at(vars(nrc), as.numeric)\n\n\n\n\nCode\n#Checking scale\nhead(Procure_scaled$nrc)\n\n\n[1] -0.8413083 -0.8413083 -0.8413083 -0.8413083 -0.8413083 -0.8413083\n\n\nCode\nhead(Procure_scaled$ca_bids_all)\n\n\n[1]  0.5530255 -0.1404550 -0.1404550 -0.6027754  0.3218654  0.5530255\n\n\nCode\nsummary(Procure_scaled$nrc)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-1.0156 -0.6685 -0.4557  0.0000  0.2466  2.1803 \n\n\nCode\nsummary(Procure_scaled$ca_bids_all)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.6028 -0.6028 -0.3716  0.0000  0.0907 10.2618 \n\n\n\n\nCode\nsummary(Procure_scaled$lca_contract_value)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -3.632  10.933  12.344  12.286  13.681  21.480 \n\n\nCode\nProcure_scaled = Procure_scaled %&gt;% \n  filter(!is.na(lca_contract_value))\n\n\n\n\nCode\n#assigning and sampling indices\nindex.spline = 1:nrow(Procure_scaled)\n\nset.seed(12345) #for random \ntrain_index.spline = sample(index.spline, round(0.90*nrow(Procure_scaled)), replace = FALSE) #takes sample of 90% of rows without replacement\n\ntest_index.spline = setdiff(index.spline, train_index.spline) #takes untaken rows\n\n\n\n\nCode\n#creating train and test sets for spline\ntrain_x.sp = Procure_scaled[train_index.spline, ] %&gt;% \n  select(-lca_contract_value, -ca_contract_valuec) #Procure_scaled[train_index, ] expression to use only training subset %&gt;% everything except price and price category\n\ntrain_y.sp = Procure_scaled[train_index.spline, ] %&gt;% \n  pull(lca_contract_value)\n\ntest_x.sp = Procure_scaled[test_index.spline, ] %&gt;% \n  select(-lca_contract_value, -ca_contract_valuec)\n\ntest_y.sp = Procure_scaled[test_index.spline, ] %&gt;% \n  pull(lca_contract_value)\n\n\n\n\nCode\n#training spline regression model\nspline_model2 &lt;- lm(train_y.sp ~ bs(nrc, degree = 3) + bs(ca_bids_all, degree = 3) + Edu + Ener + Multi, data = train_x.sp)\n#base spline with degree of polynomial 3. The resulting model will have a cubic polynomial function for each of the segments between the knots (which are quantiles by default).\nsummary(spline_model2)\n\n\n\nCall:\nlm(formula = train_y.sp ~ bs(nrc, degree = 3) + bs(ca_bids_all, \n    degree = 3) + Edu + Ener + Multi, data = train_x.sp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.2785  -1.2860   0.0873   1.3266   8.8929 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  11.68783    0.01908 612.483  &lt; 2e-16 ***\nbs(nrc, degree = 3)1          0.10699    0.06769   1.581    0.114    \nbs(nrc, degree = 3)2          0.40187    0.06573   6.114 9.74e-10 ***\nbs(nrc, degree = 3)3          0.74986    0.02645  28.347  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)1  3.52129    0.06223  56.588  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)2 -3.71759    0.16550 -22.463  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)3  0.90606    0.17384   5.212 1.87e-07 ***\nEdu                           0.23924    0.01849  12.941  &lt; 2e-16 ***\nEner                          0.98373    0.02148  45.797  &lt; 2e-16 ***\nMulti                        -0.24564    0.39442  -0.623    0.533    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.049 on 145002 degrees of freedom\nMultiple R-squared:  0.06263,   Adjusted R-squared:  0.06257 \nF-statistic:  1076 on 9 and 145002 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\n#predictions by spline\npreds_spline2 = predict(spline_model2, test_x.sp)\n\nrmse_spline2 = sqrt(mean(preds_spline2 - test_y.sp)^2)\nhead(preds_spline2)\n\n\n       8       14       34       51       55       56 \n12.49270 12.33904 11.94622 12.11588 11.93343 12.11588 \n\n\n\n\nCode\n#rmse and sse for spline\nsse_spline2 = sum((preds_spline2 - test_y.sp)^2)\n\nprint(sse_spline2)\n\n\n[1] 69440.65\n\n\nCode\nprint(rmse_spline2)\n\n\n[1] 0.01712034\n\n\n\n\nCode\n#degree 1 to compare with spline degree 2\nspline_model.lr &lt;- lm(train_y.sp ~ bs(nrc, degree = 1) + bs(ca_bids_all, degree = 1) + Edu + Ener + Multi, data = train_x.sp)\nsummary(spline_model.lr)\n\n\n\nCall:\nlm(formula = train_y.sp ~ bs(nrc, degree = 1) + bs(ca_bids_all, \n    degree = 1) + Edu + Ener + Multi, data = train_x.sp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.3155  -1.2925   0.0838   1.3407   9.3581 \n\nCoefficients:\n                             Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)                 11.808730   0.008531 1384.211   &lt;2e-16 ***\nbs(nrc, degree = 1)          0.888195   0.017549   50.612   &lt;2e-16 ***\nbs(ca_bids_all, degree = 1)  1.883331   0.059426   31.692   &lt;2e-16 ***\nEdu                          0.225089   0.018657   12.064   &lt;2e-16 ***\nEner                         1.032723   0.021591   47.832   &lt;2e-16 ***\nMulti                       -0.178013   0.398264   -0.447    0.655    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.069 on 145006 degrees of freedom\nMultiple R-squared:  0.04415,   Adjusted R-squared:  0.04412 \nF-statistic:  1340 on 5 and 145006 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\n#predictions of spline degree 1\npreds_spline.lr = predict(spline_model.lr, test_x.sp)\n\nrmse_spline.lr = sqrt(mean(preds_spline.lr - test_y.sp)^2)\nhead(preds_spline.lr)\n\n\n       8       14       34       51       55       56 \n12.05751 12.16239 12.08225 11.97277 11.93270 11.97277 \n\n\n\n\nCode\n#rmse and sse for spline degree 1\nsqrt(mean(preds_spline.lr - test_y.sp)^2)\n\n\n[1] 0.01716597\n\n\nCode\nsum((preds_spline.lr - test_y.sp)^2)\n\n\n[1] 70568.34\n\n\n\n\nCode\n#breif check of predictions and obs\nsum(is.na(preds_spline2))\n\n\n[1] 0\n\n\nCode\nsummary(preds_spline2)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  11.05   11.81   12.15   12.29   12.65   14.43 \n\n\nCode\nsummary(test_y.sp)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.6561 10.9044 12.3478 12.2704 13.6981 20.3544 \n\n\n\n\nCode\n#training spline using automatic knot selection gam\nlibrary(mgcv)\n\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:Ecdat':\n\n    Gasoline\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.8-39. For overview type 'help(\"mgcv-package\")'.\n\n\nCode\n# gam uses a built-in algorithm to choose optimal knots based on the data\nspline_model.gam &lt;- gam(train_y.sp ~ bs(nrc, degree = 3) + bs(ca_bids_all, degree = 3) + Edu + Ener + Multi, data = train_x.sp)\nsummary(spline_model.gam)\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ntrain_y.sp ~ bs(nrc, degree = 3) + bs(ca_bids_all, degree = 3) + \n    Edu + Ener + Multi\n\nParametric coefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  11.68783    0.01908 612.483  &lt; 2e-16 ***\nbs(nrc, degree = 3)1          0.10699    0.06769   1.581    0.114    \nbs(nrc, degree = 3)2          0.40187    0.06573   6.114 9.74e-10 ***\nbs(nrc, degree = 3)3          0.74986    0.02645  28.347  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)1  3.52129    0.06223  56.588  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)2 -3.71759    0.16550 -22.463  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)3  0.90606    0.17384   5.212 1.87e-07 ***\nEdu                           0.23924    0.01849  12.941  &lt; 2e-16 ***\nEner                          0.98373    0.02148  45.797  &lt; 2e-16 ***\nMulti                        -0.24564    0.39442  -0.623    0.533    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  0.0626   Deviance explained = 6.26%\nGCV = 4.1992  Scale est. = 4.1989    n = 145012\n\n\n\n\nCode\n#rmse and sse for optimized-knots spline\npreds_spline.gam = predict(spline_model.gam, test_x.sp)\nrmse_spline.gam = sqrt(mean(preds_spline.gam - test_y.sp)^2)\nsse_spline.gam = sum((preds_spline.gam - test_y.sp)^2)\n\nprint(sse_spline.gam)\n\n\n[1] 69440.65\n\n\nCode\nprint(rmse_spline.gam)\n\n\n[1] 0.01712034\n\n\n\n\nCode\n#spline gam without removing outliers\nspline_model.gam2 &lt;- gam(train_y ~ bs(nrc, degree = 3) + bs(ca_bids_all, degree = 3) + Edu + Ener + Multi, data = train_x)\nsummary(spline_model.gam2)\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ntrain_y ~ bs(nrc, degree = 3) + bs(ca_bids_all, degree = 3) + \n    Edu + Ener + Multi\n\nParametric coefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  11.90826    0.01882 632.741  &lt; 2e-16 ***\nbs(nrc, degree = 3)1          0.01477    0.06829   0.216  0.82881    \nbs(nrc, degree = 3)2          0.53280    0.06617   8.052  8.2e-16 ***\nbs(nrc, degree = 3)3          0.84187    0.02665  31.586  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)1 11.07475    0.38349  28.879  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)2 -2.73186    1.81815  -1.503  0.13296    \nbs(ca_bids_all, degree = 3)3  2.58227    0.83824   3.081  0.00207 ** \nEdu                           0.24309    0.01860  13.072  &lt; 2e-16 ***\nEner                          1.04128    0.02169  48.014  &lt; 2e-16 ***\nMulti                        -0.20574    0.42263  -0.487  0.62639    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  0.0439   Deviance explained = 4.39%\nGCV = 4.2859  Scale est. = 4.2856    n = 145307\n\n\n\n\nCode\npreds_spline.gam2 = predict(spline_model.gam2, test_x)\nrmse_spline.gam2 = sqrt(mean(preds_spline.gam2 - test_y)^2)\nsse_spline.gam2 = sum((preds_spline.gam2 - test_y)^2)\n\nprint(sse_spline.gam2)\n\n\n[1] 70454.6\n\n\nCode\nprint(rmse_spline.gam2)\n\n\n[1] 0.01356418\n\n\n\n\nCode\n#coefs of spline_model.gam\ncoef(spline_model.gam)\n\n\n                 (Intercept)         bs(nrc, degree = 3)1 \n                  11.6878264                    0.1069901 \n        bs(nrc, degree = 3)2         bs(nrc, degree = 3)3 \n                   0.4018697                    0.7498629 \nbs(ca_bids_all, degree = 3)1 bs(ca_bids_all, degree = 3)2 \n                   3.5212868                   -3.7175900 \nbs(ca_bids_all, degree = 3)3                          Edu \n                   0.9060622                    0.2392372 \n                        Ener                        Multi \n                   0.9837282                   -0.2456402 \n\n\n\n\nCode\n#Plot to compare with the previous one of nrc vs value\nProcurement3 %&gt;% \n  ggplot(aes(x=nrc, y=lca_contract_value)) +\n  geom_point() +\n  geom_smooth(method = \"gam\", formula = y ~ s(x), se = FALSE)\n\n\n\n\n\n\n\nCode\n#compare with simple linear regression\nlr_check = lm(train_y.sp ~ nrc + ca_bids_all + Edu + Ener + Multi, data = train_x.sp)\nsummary(lr_check)\n\n\n\nCall:\nlm(formula = train_y.sp ~ nrc + ca_bids_all + Edu + Ener + Multi, \n    data = train_x.sp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.3155  -1.2925   0.0838   1.3407   9.3581 \n\nCoefficients:\n             Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) 12.195462   0.005944 2051.686   &lt;2e-16 ***\nnrc          0.277917   0.005491   50.612   &lt;2e-16 ***\nca_bids_all  0.173347   0.005470   31.692   &lt;2e-16 ***\nEdu          0.225089   0.018657   12.064   &lt;2e-16 ***\nEner         1.032723   0.021591   47.832   &lt;2e-16 ***\nMulti       -0.178013   0.398264   -0.447    0.655    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.069 on 145006 degrees of freedom\nMultiple R-squared:  0.04415,   Adjusted R-squared:  0.04412 \nF-statistic:  1340 on 5 and 145006 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\n#rmse and sse for linear reg\npreds_lrcheck = predict(lr_check, test_x.sp)\n\nrmse_lrcheck = sqrt(mean(preds_lrcheck - test_y.sp)^2)\n\nsqrt(mean(preds_lrcheck - test_y.sp)^2)\n\n\n[1] 0.01716597\n\n\nCode\nsum((preds_lrcheck - test_y.sp)^2)\n\n\n[1] 70568.34\n\n\n\n\nCode\n#gam spline with all vars\nspline_model.gam3 &lt;- gam(train_y ~ bs(nrc, degree = 3) + bs(ca_bids_all, degree = 3) + AFR +EAP +ECA +LCR +MNA +SAR + ca_Consul.Service + ca_Goods + ca_proc_open + ca_proc_quality + ca_proc_source + ca_proc_restricted + corr_signp1 +corr_signp2 + corr_signp3 + taxhaven + not.taxhaven + P + Health + Edu + W + I + Tran + Fin + A + Ener + Info + H + Multi + Priv, data = train_x)\n\nsummary(spline_model.gam3)\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ntrain_y ~ bs(nrc, degree = 3) + bs(ca_bids_all, degree = 3) + \n    AFR + EAP + ECA + LCR + MNA + SAR + ca_Consul.Service + ca_Goods + \n    ca_proc_open + ca_proc_quality + ca_proc_source + ca_proc_restricted + \n    corr_signp1 + corr_signp2 + corr_signp3 + taxhaven + not.taxhaven + \n    P + Health + Edu + W + I + Tran + Fin + A + Ener + Info + \n    H + Multi + Priv\n\nParametric coefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  10.24556    0.22866  44.806  &lt; 2e-16 ***\nbs(nrc, degree = 3)1          0.83188    0.07045  11.807  &lt; 2e-16 ***\nbs(nrc, degree = 3)2          0.64910    0.07630   8.507  &lt; 2e-16 ***\nbs(nrc, degree = 3)3          0.71961    0.02955  24.356  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)1 -0.07827    0.36783  -0.213  0.83150    \nbs(ca_bids_all, degree = 3)2  6.43106    1.63680   3.929 8.53e-05 ***\nbs(ca_bids_all, degree = 3)3 -0.64031    0.75370  -0.850  0.39557    \nAFR                           1.82642    0.03962  46.100  &lt; 2e-16 ***\nEAP                           1.70413    0.04063  41.944  &lt; 2e-16 ***\nECA                           1.63749    0.03979  41.158  &lt; 2e-16 ***\nLCR                           1.30747    0.03976  32.884  &lt; 2e-16 ***\nMNA                           1.98542    0.04169  47.623  &lt; 2e-16 ***\nSAR                           1.78463    0.04135  43.156  &lt; 2e-16 ***\nca_Consul.Service            -1.26199    0.03122 -40.428  &lt; 2e-16 ***\nca_Goods                     -0.63796    0.01323 -48.204  &lt; 2e-16 ***\nca_proc_open                 -0.42812    0.03361 -12.739  &lt; 2e-16 ***\nca_proc_quality              -0.91561    0.02399 -38.174  &lt; 2e-16 ***\nca_proc_source               -0.76538    0.01804 -42.417  &lt; 2e-16 ***\nca_proc_restricted           -1.38403    0.03981 -34.767  &lt; 2e-16 ***\ncorr_signp1                   0.53926    0.01612  33.450  &lt; 2e-16 ***\ncorr_signp2                   0.32408    0.01723  18.805  &lt; 2e-16 ***\ncorr_signp3                   0.75086    0.02056  36.521  &lt; 2e-16 ***\ntaxhaven                      0.86965    0.03552  24.482  &lt; 2e-16 ***\nnot.taxhaven                  0.83840    0.01265  66.259  &lt; 2e-16 ***\nP                            -0.16916    0.26336  -0.642  0.52066    \nHealth                        0.42271    0.26350   1.604  0.10867    \nEdu                           0.70060    0.26369   2.657  0.00789 ** \nW                             0.79292    0.26368   3.007  0.00264 ** \nI                             0.15557    0.26414   0.589  0.55589    \nTran                          1.27607    0.26362   4.841 1.30e-06 ***\nFin                           0.15867    0.26461   0.600  0.54876    \nA                             0.82551    0.26374   3.130  0.00175 ** \nEner                          1.34328    0.26388   5.090 3.58e-07 ***\nInfo                         -0.04931    0.26670  -0.185  0.85333    \nH                             0.70120    0.36051   1.945  0.05178 .  \nMulti                         0.15113    0.46191   0.327  0.74352    \nPriv                          1.80728    1.87819   0.962  0.33593    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRank: 36/37\nR-sq.(adj) =  0.229   Deviance explained = 22.9%\nGCV = 3.4586  Scale est. = 3.4577    n = 145307\n\n\n\n\nCode\n#rmse and sse for spline with all vars\npreds_spline.gam3 = predict(spline_model.gam3, test_x)\n\nrmse_sp.gam3 = sqrt(mean(preds_spline.gam3 - test_y)^2)\nsse_sp.gam3  = sum((preds_spline.gam3 - test_y)^2)\n\nprint(rmse_sp.gam3)\n\n\n[1] 0.01400472\n\n\nCode\nprint(sse_sp.gam3)\n\n\n[1] 56856.29"
  },
  {
    "objectID": "ml_procurement.html#regression-tree",
    "href": "ml_procurement.html#regression-tree",
    "title": "Procurement Contracts",
    "section": "Regression Tree",
    "text": "Regression Tree\n\n\nCode\nsum(is.na(Procurement3)) #no missing values\n\n\n[1] 0\n\n\n\n\nCode\n#libraries for Trees\nlibrary(rpart)\nlibrary(rpart.plot) #prettier plots for rpart\nlibrary(tibble)\n\n\n\n\nCode\n#checking the structure of the dataset\nstr(Procurement3)\n\n\n'data.frame':   161452 obs. of  39 variables:\n $ ca_contract_valuec: chr  \"50.000-199.999\" \"200.000-\" \"200.000-\" \"200.000-\" ...\n $ lca_contract_value: num  12 12.4 12.3 13.5 11.3 ...\n $ AFR               : num  1 1 1 1 1 1 1 1 1 1 ...\n $ EAP               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ ECA               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ LCR               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ MNA               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ SAR               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ ca_Consul.Service : num  0 0 0 1 0 0 0 0 0 0 ...\n $ ca_Goods          : num  0 0 0 0 1 0 1 1 1 1 ...\n $ ca_bids_all       : int  6 3 3 1 5 6 1 6 5 2 ...\n $ ca_proc_open      : num  1 1 1 0 1 1 0 1 1 1 ...\n $ ca_proc_quality   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ ca_proc_source    : num  0 0 0 1 0 0 1 0 0 0 ...\n $ ca_proc_restricted: num  0 0 0 0 0 0 0 0 0 0 ...\n $ singleb           : num  0 0 0 1 0 0 1 0 0 0 ...\n $ corr_signp1       : num  1 0 0 0 1 0 0 1 1 0 ...\n $ corr_signp2       : int  0 0 0 0 0 0 1 0 0 1 ...\n $ corr_signp3       : int  0 0 0 1 0 0 0 0 0 0 ...\n $ nrc               : int  858 858 858 858 858 858 858 858 858 858 ...\n $ taxhaven          : num  0 0 0 0 0 0 0 0 0 0 ...\n $ not.taxhaven      : num  0 0 0 1 1 0 1 1 1 0 ...\n $ cri_wb.d0         : num  1 0 0 0 1 0 0 1 1 0 ...\n $ cri_wb.d25        : num  0 1 1 0 0 1 0 0 0 1 ...\n $ cri_wb.d50        : num  0 0 0 0 0 0 0 0 0 0 ...\n $ cri_wb.d75        : num  0 0 0 1 0 0 1 0 0 0 ...\n $ P                 : num  1 0 0 0 0 1 0 0 0 0 ...\n $ Health            : num  0 1 0 0 0 0 1 0 0 0 ...\n $ Edu               : num  0 0 1 0 0 0 0 0 0 0 ...\n $ W                 : num  0 0 0 1 0 0 0 0 0 0 ...\n $ I                 : num  0 0 0 0 1 0 0 1 0 0 ...\n $ Tran              : num  0 0 0 0 0 0 0 0 1 0 ...\n $ Fin               : num  0 0 0 0 0 0 0 0 0 1 ...\n $ A                 : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Ener              : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Info              : num  0 0 0 0 0 0 0 0 0 0 ...\n $ H                 : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Multi             : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Priv              : num  0 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:13] 59655 59815 138472 138473 138474 138475 138476 138477 138478 138479 ...\n  ..- attr(*, \"names\")= chr [1:13] \"59655\" \"59815\" \"138472\" \"138473\" ...\n\n\nWe need to recode dummies as factors for the decision tree.\n\n\nCode\n#changing dummies from numeric to factor\ncols_for_factor &lt;- c(\"AFR\", \"EAP\", \"ECA\", \"LCR\", \"MNA\", \"SAR\", \"ca_Consul.Service\", \"ca_Goods\", \"ca_proc_open\", \"ca_proc_quality\", \"ca_proc_source\", \"ca_proc_restricted\", \"singleb\", \"corr_signp1\", \"corr_signp2\", \"corr_signp3\", \"taxhaven\", \"not.taxhaven\", \"P\", \"Health\", \"Edu\", \"W\", \"I\", \"Tran\", \"Fin\", \"A\", \"Ener\", \"Info\", \"H\", \"Multi\", \"Priv\")\n\nProcurement5 &lt;- Procurement3 %&gt;%\n  select( -starts_with(\"cri_wb.d\"), -ca_contract_valuec) %&gt;% \n  mutate_at(all_of(vars(cols_for_factor)), factor)\n\n\nWarning: Using `all_of()` outside of a selecting function was deprecated in tidyselect\n1.2.0.\nℹ See details at\n  &lt;https://tidyselect.r-lib.org/reference/faq-selection-context.html&gt;\n\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(cols_for_factor)\n\n  # Now:\n  data %&gt;% select(all_of(cols_for_factor))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n\nCode\n#Procure_scaled2[cols_for_factor] = lapply(Procure_scaled[cols_for_factor], factor)\n#Warning: Using an external vector in selections was deprecated in tidyselect 1.1.0. Please use `all_of()` or `any_of()` instead.\n\n\n\n\nCode\nstr(Procurement5)\n\n\n'data.frame':   161452 obs. of  34 variables:\n $ lca_contract_value: num  12 12.4 12.3 13.5 11.3 ...\n $ AFR               : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 2 2 2 2 ...\n $ EAP               : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ ECA               : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ LCR               : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ MNA               : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ SAR               : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ ca_Consul.Service : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 1 1 1 ...\n $ ca_Goods          : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 2 1 2 2 2 2 ...\n $ ca_bids_all       : int  6 3 3 1 5 6 1 6 5 2 ...\n $ ca_proc_open      : Factor w/ 2 levels \"0\",\"1\": 2 2 2 1 2 2 1 2 2 2 ...\n $ ca_proc_quality   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ ca_proc_source    : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 2 1 1 1 ...\n $ ca_proc_restricted: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ singleb           : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 2 1 1 1 ...\n $ corr_signp1       : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 2 1 1 2 2 1 ...\n $ corr_signp2       : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 2 1 1 2 ...\n $ corr_signp3       : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 1 1 1 ...\n $ nrc               : int  858 858 858 858 858 858 858 858 858 858 ...\n $ taxhaven          : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ not.taxhaven      : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 2 1 2 2 2 1 ...\n $ P                 : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 1 2 1 1 1 1 ...\n $ Health            : Factor w/ 2 levels \"0\",\"1\": 1 2 1 1 1 1 2 1 1 1 ...\n $ Edu               : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 1 1 1 1 1 1 ...\n $ W                 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 1 1 1 ...\n $ I                 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 2 1 1 2 1 1 ...\n $ Tran              : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ Fin               : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 2 ...\n $ A                 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Ener              : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Info              : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ H                 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Multi             : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Priv              : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:13] 59655 59815 138472 138473 138474 138475 138476 138477 138478 138479 ...\n  ..- attr(*, \"names\")= chr [1:13] \"59655\" \"59815\" \"138472\" \"138473\" ...\n\n\n\n\nCode\n#assigning and sampling indices for tree model\nindex.t = 1:nrow(Procurement5)\n\nset.seed(12345) #for random \ntrain_index.t = sample(index.t, round(0.90*nrow(Procurement4)), replace = FALSE) #takes sample of 90% of rows without replacement\n\ntest_index.t = setdiff(index.t, train_index.t) #takes untaken rows\n\n\n\n\nCode\n#creating train and test sets for tree model\ntrain_x.t = Procurement5[train_index.t, ] %&gt;% \n  select(-lca_contract_value) #Procure_scaled[train_index, ] expression to use only training subset %&gt;% everything except price and price category\n\ntrain_y.t = Procurement5[train_index.t, ] %&gt;% \n  pull(lca_contract_value)\n\ntest_x.t = Procurement5[test_index.t, ] %&gt;% \n  select(-lca_contract_value)\n\ntest_y.t = Procurement5[test_index.t, ] %&gt;% \n  pull(lca_contract_value)\n\n\n\n\nCode\n#first regression tree model\ntree_model &lt;- rpart(\n  formula = train_y.t ~ .,\n  data    = train_x.t,\n  method  = \"anova\"\n  )\n\n\n\n\nCode\n#plot for the first tree\nrpart.plot(tree_model, digits = 4)\n\n\n\n\n\nCode\n#digits = The number of significant digits in displayed numbers\n\n\n\n\nCode\n#structure of the tree\ntree_model\n\n\nn= 145307 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 145307 651308.50 12.28944  \n   2) ca_proc_open=0 64392 263413.50 11.67573  \n     4) not.taxhaven=0 43530 174856.70 11.34876  \n       8) nrc&lt; 4495.5 32645 124934.50 11.12766  \n        16) AFR=0 23100  90592.66 10.81381 *\n        17) AFR=1 9545  26559.77 11.88722 *\n       9) nrc&gt;=4495.5 10885  43540.74 12.01183 *\n     5) not.taxhaven=1 20862  74191.81 12.35800 *\n   3) ca_proc_open=1 80915 344342.10 12.77783  \n     6) P=1 22287  76991.40 11.79576 *\n     7) P=0 58628 237684.10 13.15116  \n      14) Tran=0 48010 179324.80 12.97448  \n        28) Ener=0 41969 148938.10 12.82764 *\n        29) Ener=1 6041  23194.13 13.99469 *\n      15) Tran=1 10618  50084.31 13.95003 *\n\n\n\n\nCode\n#plot size of the tree\nplotcp(tree_model)\n\n\n\n\n\nCode\n#To compare the error for each alpha value, rpart performs a 10-fold cross validation so that the error associated with a given alpa value is computed on the hold-out validation data.\n\n\n\n\nCode\n#cptable for the first tree\ntree_model$cptable\n\n\n          CP nsplit rel error    xerror        xstd\n1 0.06686981      0 1.0000000 1.0000376 0.004230378\n2 0.04554926      1 0.9331302 0.9331805 0.003978562\n3 0.02205551      2 0.8875809 0.8876452 0.003861356\n4 0.01270517      3 0.8655254 0.8656022 0.003788718\n5 0.01104325      4 0.8528203 0.8529071 0.003725318\n6 0.01087315      5 0.8417770 0.8448465 0.003705916\n7 0.01000000      7 0.8200307 0.8201653 0.003598033\n\n\n\n\nCode\n#rmse of the first tree_model\npreds_tree = predict(tree_model, test_x.t)\n\nrmse_tree = sqrt(mean(preds_tree - test_y.t)^2)\nsse_tree = sum((preds_tree - test_y.t)^2)\nprint(rmse_tree)\n\n\n[1] 0.01486034\n\n\nCode\nprint(sse_tree)\n\n\n[1] 60814.71"
  },
  {
    "objectID": "ml_procurement.html#tuning-regression-tree",
    "href": "ml_procurement.html#tuning-regression-tree",
    "title": "Procurement Contracts",
    "section": "Tuning Regression Tree",
    "text": "Tuning Regression Tree\n\n\nCode\n#expand grid of the minsplit and maxdepth\nhyper_grid.t &lt;- expand.grid(\n  minsplit = seq(5, 20, 1),\n  maxdepth = seq(8, 15, 1)\n)\n#create a hyper parameter grid\n#minsplit is the minimum number of data points required to attempt a split in the tree. Default=20\n#maxdepth is the maximum number of internal nodes between the root node and the terminal nodes (leafs). Default=30\n\n\nWARNING: Takes a lot of time to run\n\n\nCode\n#loop function for training trees\n\nstart_time = Sys.time()\n\ntree_models = list() #creates a list that will contain tree models\n\nfor (i in 1:nrow(hyper_grid.t)) {\n  \n  # gets minsplit, maxdepth values at row i in hyper_grid.t\n  minsplit = hyper_grid.t$minsplit[i]\n  maxdepth = hyper_grid.t$maxdepth[i]\n\n  # trains a model and stores in the tree_models list\n  # models[[i]] = rpart(...) assigns a trained decision tree model to i-th element.\n  tree_models[[i]] = rpart(\n    formula = train_y.t ~ .,\n    data    = train_x.t,\n    method  = \"anova\",\n    control = list(minsplit = minsplit, maxdepth = maxdepth)\n    )\n}\n\nprint(Sys.time() - start_time)\n\n\nTime difference of 12.40899 mins\n\n\nCode\n#took a lot of time to run\n\n\n\n\nCode\n#Functions to get the parameters with lowest error\n# function to get optimal cp\nget_cp &lt;- function(x) {\n  min    &lt;- which.min(x$cptable[, \"xerror\"])\n  cp &lt;- x$cptable[min, \"CP\"] \n}\n\n# function to get minimum error\nget_min_error &lt;- function(x) {\n  min    &lt;- which.min(x$cptable[, \"xerror\"])\n  xerror &lt;- x$cptable[min, \"xerror\"] \n}\n\nhyper_grid.t %&gt;%\n  mutate(\n    cp    = purrr::map_dbl(tree_models, get_cp),\n    error = purrr::map_dbl(tree_models, get_min_error)\n    ) %&gt;%\n  arrange(error) %&gt;%\n  top_n(-5, wt = error)\n\n\n  minsplit maxdepth   cp     error\n1        6       10 0.01 0.8200839\n2        9       15 0.01 0.8200990\n3       13        8 0.01 0.8200991\n4       17       15 0.01 0.8201020\n5       15       15 0.01 0.8201025\n\n\nCode\n#Mutates hyper_grid.t and adds columns with cp and errors and then returns top 5 rows with the lowest error value.\n#purrr::map_dbl() takes two arguments: the first argument is the vector or list that we want to apply the function to, and the second argument is the function that we want to apply. It applies the function to each element and returns a double (numeric).\n\n\n\n\nCode\n#tuned regression tree\ntuned_tree &lt;- rpart(\n    formula = train_y.t ~ .,\n    data    = train_x.t,\n    method  = \"anova\",\n    control = list(minsplit = 8, maxdepth = 13, cp = 0.01)\n    )\n\n\n\n\nCode\n#RMSE and SSE for the tuned tree\npreds_tree2 = predict(tuned_tree, test_x.t)\n\nrmse_tunedtree = sqrt(mean(preds_tree2 - test_y.t)^2)\nsse_tunedtree = sum((preds_tree2 - test_y.t)^2)\nprint(rmse_tunedtree)\n\n\n[1] 0.01486034\n\n\nCode\nprint(sse_tunedtree)\n\n\n[1] 60814.71\n\n\n\n\nCode\n#plot for the tuned tree\nrpart.plot(tuned_tree, digits = 4)\n\n\n\n\n\nAbsolutely no difference between the automatically optimized regression tree and the tuned one."
  },
  {
    "objectID": "ml_procurement.html#bagging",
    "href": "ml_procurement.html#bagging",
    "title": "Procurement Contracts",
    "section": "Bagging",
    "text": "Bagging\n\n\nCode\n#library ipred and carte for bagging\nlibrary(ipred)\nlibrary(caret)\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\n\n\nCode\n#training first bagged model\nset.seed(123)\n\n# train bagged model\nbag_tree &lt;- bagging(\n  formula = train_y.t ~ .,\n  data    = train_x.t,\n  coob    = TRUE,\n  importance = TRUE\n)\n\n#importance=TRUE computes and stores the variables' relative importance. \n\nbag_tree\n\n\n\nBagging regression trees with 25 bootstrap replications \n\nCall: bagging.data.frame(formula = train_y.t ~ ., data = train_x.t, \n    coob = TRUE, importance = TRUE)\n\nOut-of-bag estimate of root mean squared error:  1.9175 \n\n\n\n\nCode\n#plot for var importance\nvarimp = varImp(bag_tree) #importance from caret package\nvarimp = rownames_to_column(varimp, var = \"variable\")\n\n# create bar plot of variable importance measures\nvarimp %&gt;% \n  filter(Overall!=0) %&gt;% \n  ggplot(aes(x = Overall, \n             xend = 0, \n             y = reorder(variable, Overall), \n             yend=variable)) +\n  geom_segment() +\n  geom_point() +\n  labs(title = \"Variable Importance Plot for Bagging Model\", x = \"Importance\", y = \"Variables\")\n\n\n\n\n\nCode\n#geom_bar(stat = \"identity\", fill = \"steelblue\") +\n\n\n\n\nCode\n#rmse and sse for default bagging (first bagtree)\npreds_tree3 = predict(bag_tree, test_x.t)\n\nrmse_bagtree = sqrt(mean(preds_tree3 - test_y.t)^2)\nsse_bagtree = sum((preds_tree3 - test_y.t)^2)\nprint(rmse_bagtree)\n\n\n[1] 0.01477689\n\n\nCode\nprint(sse_bagtree)\n\n\n[1] 60813.53\n\n\nWARNING: Takes time to run\n\n\nCode\n#loop for plotting ntree vs rmse\n\nstart_time = Sys.time()\n\n# assess 10-50 bagged trees\nntree &lt;- 10:50\n\n# create empty vector to store OOB RMSE values\nRMSE &lt;- vector(mode = \"numeric\", length = length(ntree))\n\nfor (i in seq_along(ntree)) {\n  # reproducibility\n  set.seed(123)\n  \n  # training bagged models\n  bagged_models &lt;- bagging(\n  formula = train_y.t ~ .,\n  data    = train_x.t,\n  coob    = TRUE,\n  nbagg   = ntree[i]\n)\n  # get OOB error\n  RMSE[i] &lt;- bagged_models$err\n}\n\nprint(Sys.time() - start_time)\n\n\nTime difference of 23.64319 mins\n\n\n\n\nCode\n#plot ntree vs RMSE\nplot(ntree, RMSE, type = 'l', lwd = 2)\nabline(v = 11, col = \"red\", lty = \"dashed\")\nabline(v = 25, col = \"blue\", lty = \"dashed\") #25 is default number of bootstraps\n\n\n\n\n\nWe can see that the error drops and suddenly rises. After that the fall is slower. Given that 11 bootstraps give the lowest error before the default of 25, we can use it to reduce computation time.\n\n\nCode\nmin(RMSE) #the minimum eror is not significantly lower\n\n\n[1] 1.917357\n\n\n\n\nCode\n#training new bagged model\nset.seed(123)\n\n# train bagged model with 11 trees\nbag_tree2 &lt;- bagging(\n  formula = train_y.t ~ .,\n  data    = train_x.t,\n  coob    = TRUE,\n  nbagg   = 11\n)\n\nbag_tree2\n\n\n\nBagging regression trees with 11 bootstrap replications \n\nCall: bagging.data.frame(formula = train_y.t ~ ., data = train_x.t, \n    coob = TRUE, nbagg = 11)\n\nOut-of-bag estimate of root mean squared error:  1.9174 \n\n\n\n\nCode\n#rmse and sse for optimal bagging\npreds_tree4 = predict(bag_tree2, test_x.t)\n\nrmse_bagtree2 = sqrt(mean(preds_tree4 - test_y.t)^2)\nsse_bagtree2 = sum((preds_tree4 - test_y.t)^2)\nprint(rmse_bagtree2)\n\n\n[1] 0.01353659\n\n\nCode\nprint(sse_bagtree2)\n\n\n[1] 60816.51"
  },
  {
    "objectID": "ml_procurement.html#support-vector-machine",
    "href": "ml_procurement.html#support-vector-machine",
    "title": "Procurement Contracts",
    "section": "Support Vector Machine",
    "text": "Support Vector Machine\n\n\nCode\n#library e1071 for SVM\nlibrary(e1071)\n\n\n\n\nCode\n#Plotting nrc vs bidders and looking at classes\nProcurement3 %&gt;% \n  filter(!(ca_bids_all &gt;= 50)) %&gt;% \n  mutate(above200k = recode_factor(ca_contract_valuec,\n                                \"200.000-\"=\"Yes\",\n                                .default = \"No\")) %&gt;% \n  ggplot(aes(x=ca_bids_all, \n             y=nrc,\n             color=above200k)) +\n  geom_point(size = 2) +\n  scale_color_manual(values = c(\"#FF0000\", \"#000000\")) +\n  labs(title = \"Value\", x = \"bidders\", y = \"nrc\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\nstr(Procure_scaled)\n\n\n'data.frame':   161125 obs. of  39 variables:\n $ ca_contract_valuec: chr  \"50.000-199.999\" \"200.000-\" \"200.000-\" \"200.000-\" ...\n $ lca_contract_value: num  12 12.4 12.3 13.5 11.3 ...\n $ AFR               : num  1 1 1 1 1 1 1 1 1 1 ...\n $ EAP               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ ECA               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ LCR               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ MNA               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ SAR               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ ca_Consul.Service : num  0 0 0 1 0 0 0 0 0 0 ...\n $ ca_Goods          : num  0 0 0 0 1 0 1 1 1 1 ...\n $ ca_bids_all       : num  0.553 -0.14 -0.14 -0.603 0.322 ...\n $ ca_proc_open      : num  1 1 1 0 1 1 0 1 1 1 ...\n $ ca_proc_quality   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ ca_proc_source    : num  0 0 0 1 0 0 1 0 0 0 ...\n $ ca_proc_restricted: num  0 0 0 0 0 0 0 0 0 0 ...\n $ singleb           : num  0 0 0 1 0 0 1 0 0 0 ...\n $ corr_signp1       : num  1 0 0 0 1 0 0 1 1 0 ...\n $ corr_signp2       : int  0 0 0 0 0 0 1 0 0 1 ...\n $ corr_signp3       : int  0 0 0 1 0 0 0 0 0 0 ...\n $ nrc               : num  -0.841 -0.841 -0.841 -0.841 -0.841 ...\n $ taxhaven          : num  0 0 0 0 0 0 0 0 0 0 ...\n $ not.taxhaven      : num  0 0 0 1 1 0 1 1 1 0 ...\n $ cri_wb.d0         : num  1 0 0 0 1 0 0 1 1 0 ...\n $ cri_wb.d25        : num  0 1 1 0 0 1 0 0 0 1 ...\n $ cri_wb.d50        : num  0 0 0 0 0 0 0 0 0 0 ...\n $ cri_wb.d75        : num  0 0 0 1 0 0 1 0 0 0 ...\n $ P                 : num  1 0 0 0 0 1 0 0 0 0 ...\n $ Health            : num  0 1 0 0 0 0 1 0 0 0 ...\n $ Edu               : num  0 0 1 0 0 0 0 0 0 0 ...\n $ W                 : num  0 0 0 1 0 0 0 0 0 0 ...\n $ I                 : num  0 0 0 0 1 0 0 1 0 0 ...\n $ Tran              : num  0 0 0 0 0 0 0 0 1 0 ...\n $ Fin               : num  0 0 0 0 0 0 0 0 0 1 ...\n $ A                 : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Ener              : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Info              : num  0 0 0 0 0 0 0 0 0 0 ...\n $ H                 : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Multi             : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Priv              : num  0 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:13] 59655 59815 138472 138473 138474 138475 138476 138477 138478 138479 ...\n  ..- attr(*, \"names\")= chr [1:13] \"59655\" \"59815\" \"138472\" \"138473\" ...\n\n\nWe need scaled data, because the svm models take very long to run.\n\n\nCode\n#Procurement.svm\nProcurement.svm = Procure_scaled %&gt;% \n  mutate(abv200k = factor(ifelse(ca_contract_valuec==\"200.000-\", 1, 0))) %&gt;% \n  select(abv200k, \n         nrc, \n         ca_bids_all, \n         P) %&gt;% \n  mutate_at(vars(ca_bids_all, nrc), \n            funs(as.numeric(.)))\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\n\n\nCode\n#subsetting data for svm\nidx.svm = 1:nrow(Procurement.svm)\n\nset.seed(12345) #for random\ntrain_idx.svm = sample(idx.svm, round(0.50*nrow(Procurement.svm)), replace = FALSE) #takes sample of 50% of rows without replacement\n\ntest_idx.svm = sample(setdiff(idx.svm, train_idx.svm), round(0.10*nrow(Procurement.svm))) #takes 10% from the remaining rows for test\n\ntune_idx.svm = sample(idx.svm, round(0.03*nrow(Procurement.svm)), replace = FALSE) #index of the obs that we will use for tuning. Only 3%, because it is very time-consuming.\n\ntest_idx.svm2 = sample(setdiff(idx.svm, tune_idx.svm), round(0.10*nrow(Procurement.svm)))\n\n\n\n\nCode\nhead(Procurement.svm[train_idx.svm,])\n\n\n       abv200k        nrc ca_bids_all P\n122931       0 -0.4101392  1.01534588 0\n98909        1 -0.1583185 -0.14045500 0\n65611        1  0.9206223  0.09070517 0\n95847        1 -0.4101392  2.63346713 0\n91914        1 -0.7741290 -0.37161518 0\n62368        1  0.9206223 -0.14045500 0\n\n\n\n\nCode\n#training first svm model\nset.seed(123)\n# sample training data and fit model\nsvm.m1 &lt;- svm(abv200k~ ., data = Procurement.svm[train_idx.svm,], kernel = \"radial\")\n\n\n\n\nCode\npreds_svm1 = predict(svm.m1, newdata = Procurement.svm[test_idx.svm,])\n\n\n\n\nCode\nhead(preds_svm1)\n\n\n119261 139363 107621 129186  93690  85110 \n     1      1      0      0      1      0 \nLevels: 0 1\n\n\n\n\nCode\n#pred.rate.svm\npred.rate.svm = mean(preds_svm1 == Procurement.svm[test_idx.svm,]$abv200k)\n\nprint(pred.rate.svm)\n\n\n[1] 0.6296549\n\n\nWARNING: The next chunk is very time-consuming.\n\n\nCode\n#tuning svm using tune function and ranges of pars\n\nstart_time = Sys.time()\n\nset.seed(12345)\nsvm.tune &lt;- tune(svm, abv200k~., data = Procurement.svm[tune_idx.svm,], kernel = \"radial\",\n                 ranges = list(cost = c(0.1,1,10,100),\n                 gamma = c(0.25,0.5,1,2)))\n\nsvm.tune$best.model\n\n\n\nCall:\nbest.tune(METHOD = svm, train.x = abv200k ~ ., data = Procurement.svm[tune_idx.svm, \n    ], ranges = list(cost = c(0.1, 1, 10, 100), gamma = c(0.25, 0.5, \n    1, 2)), kernel = \"radial\")\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  3783\n\n\nCode\n#Takes forever to run, I used a small sample to find the best model to save time.\n\nprint(Sys.time() - start_time)\n\n\nTime difference of 3.309396 mins\n\n\n\n\nCode\npreds_svm2 = predict(svm.tune$best.model, newdata = Procurement.svm[test_idx.svm2,])\n\n\n\n\nCode\n#tuned pred.rate.svm2\n\npred.rate.svm2 = mean(preds_svm2 == Procurement.svm[test_idx.svm2,]$abv200k)\n\nprint(pred.rate.svm2)\n\n\n[1] 0.6310204"
  },
  {
    "objectID": "ml_procurement.html#comparing-rmse-sse-prediction-rate-and-conclusion",
    "href": "ml_procurement.html#comparing-rmse-sse-prediction-rate-and-conclusion",
    "title": "Procurement Contracts",
    "section": "Comparing RMSE, SSE, & Prediction rate and Conclusion",
    "text": "Comparing RMSE, SSE, & Prediction rate and Conclusion\n\n\nCode\n# creating Errdf\n\nrmse.var = c(rmse_r, new_rmse.r, rmse_spline.bids, rmse_sp.gam3, rmse_tunedtree, rmse_bagtree2)\nsse.var = c(sse_r, new_sse.r, sse_spline.bids, sse_sp.gam3, sse_tunedtree, sse_bagtree2)\nmodel.var = c(\"lm_ridge.c\", \"lm_ridge.c2\", \"spline_model.bids\", \"spline_model.gam3\", \"tuned_tree\", \"bag_tree2\")\n\nErrdf = data.frame(model.var, rmse.var, sse.var)\nErrdf\n\n\n          model.var   rmse.var  sse.var\n1        lm_ridge.c 0.01447194 56996.77\n2       lm_ridge.c2 0.01409955 57044.26\n3 spline_model.bids 0.01356418 70454.60\n4 spline_model.gam3 0.01400472 56856.29\n5        tuned_tree 0.01486034 60814.71\n6         bag_tree2 0.01353659 60816.51\n\n\n\n\nCode\n#Plot RMSE\n\nErrdf %&gt;% \n  group_by(rmse.var, sse.var) %&gt;% \n  ggplot(aes(x = rmse.var, \n             xend = 0.013, \n             y = reorder(model.var, desc(rmse.var)), \n             yend=model.var,\n             label=round(rmse.var, 4))) +\n  geom_segment() +\n  geom_point() +\n  geom_text(nudge_x = 0, nudge_y = 0.1) +\n  labs(title = \"RMSE by model\", x = \"RMSE\", y = \"Models\")\n\n\n\n\n\n\n\nCode\n#Plot SSE\n\nErrdf %&gt;% \n  ggplot(aes(x = sse.var, \n             xend = 50000, \n             y = reorder(model.var, desc(sse.var)), \n             yend=model.var,\n             label=round(sse.var, 1))) +\n  geom_segment() +\n  geom_point() +\n  geom_text(nudge_x = 0, nudge_y = 0.1) +\n  labs(title = \"SSE by model\", x = \"SSE\", y = \"Models\")\n\n\n\n\n\n\n\nCode\n#creating Errdf2\n\npred.rate = c(pred.rate.log, pred.rate.knn3, pred.rate.svm2)\npred.model = c(\"log_model_train.2\", \"knn3\", \"svm.tune\")\n\nErrdf2 = data.frame(pred.rate, pred.model)\nErrdf2\n\n\n  pred.rate        pred.model\n1 0.6688758 log_model_train.2\n2 0.7591205              knn3\n3 0.6310204          svm.tune\n\n\n\n\nCode\n#Plot for classifications correctness rates\n\nErrdf2 %&gt;% \n  ggplot(aes(pred.model, pred.rate, label = round(pred.rate, 2))) +\n  geom_bar(stat = \"identity\", width = 0.5) +\n  geom_text(nudge_x = 0, nudge_y = 0.05) +\n  labs(title = \"Prediction Rates\", x = \"Methods of Classification\", y = \"Correct Predictions\") +\n  theme_minimal()"
  },
  {
    "objectID": "ml_procurement.html#further-discussion",
    "href": "ml_procurement.html#further-discussion",
    "title": "Procurement Contracts",
    "section": "Further discussion",
    "text": "Further discussion\nThere is a variable for period between the time of award of the contract and the signing of the said contract. I excluded the variable for now, since there are many missing values and the effect on the value is unclear.\n\n\nCode\nsummary(Procurement$ca_signper)\n\n\n   Length     Class      Mode \n   248000 character character \n\n\nCode\ntest = Procurement\ntest$ca_signper &lt;- as.numeric(Procurement$ca_signper)\n\n\nWarning: NAs introduced by coercion\n\n\nCode\nsummary(test$ca_signper)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.      NA's \n-29231.00      5.00     20.00     30.14     48.00   4384.00     11857 \n\n\nCode\ntest = test %&gt;% \n  filter(!is.na(cri_wb))\n\n\n\n\nCode\n4436/161465\n\n\n[1] 0.02747345\n\n\n\n\nCode\ntest %&gt;% \n  filter(!(ca_signper &lt;= -500)) %&gt;%\n  ggplot(aes(x=ca_signper)) + \n  geom_density()\n\n\n\n\n\n\n\nCode\ntest %&gt;% \n  filter(!is.na(ca_signper)) %&gt;% \n  filter(ca_signper &lt;= -900) %&gt;% \n  count()\n\n\n    n\n1 195\n\n\n\n\nCode\ntest %&gt;% \n  filter(!is.na(ca_signper)) %&gt;% \n  filter(ca_signper &gt;= -900) %&gt;%\n  ggplot(aes(x=ca_signper, y=lca_contract_value)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 13 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 13 rows containing missing values (geom_point)."
  },
  {
    "objectID": "sanctions.html",
    "href": "sanctions.html",
    "title": "Code For When Do Economic Snactions Work?",
    "section": "",
    "text": "Code\nlibrary(readxl)\nGSDB &lt;- read_excel(\"GSDB_V2.xls\")\nView(GSDB)"
  },
  {
    "objectID": "sanctions.html#data-manipulation",
    "href": "sanctions.html#data-manipulation",
    "title": "Code For When Do Economic Snactions Work?",
    "section": "Data Manipulation",
    "text": "Data Manipulation\n\n\n    case_id     sanctioned_state   sanctioning_state      begin     \n Min.   :   1   Length:1101        Length:1101        Min.   :1949  \n 1st Qu.: 276   Class :character   Class :character   1st Qu.:1982  \n Median : 551   Mode  :character   Mode  :character   Median :1996  \n Mean   : 551                                         Mean   :1993  \n 3rd Qu.: 826                                         3rd Qu.:2009  \n Max.   :1101                                         Max.   :2019  \n      end           trade        descr_trade             arms       \n Min.   :1951   Min.   :0.0000   Length:1101        Min.   :0.0000  \n 1st Qu.:1990   1st Qu.:0.0000   Class :character   1st Qu.:0.0000  \n Median :2001   Median :0.0000   Mode  :character   Median :0.0000  \n Mean   :2000   Mean   :0.3697                      Mean   :0.2207  \n 3rd Qu.:2016   3rd Qu.:1.0000                      3rd Qu.:0.0000  \n Max.   :2019   Max.   :1.0000                      Max.   :1.0000  \n    military        financial          travel           other       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.2007   Mean   :0.4986   Mean   :0.2116   Mean   :0.1689  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  objective           success         \n Length:1101        Length:1101       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.5     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(stringr)\n\nSansBinary = dplyr::filter(GSDB, grepl(\"success_total|failed|success_part\", success)) #consists of sans with conclusion\n\nSansUnit = filter(SansBinary, str_count(SansBinary$objective, \",\")==0) #single objective\n\nSansUnit &lt;- SansUnit %&gt;% \n  mutate(SuccessBinary = ifelse(grepl(\"success_total\", success), 2, ifelse(grepl(\"failed\", success), 0, 1))) #gives numeric values to success outcomes\n\n#SansBinary &lt;- SansBinary %&gt;% \n#  mutate(SuccessBinary = ifelse(!grepl(\"[^success_total]\", success), 2, ifelse(!grepl(\"[^failed]\", success), 0, 1)))\n\n\n\n\nCode\nSansBinary2 = dplyr::filter(GSDB, grepl(\"success_total|failed\", success))\n\nSansBinary2 = filter(SansBinary2, str_count(SansBinary2$objective, \",\")==0)\n\nSansBinary2 &lt;- SansBinary2 %&gt;% \n  mutate(SuccessBinary2 = ifelse(grepl(\"success_total\", success), 1, 0))\n\n#only total success or total failure\n\n\n\n\nCode\nFreedom_Status_Data &lt;- read_excel(\"Freedom Status Data.xlsx\", \n    sheet = \"Country Ratings, Statuses \", \n    skip = 1)\n\n\n\n\nCode\nhead(Freedom_Status_Data)\n\n\n# A tibble: 6 × 148\n  Countries            ...2  ...3  Regime...4 ...5  ...6  Regime...7 ...8  ...9 \n  &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;\n1 Year(s) Under Review &lt;NA&gt;  &lt;NA&gt;  1972       &lt;NA&gt;  &lt;NA&gt;  1973       &lt;NA&gt;  &lt;NA&gt; \n2 Metrics              PR    CL    Status     PR    CL    Status     PR    CL   \n3 Afghanistan          4     5     PF         7     6     NF         7     6    \n4 Albania              7     7     NF         7     7     NF         7     7    \n5 Algeria              6     6     NF         6     6     NF         6     6    \n6 Andorra              4     3     PF         4     4     PF         4     4    \n# … with 139 more variables: Regime...10 &lt;chr&gt;, ...11 &lt;chr&gt;, ...12 &lt;chr&gt;,\n#   Regime...13 &lt;chr&gt;, ...14 &lt;chr&gt;, ...15 &lt;chr&gt;, Regime...16 &lt;chr&gt;,\n#   ...17 &lt;chr&gt;, ...18 &lt;chr&gt;, Regime...19 &lt;chr&gt;, ...20 &lt;chr&gt;, ...21 &lt;chr&gt;,\n#   Regime...22 &lt;chr&gt;, ...23 &lt;chr&gt;, ...24 &lt;chr&gt;, Regime...25 &lt;chr&gt;,\n#   ...26 &lt;chr&gt;, ...27 &lt;chr&gt;, Regime...28 &lt;chr&gt;, ...29 &lt;chr&gt;, ...30 &lt;chr&gt;,\n#   Regime...31 &lt;chr&gt;, ...32 &lt;chr&gt;, ...33 &lt;chr&gt;, Regime...34 &lt;chr&gt;,\n#   ...35 &lt;chr&gt;, ...36 &lt;chr&gt;, Regime...37 &lt;chr&gt;, ...38 &lt;chr&gt;, ...39 &lt;chr&gt;, …\n\n\n\n\nCode\nunique(SansUnit$sanctioned_state)\n\n\n  [1] \"Afghanistan\"                                   \n  [2] \"Albania\"                                       \n  [3] \"Algeria\"                                       \n  [4] \"Angola\"                                        \n  [5] \"Antigua and Barbuda\"                           \n  [6] \"Argentina\"                                     \n  [7] \"Australia\"                                     \n  [8] \"Austria\"                                       \n  [9] \"Azerbaijan\"                                    \n [10] \"Belarus\"                                       \n [11] \"Belize\"                                        \n [12] \"Benin\"                                         \n [13] \"Bolivia\"                                       \n [14] \"Bosnia and Herzegovina\"                        \n [15] \"Brazil\"                                        \n [16] \"Bulgaria\"                                      \n [17] \"Cambodia\"                                      \n [18] \"Cameroon\"                                      \n [19] \"Canada\"                                        \n [20] \"Central African Republic\"                      \n [21] \"Ceylon\"                                        \n [22] \"Chad\"                                          \n [23] \"Chile\"                                         \n [24] \"China\"                                         \n [25] \"Colombia\"                                      \n [26] \"Congo (Brazzaville)\"                           \n [27] \"Congo, Democratic Republic of the\"             \n [28] \"Costa Rica\"                                    \n [29] \"Cote d'Ivoire\"                                 \n [30] \"Croatia\"                                       \n [31] \"Cuba\"                                          \n [32] \"Cyprus\"                                        \n [33] \"Denmark\"                                       \n [34] \"Dominica\"                                      \n [35] \"Dominican Republic\"                            \n [36] \"EEC\"                                           \n [37] \"EU\"                                            \n [38] \"Ecuador\"                                       \n [39] \"Egypt, Arab Rep.\"                              \n [40] \"El Salvador\"                                   \n [41] \"El Salvador, Honduras\"                         \n [42] \"Eritrea\"                                       \n [43] \"Estonia\"                                       \n [44] \"Estonia, Latvia, Lithuania\"                    \n [45] \"Ethiopia (excludes Eritrea)\"                   \n [46] \"Fiji\"                                          \n [47] \"Finland\"                                       \n [48] \"France\"                                        \n [49] \"Gambia, The\"                                   \n [50] \"Georgia\"                                       \n [51] \"German Democratic Republic\"                    \n [52] \"Germany\"                                       \n [53] \"Grenada\"                                       \n [54] \"Guatemala\"                                     \n [55] \"Guinea\"                                        \n [56] \"Guinea-Bissau\"                                 \n [57] \"Haiti\"                                         \n [58] \"Honduras\"                                      \n [59] \"ICC Rome Statute Signatories\"                  \n [60] \"India\"                                         \n [61] \"India, Pakistan\"                               \n [62] \"Indonesia\"                                     \n [63] \"Iran\"                                          \n [64] \"Iraq\"                                          \n [65] \"Ireland\"                                       \n [66] \"Israel\"                                        \n [67] \"Italy\"                                         \n [68] \"Japan\"                                         \n [69] \"Kazakhstan\"                                    \n [70] \"Kenya\"                                         \n [71] \"Korea, North\"                                  \n [72] \"Korea, South\"                                  \n [73] \"Kuwait\"                                        \n [74] \"Kyrgyzstan\"                                    \n [75] \"Laos\"                                          \n [76] \"Latvia\"                                        \n [77] \"Lebanon\"                                       \n [78] \"Lesotho\"                                       \n [79] \"Liberia\"                                       \n [80] \"Libya\"                                         \n [81] \"Lithuania\"                                     \n [82] \"Malagasy Republic\"                             \n [83] \"Malawi\"                                        \n [84] \"Malaya\"                                        \n [85] \"Mali\"                                          \n [86] \"Mauritania\"                                    \n [87] \"Mozambique\"                                    \n [88] \"Myanmar\"                                       \n [89] \"Nepal\"                                         \n [90] \"Netherlands\"                                   \n [91] \"New Zealand\"                                   \n [92] \"Nicaragua\"                                     \n [93] \"Niger\"                                         \n [94] \"Nigeria\"                                       \n [95] \"North Vietnam\"                                 \n [96] \"Norway\"                                        \n [97] \"Pakistan\"                                      \n [98] \"Palestine\"                                     \n [99] \"Panama\"                                        \n[100] \"Paraguay\"                                      \n[101] \"Peru\"                                          \n[102] \"Philippines\"                                   \n[103] \"Poland, Hungary, Czechoslovakia\"               \n[104] \"Portugal\"                                      \n[105] \"Rhodesia\"                                      \n[106] \"Romania\"                                       \n[107] \"Russia\"                                        \n[108] \"Rwanda\"                                        \n[109] \"Sierra Leone\"                                  \n[110] \"Singapore\"                                     \n[111] \"South Africa\"                                  \n[112] \"South Vietnam\"                                 \n[113] \"Soviet Union\"                                  \n[114] \"Sudan\"                                         \n[115] \"Sweden\"                                        \n[116] \"Switzerland\"                                   \n[117] \"Syria\"                                         \n[118] \"Taiwan\"                                        \n[119] \"Tajikistan\"                                    \n[120] \"Tanzania\"                                      \n[121] \"Terrorist Organizations (Taliban and Al-Qaeda)\"\n[122] \"Thailand\"                                      \n[123] \"Thailand, South Vietnam\"                       \n[124] \"Togo\"                                          \n[125] \"Transjordan\"                                   \n[126] \"Tunisia\"                                       \n[127] \"Turkey\"                                        \n[128] \"Uganda\"                                        \n[129] \"Ukraine\"                                       \n[130] \"United Kingdom\"                                \n[131] \"United States\"                                 \n[132] \"Uruguay\"                                       \n[133] \"Uzbekistan\"                                    \n[134] \"Western countries\"                             \n[135] \"Yemen, North\"                                  \n[136] \"Yugoslavia\"                                    \n[137] \"Zambia\"                                        \n[138] \"Zimbabwe\"                                      \n\n\nCode\nmatched &lt;- intersect(unique(SansUnit$sanctioned_state), Freedom_Status_Data$Countries)\nall &lt;-  union(unique(SansUnit$sanctioned_state), Freedom_Status_Data$Countries)\nnon.matched &lt;- data.frame(all[!all %in% matched]); non.matched\n\n\n                            all..all..in..matched.\n1                                           Ceylon\n2                Congo, Democratic Republic of the\n3                                              EEC\n4                                               EU\n5                                 Egypt, Arab Rep.\n6                            El Salvador, Honduras\n7                       Estonia, Latvia, Lithuania\n8                      Ethiopia (excludes Eritrea)\n9                                      Gambia, The\n10                      German Democratic Republic\n11                    ICC Rome Statute Signatories\n12                                 India, Pakistan\n13                                    Korea, North\n14                                    Korea, South\n15                               Malagasy Republic\n16                                          Malaya\n17                                   North Vietnam\n18                                       Palestine\n19                 Poland, Hungary, Czechoslovakia\n20                                        Rhodesia\n21                                   South Vietnam\n22                                    Soviet Union\n23  Terrorist Organizations (Taliban and Al-Qaeda)\n24                         Thailand, South Vietnam\n25                                     Transjordan\n26                               Western countries\n27                                    Yemen, North\n28                            Year(s) Under Review\n29                                         Metrics\n30                                         Andorra\n31                                         Armenia\n32                                         Bahamas\n33                                         Bahrain\n34                                      Bangladesh\n35                                        Barbados\n36                                         Belgium\n37                                          Bhutan\n38                                        Botswana\n39                                          Brunei\n40                                    Burkina Faso\n41                                         Burundi\n42                                      Cabo Verde\n43                                         Comoros\n44                                Congo (Kinshasa)\n45                                  Czech Republic\n46                                  Czechoslovakia\n47                                        Djibouti\n48                                           Egypt\n49                               Equatorial Guinea\n50                                        Eswatini\n51                                        Ethiopia\n52                                           Gabon\n53                                     Germany, E.\n54                                     Germany, W.\n55                                           Ghana\n56                                          Greece\n57                                          Guyana\n58                                         Hungary\n59                                         Iceland\n60                                         Jamaica\n61                                          Jordan\n62                                        Kiribati\n63                                          Kosovo\n64                                   Liechtenstein\n65                                      Luxembourg\n66                                 North Macedonia\n67                                      Madagascar\n68                                        Malaysia\n69                                        Maldives\n70                                           Malta\n71                                Marshall Islands\n72                                       Mauritius\n73                                          Mexico\n74                                      Micronesia\n75                                         Moldova\n76                                          Monaco\n77                                        Mongolia\n78                                      Montenegro\n79                                         Morocco\n80                                         Namibia\n81                                           Nauru\n82                                     North Korea\n83                                            Oman\n84                                           Palau\n85                                Papua New Guinea\n86                                          Poland\n87                                           Qatar\n88                                           Samoa\n89                                      San Marino\n90                           Sao Tome and Principe\n91                                    Saudi Arabia\n92                                         Senegal\n93                                          Serbia\n94                           Serbia and Montenegro\n95                                      Seychelles\n96                                        Slovakia\n97                                        Slovenia\n98                                 Solomon Islands\n99                                         Somalia\n100                                    South Korea\n101                                    South Sudan\n102                                          Spain\n103                                      Sri Lanka\n104                            St. Kitts and Nevis\n105                                      St. Lucia\n106                 St. Vincent and the Grenadines\n107                                       Suriname\n108                                     The Gambia\n109                                    Timor-Leste\n110                                          Tonga\n111                            Trinidad and Tobago\n112                                   Turkmenistan\n113                                         Tuvalu\n114                           United Arab Emirates\n115                                           USSR\n116                                        Vanuatu\n117                                      Venezuela\n118                                        Vietnam\n119                                    Vietnam, N.\n120                                    Vietnam, S.\n121                                          Yemen\n122                                      Yemen, N.\n123                                      Yemen, S.\n\n\n\n\nCode\n#Trade share data\nAPI_NE_TRD_GNFS &lt;- read_excel(\"API_NE.TRD.GNFS.xlsx\", col_types = c(\"text\", \"skip\", \"text\", \n         \"skip\", \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\", \"numeric\", \"numeric\", \n         \"numeric\"), skip = 4)\nhead(API_NE_TRD_GNFS)\n\n\n# A tibble: 6 × 63\n  `Country Name`      `Indicator Name` `1960` `1961` `1962` `1963` `1964` `1965`\n  &lt;chr&gt;               &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Aruba               Trade (% of GDP)   NA     NA     NA     NA     NA     NA  \n2 Africa Eastern and… Trade (% of GDP)   NA     NA     NA     NA     NA     NA  \n3 Afghanistan         Trade (% of GDP)   11.2   12.6   14.2   26.0   26.9   32.7\n4 Africa Western and… Trade (% of GDP)   33.2   33.1   29.4   29.9   31.4   33.9\n5 Angola              Trade (% of GDP)   NA     NA     NA     NA     NA     NA  \n6 Albania             Trade (% of GDP)   NA     NA     NA     NA     NA     NA  \n# … with 55 more variables: `1966` &lt;dbl&gt;, `1967` &lt;dbl&gt;, `1968` &lt;dbl&gt;,\n#   `1969` &lt;dbl&gt;, `1970` &lt;dbl&gt;, `1971` &lt;dbl&gt;, `1972` &lt;dbl&gt;, `1973` &lt;dbl&gt;,\n#   `1974` &lt;dbl&gt;, `1975` &lt;dbl&gt;, `1976` &lt;dbl&gt;, `1977` &lt;dbl&gt;, `1978` &lt;dbl&gt;,\n#   `1979` &lt;dbl&gt;, `1980` &lt;dbl&gt;, `1981` &lt;dbl&gt;, `1982` &lt;dbl&gt;, `1983` &lt;dbl&gt;,\n#   `1984` &lt;dbl&gt;, `1985` &lt;dbl&gt;, `1986` &lt;dbl&gt;, `1987` &lt;dbl&gt;, `1988` &lt;dbl&gt;,\n#   `1989` &lt;dbl&gt;, `1990` &lt;dbl&gt;, `1991` &lt;dbl&gt;, `1992` &lt;dbl&gt;, `1993` &lt;dbl&gt;,\n#   `1994` &lt;dbl&gt;, `1995` &lt;dbl&gt;, `1996` &lt;dbl&gt;, `1997` &lt;dbl&gt;, `1998` &lt;dbl&gt;, …\n\n\n\n\nCode\n#identifying non-matching country names\nmatched2 &lt;- intersect(unique(SansUnit$sanctioned_state), API_NE_TRD_GNFS$`Country Name`)\nall2 &lt;-  union(unique(SansUnit$sanctioned_state), API_NE_TRD_GNFS$`Country Name`)\nnon.matched2 &lt;- data.frame(all2[!all %in% matched2]); non.matched2\n\n\n                              all2..all..in..matched2.\n1                                               Ceylon\n2                                  Congo (Brazzaville)\n3                    Congo, Democratic Republic of the\n4                                                  EEC\n5                                                   EU\n6                                El Salvador, Honduras\n7                           Estonia, Latvia, Lithuania\n8                          Ethiopia (excludes Eritrea)\n9                           German Democratic Republic\n10                        ICC Rome Statute Signatories\n11                                     India, Pakistan\n12                                                Iran\n13                                        Korea, North\n14                                        Korea, South\n15                                          Kyrgyzstan\n16                                                Laos\n17                                   Malagasy Republic\n18                                              Malaya\n19                                       North Vietnam\n20                                           Palestine\n21                     Poland, Hungary, Czechoslovakia\n22                                            Rhodesia\n23                                              Russia\n24                                       South Vietnam\n25                                        Soviet Union\n26                                               Syria\n27                                              Taiwan\n28      Terrorist Organizations (Taliban and Al-Qaeda)\n29                             Thailand, South Vietnam\n30                                         Transjordan\n31                                   Western countries\n32                                        Yemen, North\n33                                          Yugoslavia\n34                                               Aruba\n35                         Africa Eastern and Southern\n36                          Africa Western and Central\n37                                             Andorra\n38                                          Arab World\n39                                United Arab Emirates\n40                                             Armenia\n41                                      American Samoa\n42                                             Burundi\n43                                             Belgium\n44                                        Burkina Faso\n45                                          Bangladesh\n46                                             Bahrain\n47                                        Bahamas, The\n48                                             Bermuda\n49                                            Barbados\n50                                   Brunei Darussalam\n51                                              Bhutan\n52                                            Botswana\n53                      Central Europe and the Baltics\n54                                     Channel Islands\n55                                    Congo, Dem. Rep.\n56                                         Congo, Rep.\n57                                             Comoros\n58                                          Cabo Verde\n59                              Caribbean small states\n60                                             Curacao\n61                                      Cayman Islands\n62                                      Czech Republic\n63                                            Djibouti\n64         East Asia & Pacific (excluding high income)\n65                          Early-demographic dividend\n66                                 East Asia & Pacific\n67       Europe & Central Asia (excluding high income)\n68                               Europe & Central Asia\n69                                           Euro area\n70                                               Spain\n71                                            Ethiopia\n72                                      European Union\n73            Fragile and conflict affected situations\n74                                       Faroe Islands\n75                               Micronesia, Fed. Sts.\n76                                               Gabon\n77                                               Ghana\n78                                           Gibraltar\n79                                   Equatorial Guinea\n80                                              Greece\n81                                           Greenland\n82                                                Guam\n83                                              Guyana\n84                                         High income\n85                                Hong Kong SAR, China\n86              Heavily indebted poor countries (HIPC)\n87                                             Hungary\n88                                           IBRD only\n89                                    IDA & IBRD total\n90                                           IDA total\n91                                           IDA blend\n92                                            IDA only\n93                                         Isle of Man\n94                                      Not classified\n95                                  Iran, Islamic Rep.\n96                                             Iceland\n97                                             Jamaica\n98                                              Jordan\n99                                     Kyrgyz Republic\n100                                           Kiribati\n101                                St. Kitts and Nevis\n102                                        Korea, Rep.\n103  Latin America & Caribbean (excluding high income)\n104                                            Lao PDR\n105                                          St. Lucia\n106                          Latin America & Caribbean\n107       Least developed countries: UN classification\n108                                         Low income\n109                                      Liechtenstein\n110                                          Sri Lanka\n111                                Lower middle income\n112                                Low & middle income\n113                          Late-demographic dividend\n114                                         Luxembourg\n115                                   Macao SAR, China\n116                           St. Martin (French part)\n117                                            Morocco\n118                                             Monaco\n119                                            Moldova\n120                                         Madagascar\n121                                           Maldives\n122                         Middle East & North Africa\n123                                             Mexico\n124                                   Marshall Islands\n125                                      Middle income\n126                                    North Macedonia\n127                                              Malta\n128 Middle East & North Africa (excluding high income)\n129                                         Montenegro\n130                                   French Polynesia\n131                                            Senegal\n132                                    Solomon Islands\n133                                           Suriname\n134                                    Slovak Republic\n135                                         Seychelles\n136         East Asia & Pacific (IDA & IBRD countries)\n137       Europe & Central Asia (IDA & IBRD countries)\n138                            South Asia (IDA & IBRD)\n139                              Virgin Islands (U.S.)\n140                                            Vanuatu\n141                                              Samoa\n\n\n\n\nCode\nSansOngoing &lt;- GSDB %&gt;% \n  mutate(StatusOngoing = ifelse(success == \"ongoing\", 1, 0))\n\nsummary(SansOngoing$StatusOngoing)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.1426  0.0000  1.0000 \n\n\n\n\nCode\n#creating a list of all sanctioned states that are needed in the dataset\nlibrary(writexl)\nSanctioned_States = data.frame(unique(SansUnit$sanctioned_state))\nwrite_xlsx(Sanctioned_States, \"Sanctioned_States.xlsx\")\n\n\n\n\nCode\n#creating duration\nSansUnit &lt;- SansUnit %&gt;% \n  mutate(duration = end - begin) %&gt;% \n  mutate(duration = replace(duration, duration == 0, 1))\n\n\n\n\nCode\nSansBinary2 &lt;- SansBinary2 %&gt;% \n  mutate(duration = end - begin) %&gt;% \n  mutate(duration = replace(duration, duration == 0, 1))\n\nSansBinary2 &lt;- SansBinary2 %&gt;% \n  mutate(comprehensive = ifelse(grepl(\"exp_compl, imp_compl\", descr_trade), 1, 0))\n\n#library(stringr)\nSansBinary2 &lt;- SansBinary2 %&gt;% \n  mutate(commitment = str_count(SansBinary2$sanctioning_state, \",\")) %&gt;% \n  mutate(commitment = commitment + 1)\n\n\n\n\nCode\nSansUnit &lt;- SansUnit %&gt;% \n  mutate(comprehensive = ifelse(grepl(\"exp_compl, imp_compl\", descr_trade), 1, 0))\n\n\n\n\nCode\n#commitment variable, counting the number of countries participating\nSansUnit &lt;- SansUnit %&gt;% \n  mutate(commitment = str_count(SansUnit$sanctioning_state, \",\")) %&gt;% \n  mutate(commitment = commitment + 1)\n\n\n\n\nCode\nMostSanctioned = data.frame(sort(table(GSDB$sanctioned_state), decreasing = TRUE)[1:11])\nMostSanctioned\n\n\n               Var1 Freq\n1              Fiji   29\n2              Iran   28\n3      South Africa   28\n4           Myanmar   27\n5          Cambodia   24\n6             China   24\n7          Pakistan   24\n8  Egypt, Arab Rep.   20\n9             Libya   20\n10           France   19\n11     Korea, North   19\n\n\n\n\nCode\nsort(table(SansUnit$commitment), decreasing = TRUE)\n\n\n\n  1   2  11   3   4   5   6   7   8  15 \n573  14   7   3   3   3   2   1   1   1 \n\n\n\n\nCode\nsaning_part = sort(table(SansUnit$sanctioning_state), decreasing = TRUE); head(saning_part)\n\n\n\nUnited States            EU            UN        Canada         Japan \n          193            41            35            23            21 \n    Australia \n           20 \n\n\n\n\nCode\nMostPopularObj = data.frame(sort(table(GSDB$objective), decreasing = TRUE)[1:5])\nMostPopularObj\n\n\n           Var1 Freq\n1 policy_change  152\n2     democracy  143\n3  human_rights  122\n4         other   92\n5   prevent_war   89\n\n\n\n\nCode\nSuccSansByYear = dplyr::filter(SansBinary2, SuccessBinary2 == 1)\n\nmean(SuccSansByYear$end)\n\n\n[1] 1997.567\n\n\nCode\nsort(table(SuccSansByYear$end), decreasing = TRUE)[1:10]\n\n\n\n2016 1994 2014 2003 1993 1992 2011 1998 1996 2004 \n  27   23   23   16   15   11   11   10    9    9 \n\n\n\n\nCode\nRegime_data = select(Freedom_Status_Data, \"Countries\", starts_with(\"Regime\"))\n\n\n\n\nCode\nwrite_xlsx(Regime_data, \"Regime_data.xlsx\")\n\n\n\n\nCode\nGSDB_Countries_Begin = select(GSDB, \"sanctioned_state\", \"begin\")\n\n\n\n\nCode\nSans_Countries_Begin = select(SansUnit, \"sanctioned_state\", \"begin\")\n\nwrite_xlsx(Sans_Countries_Begin, \"Sans_Countries_Begin.xlsx\")\n\n\n\n\nCode\nT_Regime_data = t(Regime_data)\nclass(T_Regime_data)\n\n\n[1] \"matrix\" \"array\" \n\n\nCode\nT_Regime_data = as.data.frame(T_Regime_data)\n#transposing Regime_data\n\n\n\n\nCode\nlibrary(janitor)\n\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\nCode\nT_Regime_data = T_Regime_data %&gt;%\n  row_to_names(row_number = 1)\n\n\n\n\nCode\n#imported merged data from stata file. I merged data on trade share, regime, and sans. The Trade share for the year of sanctions is taken from 1 year ahead of the start of the sanctions.\nMerged_data &lt;- read_excel(\"Copy of merged3.xlsx\", \n    col_types = c(\"text\", \"numeric\", \"text\", \n        \"skip\", \"numeric\", \"skip\", \"numeric\", \n        \"skip\", \"numeric\"))\n\n\nNew names:\n• `` -&gt; `...5`\n\n\nCode\nView(Merged_data)\n\n\n\n\nCode\n#merging merged_data and the list of when sans started to check compatability\nTest_data = inner_join(Sans_Countries_Begin, Merged_data, by = c(\"sanctioned_state\", \"begin\"))\nTest_data2 = dplyr::select(Test_data, \"sanctioned_state\", \"begin\", \"Status\", \"Trade_%\")\n\n\n\n\nCode\nTest_data2 &lt;- Test_data2 %&gt;% \n  mutate(regime = ifelse(Status == \"F\", 1, 0)) #Changing Letters to numbers: if Free 1, else 0\n\n\n\n\nCode\nModel_data = inner_join(SansUnit, Merged_data, by = c(\"sanctioned_state\", \"begin\"))\nModel_data &lt;- Model_data %&gt;% \n  mutate(regime = ifelse(Status == \"F\", 1, 0))\n#merging SansUnit with merged data, creating numeric regime var and creating Model_data\n\n\n\n\nCode\nsort(table(Model_data$success), decreasing = TRUE)\n\n\n\nsuccess_total        failed  success_part \n          243           135            88 \n\n\n\n\nCode\nmatched3 &lt;- intersect(SansUnit$sanctioned_state, Model_data$sanctioned_state)\nall3 &lt;-  union(SansUnit$sanctioned_state, Model_data$sanctioned_state)\nnon.matched3 &lt;- data.frame(all3[!all3 %in% matched3]); non.matched3\n\n\n                        all3..all3..in..matched3.\n1                                          Ceylon\n2                              Dominican Republic\n3                                             EEC\n4                                              EU\n5                           El Salvador, Honduras\n6                      Estonia, Latvia, Lithuania\n7                                         Finland\n8                                     Gambia, The\n9                      German Democratic Republic\n10                                        Germany\n11                   ICC Rome Statute Signatories\n12                                India, Pakistan\n13                                           Laos\n14                              Malagasy Republic\n15                                         Malaya\n16                                     Mozambique\n17                                  North Vietnam\n18                                      Palestine\n19                Poland, Hungary, Czechoslovakia\n20                                       Rhodesia\n21                                      Singapore\n22                                         Sweden\n23 Terrorist Organizations (Taliban and Al-Qaeda)\n24                        Thailand, South Vietnam\n25                                    Transjordan\n26                                        Tunisia\n27                              Western countries\n28                                   Yemen, North\n\n\n\n\nCode\nwrite_xlsx(SansUnit, \"SansUnit.xlsx\")\n\n\n\n\nCode\nModel_data &lt;- Model_data %&gt;% \n  mutate(multilateral = ifelse(grepl(\"EU|UN|League of Arab States|African Union|Commonwealth|EEC|ECOWAS|Organisation of African Unity|Organization of American States|EU, Croatia, Macedonia, Montenegro, Iceland, Albania, Bosnia and Herzegovina, Liechtenstein, Norway, Moldova, Armenia|G8|MERCOSUR|NATO|NAFTA|OAPEC|Organization of Eastern Carribean States|Pacific Islands Forum|Paris Agreement Signatories|SADC\", sanctioning_state), 1, 0)) #creating multilateral where organizations participated in sanctioning\n\n\n\n\nCode\nsum(is.na(Model_data$`Trade_%`)) #counting missing values\n\n\n[1] 91\n\n\n\n\nCode\nmissing_trade = dplyr::filter(Model_data, is.na(Model_data$`Trade_%`))\n\n\n\n\nCode\nsum(Model_data$military)\n\n\n[1] 105\n\n\nCode\nsum(Model_data$other)\n\n\n[1] 74\n\n\nCode\n#counting the num of instances of objectives\n\n\n\n\nCode\nunique(Model_data$objective) #unique objectives of sans\n\n\n[1] \"terrorism\"            \"human_rights\"         \"policy_change\"       \n[4] \"democracy\"            \"end_war\"              \"destab_regime\"       \n[7] \"other\"                \"territorial_conflict\" \"prevent_war\"         \n\n\n\n\nCode\nlibrary(readxl)\nGDPbyYear &lt;- read_excel(\"GDPbyYear.xlsx\", \n    col_types = c(\"text\", \"numeric\", \"skip\", \n        \"skip\", \"numeric\"))\nView(GDPbyYear)\n\n\n\n\nCode\nmatched5 &lt;- intersect(Model_data$sanctioned_state, GDPbyYear$`Country/Area`)\nall5 &lt;-  union(Model_data$sanctioned_state, GDPbyYear$`Country/Area`)\nnon.matched5 &lt;- data.frame(all5[!all5 %in% matched5])\n#list of countries, names of which do not match. I manually changed them to comply with GSDB names\n\n\n\n\nCode\n#GDP by year with new names\nGDPbyYear2 &lt;- read_excel(\"GDPbyYear.xlsx\", \n    sheet = \"Sheet1\")\nView(GDPbyYear2)\n\n\n\n\nCode\nModel_data2 = inner_join(Model_data[ , -which(names(Model_data) %in% c(\"Trade_%_act\",\"...5\", \"regime\"))], GDPbyYear2, by = c(\"sanctioned_state\", \"begin\")) #including only matching rows in sanctioned_state and begin, excluding the three columns in c()\n\nModel_data2$GDP_san = Model_data2$GDP_san/1000000000\n#simplifying GDP by 1 billion\n\nModel_data2 = Model_data2 %&gt;% \n  mutate(Success = ifelse(success==\"success_total\", 1, 0))\n#if total success 1, esle 0\n\nModel_data2 &lt;- Model_data2 %&gt;% \n  mutate(democracy = ifelse(objective == \"democracy\", 1, 0)) %&gt;% \n  mutate(human_rights = ifelse(objective == \"human_rights\", 1, 0)) %&gt;% \n  mutate(policy_change = ifelse(objective == \"policy_change\", 1, 0)) %&gt;%\n  mutate(other_obj = ifelse(objective == \"other\", 1, 0)) %&gt;%\n  mutate(prevent_war = ifelse(objective == \"prevent_war\", 1, 0)) %&gt;%\n  mutate(end_war = ifelse(objective == \"end_war\", 1, 0)) %&gt;%\n  mutate(destab_regime = ifelse(objective == \"destab_regime\", 1, 0)) %&gt;%\n  mutate(terrorism = ifelse(objective == \"terrorism\", 1, 0)) %&gt;%\n  mutate(territorial_conflict = ifelse(objective == \"territorial_conflict\", 1, 0)) %&gt;% \n  mutate(regime = C(as.factor(Status)))\n#creating new dummy vars based on objectives and making factor values for regime\n\n#Pay attention to Trade and territorialconfl\nMDLogitFull = Model_data2[ , -which(names(Model_data2) %in% c(\"case_id\", \"sanctioned_state\", \"sanctioning_state\", \"objective\", \"success\", \"success_ord\",\"territorial_conflict\", \"Status\", \"SuccessBinary\"))] # creating a dataset of independent vars\n\n\n\n\nCode\nMDLogitFull = MDLogitFull %&gt;% \n  mutate(comprehensive = ifelse(grepl(\"exp_compl, imp_compl\", descr_trade), 1, 0), \n  duration = end - begin, \n  duration = replace(duration, duration == 0, 1))\n#adding vars for comprehensive sans and duration of sans\n\n\n\n\nCode\nMDLogitFull = dplyr::select(MDLogitFull, \"Success\",\"trade\", \"financial\", \"arms\", \"military\", \"travel\", \"other\", \"comprehensive\", \"multilateral\", \"democracy\", \"human_rights\", \"policy_change\", \"other_obj\", \"prevent_war\", \"end_war\", \"destab_regime\", \"terrorism\", \"Trade_%\", \"GDP_san\", \"duration\", \"regime\") #rearranging columns and deleting extra columns\n\n\n\n\nCode\nlibrary(writexl)\nwrite_xlsx(MDLogitFull, \"MDLogitFull.xlsx\")\n\n\n\n\nCode\nsummary(MDLogitFull)\n\n\n    Success           trade          financial           arms       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.5226   Mean   :0.4215   Mean   :0.4602   Mean   :0.2022  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n                                                                    \n    military          travel          other        comprehensive    \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.00000  \n Median :0.0000   Median :0.000   Median :0.0000   Median :0.00000  \n Mean   :0.2258   Mean   :0.172   Mean   :0.1591   Mean   :0.05161  \n 3rd Qu.:0.0000   3rd Qu.:0.000   3rd Qu.:0.0000   3rd Qu.:0.00000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.00000  \n                                                                    \n  multilateral      democracy       human_rights    policy_change   \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.2882   Mean   :0.2516   Mean   :0.1763   Mean   :0.1376  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n                                                                    \n   other_obj       prevent_war        end_war       destab_regime    \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.00000  \n Mean   :0.1355   Mean   :0.1226   Mean   :0.1032   Mean   :0.03226  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.00000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  \n                                                                     \n   terrorism          Trade_%             GDP_san             duration     \n Min.   :0.00000   Min.   :  0.02689   Min.   :    0.310   Min.   : 1.000  \n 1st Qu.:0.00000   1st Qu.: 37.43444   1st Qu.:    6.013   1st Qu.: 1.000  \n Median :0.00000   Median : 49.59553   Median :   30.790   Median : 2.000  \n Mean   :0.02796   Mean   : 58.79129   Mean   :  326.336   Mean   : 4.525  \n 3rd Qu.:0.00000   3rd Qu.: 76.68788   3rd Qu.:  157.567   3rd Qu.: 6.000  \n Max.   :1.00000   Max.   :173.36859   Max.   :14121.056   Max.   :34.000  \n                   NA's   :90                                              \n regime  \n F : 82  \n NF:217  \n PF:166  \n         \n         \n         \n         \n\n\nCode\nsummary(Model_data2$end)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1974    1993    2001    2000    2011    2019 \n\n\nCode\nlibrary(vtable)\n\n\nLoading required package: kableExtra\n\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\nCode\nsumtable(MDLogitFull, out = \"csv\", file = \"SumStats.csv\")\n\n\n        Variable   N    Mean Std. Dev.   Min Pctl. 25 Pctl. 75       Max\n1        Success 465   0.523       0.5     0        0        1         1\n2          trade 465   0.422     0.494     0        0        1         1\n3      financial 465    0.46     0.499     0        0        1         1\n4           arms 465   0.202     0.402     0        0        0         1\n5       military 465   0.226     0.419     0        0        0         1\n6         travel 465   0.172     0.378     0        0        0         1\n7          other 465   0.159     0.366     0        0        0         1\n8  comprehensive 465   0.052     0.221     0        0        0         1\n9   multilateral 465   0.288     0.453     0        0        1         1\n10     democracy 465   0.252     0.434     0        0        1         1\n11  human_rights 465   0.176     0.382     0        0        0         1\n12 policy_change 465   0.138     0.345     0        0        0         1\n13     other_obj 465   0.135     0.343     0        0        0         1\n14   prevent_war 465   0.123     0.328     0        0        0         1\n15       end_war 465   0.103     0.305     0        0        0         1\n16 destab_regime 465   0.032     0.177     0        0        0         1\n17     terrorism 465   0.028     0.165     0        0        0         1\n18       Trade_% 375  58.791     31.46 0.027   37.434   76.688   173.369\n19       GDP_san 465 326.336  1248.268  0.31    6.013  157.567 14121.056\n20      duration 465   4.525     5.337     1        1        6        34\n21        regime 465                                                    \n22         ... F  82   17.6%                                            \n23        ... NF 217   46.7%                                            \n24        ... PF 166   35.7%                                            \n\n\n\n\nCode\nlibrary(summarytools)\n\n\n\nAttaching package: 'summarytools'\n\n\nThe following object is masked from 'package:tibble':\n\n    view\n\n\nCode\ndescr(MDLogitFull$GDP_san,\n      stats = c(\"mean\", \"sd\",\"med\", \"min\", \"max\"), \n      transpose = TRUE)\n\n\nDescriptive Statistics  \nMDLogitFull$GDP_san  \nN: 465  \n\n                  Mean   Std.Dev   Median    Min        Max\n------------- -------- --------- -------- ------ ----------\n      GDP_san   326.34   1248.27    30.79   0.31   14121.06\n\n\n\n\nCode\nModel_data2 %&gt;% \n  dplyr::select(sanctioned_state, GDP_san) %&gt;% \n  arrange(desc(GDP_san))\n\n\n# A tibble: 465 × 2\n   sanctioned_state GDP_san\n   &lt;chr&gt;              &lt;dbl&gt;\n 1 United States     14121.\n 2 United States     14121.\n 3 United States     14121.\n 4 United States      6232.\n 5 United States      5639.\n 6 France             2110.\n 7 Japan              1896.\n 8 France             1837.\n 9 France             1734.\n10 France             1734.\n# … with 455 more rows\n\n\n\n\nCode\nModel_data2 %&gt;% \n  ggplot(aes(x=\"\", y=GDP_san)) +\n  geom_boxplot() +\n    geom_jitter(color=\"black\", size=0.4, alpha=0.9) +\n    geom_text(data = subset(Model_data2, sanctioned_state=\"United States\"), aes(label = sanctioned_state)) +\n    ggtitle(\"Boxplot for GDP outliers\")\n\n\n\n\n\nCode\n#GDP_san&gt;14000\n\n\n\n\nCode\nwhich(Model_data2$GDP_san&gt;=14121)\n\n\n[1] 443 444 445\n\n\n\n\nCode\n(Model_data2[443,\"GDP_san\"] - mean(Model_data2$GDP_san)) / sd(Model_data2$GDP_san) #very high z-score\n\n\n   GDP_san\n1 11.05109\n\n\n\n\nCode\nplot(density(Model_data2$GDP_san))\n\n\n\n\n\n\n\nCode\n#descriptive stats without US\nModel_data2 %&gt;% \n  dplyr::select(sanctioned_state,GDP_san) %&gt;% \n  filter(`sanctioned_state` != \"United States\") %&gt;% \n  descr(stats = c(\"mean\", \"sd\",\"med\", \"min\", \"max\"), \n      transpose = TRUE)\n\n\nNon-numerical variable(s) ignored: sanctioned_state\n\n\nDescriptive Statistics  \nModel_data2$GDP_san  \nN: 460  \n\n                  Mean   Std.Dev   Median    Min       Max\n------------- -------- --------- -------- ------ ---------\n      GDP_san   211.98    425.21    30.43   0.31   2109.95\n\n\n\n\nCode\nModel_data2 %&gt;%\n  filter(sanctioned_state!=\"United States\") %&gt;% \n  ggplot(aes(GDP_san)) +\n  geom_density()\n\n\n\n\n\n\n\nCode\nMDLogitFull %&gt;%\n  bind_cols(Model_data2$sanctioned_state) %&gt;% \n  filter(...22!=\"United States\") %&gt;% \n  ggplot(aes(GDP_san)) +\n  geom_density()\n\n\nNew names:\n• `` -&gt; `...22`\n\n\n\n\n\n\n\nCode\nsum(is.na(Model_data2$GDP_san))\n\n\n[1] 0\n\n\n\n\nCode\nlapply(Model_data[, \"Status\"], table)\n\n\n$Status\n\n  F  NF  PF \n 82 218 166 \n\n\nCode\nlapply(Model_data[, \"success\"], table)\n\n\n$success\n\n       failed  success_part success_total \n          135            88           243"
  },
  {
    "objectID": "sanctions.html#old-logit-models",
    "href": "sanctions.html#old-logit-models",
    "title": "Code For When Do Economic Snactions Work?",
    "section": "OLD Logit models",
    "text": "OLD Logit models\n\n\nCode\nLogitFull = glm(Success ~ ., family = binomial(link = \"logit\"), MDLogitFull)\nsummary(LogitFull)\n\n\n\nCall:\nglm(formula = Success ~ ., family = binomial(link = \"logit\"), \n    data = MDLogitFull)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3723  -1.0737   0.4635   1.0990   1.8527  \n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   -2.1731058  1.0110560  -2.149  0.03161 * \ntrade          0.4632244  0.2849585   1.626  0.10404   \nfinancial     -0.1668916  0.2632288  -0.634  0.52607   \narms          -0.1910168  0.3407474  -0.561  0.57508   \nmilitary       0.3721241  0.2938551   1.266  0.20539   \ntravel        -0.0198639  0.3317100  -0.060  0.95225   \nother          0.6137458  0.3515275   1.746  0.08082 . \ncomprehensive  1.1865369  0.6624250   1.791  0.07326 . \nmultilateral   0.3180299  0.2697817   1.179  0.23846   \ndemocracy      1.2754308  0.9745290   1.309  0.19061   \nhuman_rights   0.4997315  0.9771651   0.511  0.60906   \npolicy_change  0.2083118  0.9647225   0.216  0.82904   \nother_obj      1.1189213  0.9654089   1.159  0.24645   \nprevent_war    0.5048024  0.9630763   0.524  0.60017   \nend_war        1.3833197  1.0272493   1.347  0.17810   \ndestab_regime  0.8478135  1.1295470   0.751  0.45291   \nterrorism     -0.0068606  1.1675191  -0.006  0.99531   \n`Trade_%`     -0.0023294  0.0040327  -0.578  0.56353   \nGDP_san        0.0004494  0.0002509   1.791  0.07330 . \nduration       0.0871889  0.0279820   3.116  0.00183 **\nregimeNF       0.7843214  0.4069757   1.927  0.05396 . \nregimePF       0.9066940  0.4054972   2.236  0.02535 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 519.41  on 374  degrees of freedom\nResidual deviance: 472.17  on 353  degrees of freedom\n  (90 observations deleted due to missingness)\nAIC: 516.17\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nCode\nMDLogitLess = MDLogitFull %&gt;% \n  filter(!is.na(`Trade_%`))\n\n\n\n\nCode\nLogit1 = glm(Success ~ other+duration+comprehensive+regime+GDP_san+end_war+democracy+other_obj, family = binomial(link = \"logit\"), MDLogitLess)\nsummary(Logit1)\n\n\n\nCall:\nglm(formula = Success ~ other + duration + comprehensive + regime + \n    GDP_san + end_war + democracy + other_obj, family = binomial(link = \"logit\"), \n    data = MDLogitLess)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-2.163  -1.136   0.477   1.123   1.820  \n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -1.6163922  0.4157974  -3.887 0.000101 ***\nother          0.6464064  0.3090774   2.091 0.036492 *  \nduration       0.0844948  0.0270453   3.124 0.001783 ** \ncomprehensive  1.3848909  0.6151098   2.251 0.024357 *  \nregimeNF       0.7762854  0.3769828   2.059 0.039474 *  \nregimePF       0.8262238  0.3889313   2.124 0.033641 *  \nGDP_san        0.0005040  0.0002484   2.029 0.042427 *  \nend_war        1.0931001  0.4556741   2.399 0.016446 *  \ndemocracy      0.7272301  0.2787235   2.609 0.009077 ** \nother_obj      0.7333717  0.3344625   2.193 0.028330 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 519.41  on 374  degrees of freedom\nResidual deviance: 479.25  on 365  degrees of freedom\nAIC: 499.25\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nCode\nprobabilities1 &lt;- Logit1 %&gt;% predict(type = \"response\")\nhead(probabilities1)\n\n\n        1         2         3         4         5         6 \n0.3207676 0.4856717 0.5240277 0.5281468 0.6314959 0.6314959 \n\n\n\n\nCode\npredicted.classes1 &lt;- ifelse(probabilities1 &gt; 0.5, 1, 0)\nhead(predicted.classes1)\n\n\n1 2 3 4 5 6 \n0 0 1 1 1 1 \n\n\n\n\nCode\nmean(predicted.classes1 == MDLogitLess$Success)\n\n\n[1] 0.656\n\n\n\n\nCode\nforplot = bind_cols(MDLogitLess$GDP_san, probabilities1, MDLogitLess$regime)\n\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n\n\nCode\ncolnames(forplot) &lt;- c(\"GDP_san\", \"Probability\", \"Regime\")"
  },
  {
    "objectID": "sanctions.html#summary-table",
    "href": "sanctions.html#summary-table",
    "title": "Code For When Do Economic Snactions Work?",
    "section": "Summary Table",
    "text": "Summary Table\n\n\nCode\nlibrary(jtools) #for summary tables\nlibrary(huxtable) #needed for jtools\n\n\n\nAttaching package: 'huxtable'\n\n\nThe following objects are masked from 'package:summarytools':\n\n    label, label&lt;-\n\n\nThe following object is masked from 'package:kableExtra':\n\n    add_footnote\n\n\nThe following object is masked from 'package:dplyr':\n\n    add_rownames\n\n\nThe following object is masked from 'package:ggplot2':\n\n    theme_grey\n\n\n\n\nCode\nexport_summs(LogitFull, Logit1, to.file = \"docx\", file.name = \"ModelEstimations.docx\")\n\n\n\n\nCode\nexport_summs(LogitFull,\n             Logit1,\n             model.names = c(\"Model 1\",\n                             \"Model 2\")) -&gt; sum_table1\nsum_table1\n\n\n\n\nModel 1Model 2\n\n(Intercept)-2.17 * -1.62 ***\n\n(1.01)  (0.42)   \n\ntrade0.46          \n\n(0.28)         \n\nfinancial-0.17          \n\n(0.26)         \n\narms-0.19          \n\n(0.34)         \n\nmilitary0.37          \n\n(0.29)         \n\ntravel-0.02          \n\n(0.33)         \n\nother0.61   0.65 *  \n\n(0.35)  (0.31)   \n\ncomprehensive1.19   1.38 *  \n\n(0.66)  (0.62)   \n\nmultilateral0.32          \n\n(0.27)         \n\ndemocracy1.28   0.73 ** \n\n(0.97)  (0.28)   \n\nhuman_rights0.50          \n\n(0.98)         \n\npolicy_change0.21          \n\n(0.96)         \n\nother_obj1.12   0.73 *  \n\n(0.97)  (0.33)   \n\nprevent_war0.50          \n\n(0.96)         \n\nend_war1.38   1.09 *  \n\n(1.03)  (0.46)   \n\ndestab_regime0.85          \n\n(1.13)         \n\nterrorism-0.01          \n\n(1.17)         \n\n`Trade_%`-0.00          \n\n(0.00)         \n\nGDP_san0.00   0.00 *  \n\n(0.00)  (0.00)   \n\nduration0.09 **0.08 ** \n\n(0.03)  (0.03)   \n\nregimeNF0.78   0.78 *  \n\n(0.41)  (0.38)   \n\nregimePF0.91 * 0.83 *  \n\n(0.41)  (0.39)   \n\nN375      375       \n\nAIC516.17   499.25    \n\nBIC602.57   538.51    \n\nPseudo R20.16   0.14    \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\n\n\nCode\nprobabilities.full &lt;- LogitFull %&gt;% predict(type = \"response\")\nhead(probabilities.full)\n\n\n        2         5         6        11        12        13 \n0.3431215 0.4884170 0.5312334 0.3674009 0.6013168 0.7101013 \n\n\nCode\npredicted.classes.full &lt;- ifelse(probabilities.full &gt; 0.5, 1, 0)\nhead(predicted.classes.full)\n\n\n 2  5  6 11 12 13 \n 0  0  1  0  1  1 \n\n\nCode\nModel_data_test = na.omit(MDLogitFull) #omitting obs with NA Trade_%\n\nmean(predicted.classes.full == Model_data_test$Success)\n\n\n[1] 0.632\n\n\n\n\nCode\npred_rate1 = tibble(\n  \"Prediction Rate\",\n  mean(predicted.classes.full == Model_data_test$Success),\n  mean(predicted.classes1 == MDLogitLess$Success)\n  )\n\ncolnames(pred_rate1) &lt;- c(\"names\",\n                          \"Model 1\",\n                          \"Model 2\")\n\npred_rate1\n\n\n\n\nnamesModel 1Model 2\n\nPrediction Rate0.6320.656"
  },
  {
    "objectID": "sanctions.html#marginal-effects",
    "href": "sanctions.html#marginal-effects",
    "title": "Code For When Do Economic Snactions Work?",
    "section": "Marginal Effects",
    "text": "Marginal Effects\n\n\nCode\nlibrary(mfx)\n\n\nLoading required package: sandwich\n\n\nLoading required package: lmtest\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nLoading required package: betareg\n\n\nCode\nmargins1 = logitmfx(Logit1, data=MDLogitLess, atmean = FALSE)\n\nmargins1\n\n\nCall:\nlogitmfx(formula = Logit1, data = MDLogitLess, atmean = FALSE)\n\nMarginal Effects:\n                   dF/dx  Std. Err.      z    P&gt;|z|   \nother         1.4478e-01 6.6904e-02 2.1640 0.030462 * \nduration      1.9015e-02 6.4178e-03 2.9629 0.003047 **\ncomprehensive 2.8106e-01 9.8854e-02 2.8432 0.004467 **\nregimeNF      1.6973e-01 7.6964e-02 2.2054 0.027429 * \nregimePF      1.8090e-01 7.9175e-02 2.2848 0.022326 * \nGDP_san       1.1343e-04 5.7143e-05 1.9850 0.047142 * \nend_war       2.3180e-01 8.4091e-02 2.7565 0.005842 **\ndemocracy     1.6323e-01 6.0226e-02 2.7103 0.006722 **\nother_obj     1.5939e-01 6.7858e-02 2.3489 0.018830 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndF/dx is for discrete change for the following variables:\n\n[1] \"other\"         \"comprehensive\" \"regimeNF\"      \"regimePF\"     \n[5] \"end_war\"       \"democracy\"     \"other_obj\"    \n\n\n\n\nCode\nlibrary(margins) #R import of Stata margins command\nexport_summs(margins1, margins(Logit1), model.names = c(\"Marginal Effects (mfx)\",\n                                                        \"Marginal Effects (margins)\"))\n\n\nWarning: contrasts dropped from factor regime\n\nWarning: contrasts dropped from factor regime\n\nWarning: contrasts dropped from factor regime\n\n\nWarning in nobs.default(m, use.fallback = TRUE): no 'nobs' method is available\n\nWarning in nobs.default(m, use.fallback = TRUE): no 'nobs' method is available\n\n\n\n\nMarginal Effects (mfx)Marginal Effects (margins)\n\nother0.14 * 0.15 * \n\n(0.07)  (0.07)  \n\nduration0.02 **0.02 **\n\n(0.01)  (0.01)  \n\ncomprehensive0.28 **0.31 * \n\n(0.10)  (0.14)  \n\nregimeNF0.17 * 0.17 * \n\n(0.08)  (0.08)  \n\nregimePF0.18 * 0.18 * \n\n(0.08)  (0.08)  \n\nGDP_san0.00 * 0.00 * \n\n(0.00)  (0.00)  \n\nend_war0.23 **0.25 * \n\n(0.08)  (0.10)  \n\ndemocracy0.16 **0.16 **\n\n(0.06)  (0.06)  \n\nother_obj0.16 * 0.17 * \n\n(0.07)  (0.07)  \n\nnobs0      0      \n\nnull.deviance519.41   519.41   \n\ndf.null374.00   374.00   \n\nlogLik-239.62   -239.62   \n\nAIC499.25   499.25   \n\nBIC538.51   538.51   \n\ndeviance479.25   479.25   \n\ndf.residual365.00   365.00   \n\nnobs.1375.00   375.00   \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nCode\n#comparing two packages values\n\n\n\n\nCode\nexport_summs(margins1,\n             margins(Logit1),\n             model.names = c(\"Marginal Effects (mfx)\",\n                             \"Marginal Effects (margins)\"),\n             to.file = \"docx\",\n             file.name = \"MarginalEffects.docx\")\n#saving into Word\n\n\n\n\nCode\nlibrary(sjPlot)\n\n\nRegistered S3 method overwritten by 'parameters':\n  method                         from      \n  format.parameters_distribution datawizard\n\n\n\nAttaching package: 'sjPlot'\n\n\nThe following object is masked from 'package:huxtable':\n\n    font_size\n\n\nCode\nlibrary(ggplot2)\ntheme_set(theme_sjplot())\n\nplot_model(Logit1, type = \"pred\", terms = c(\"duration[all]\",\n                                            \"regime\",\n                                            \"end_war\",\n                                            \"comprehensive\"))\n\n\n\n\n\nCode\nplot_model(Logit1, type = \"pred\", terms = c(\"GDP_san [0.310:2000]\",\n                                            \"regime\",\n                                            \"end_war\",\n                                            \"comprehensive\"))\n\n\n\n\n\nCode\nplot_scatter(forplot, GDP_san, Probability, colors = factor(forplot$Regime))"
  },
  {
    "objectID": "sanctions.html#new-logit",
    "href": "sanctions.html#new-logit",
    "title": "Code For When Do Economic Snactions Work?",
    "section": "New Logit",
    "text": "New Logit\n\n\nCode\nMDLogitFull %&gt;%\n  bind_cols(Model_data2$sanctioned_state) %&gt;% \n  filter(...22!=\"United States\") %&gt;% \n  dplyr::select(-22) -&gt; MDLogitFull2\n\n\nNew names:\n• `` -&gt; `...22`\n\n\n\n\nCode\nLogitFull2 = glm(Success ~ ., family = binomial(link = \"logit\"), MDLogitFull2)\nsummary(LogitFull2)\n\n\n\nCall:\nglm(formula = Success ~ ., family = binomial(link = \"logit\"), \n    data = MDLogitFull2)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3492  -1.0867   0.4741   1.0835   1.9508  \n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   -2.4945763  1.0370024  -2.406  0.01615 * \ntrade          0.4438997  0.2882404   1.540  0.12355   \nfinancial     -0.1779547  0.2647340  -0.672  0.50146   \narms          -0.1767103  0.3426006  -0.516  0.60600   \nmilitary       0.3885906  0.2978971   1.304  0.19208   \ntravel        -0.0280782  0.3327529  -0.084  0.93275   \nother          0.5754016  0.3529090   1.630  0.10301   \ncomprehensive  1.2326294  0.6667068   1.849  0.06448 . \nmultilateral   0.3944537  0.2737595   1.441  0.14962   \ndemocracy      1.3396984  0.9696278   1.382  0.16708   \nhuman_rights   0.5334473  0.9718307   0.549  0.58307   \npolicy_change  0.2586650  0.9587613   0.270  0.78732   \nother_obj      1.1732820  0.9612001   1.221  0.22222   \nprevent_war    0.4455867  0.9581219   0.465  0.64189   \nend_war        1.4413826  1.0246009   1.407  0.15949   \ndestab_regime  0.9199462  1.1262595   0.817  0.41403   \nterrorism      0.0741958  1.1662149   0.064  0.94927   \n`Trade_%`     -0.0017772  0.0040890  -0.435  0.66384   \nGDP_san        0.0007915  0.0003574   2.215  0.02678 * \nduration       0.0899221  0.0283561   3.171  0.00152 **\nregimeNF       0.9587942  0.4336647   2.211  0.02704 * \nregimePF       1.1067847  0.4369861   2.533  0.01132 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 512.66  on 369  degrees of freedom\nResidual deviance: 467.49  on 348  degrees of freedom\n  (90 observations deleted due to missingness)\nAIC: 511.49\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nCode\nprobabilities.full2 &lt;- LogitFull2 %&gt;% predict(type = \"response\")\nhead(probabilities.full2)\n\n\n        2         5         6        11        12        13 \n0.3236010 0.4704258 0.5422739 0.3728714 0.6094045 0.7344304 \n\n\nCode\npredicted.classes.full2 &lt;- ifelse(probabilities.full2 &gt; 0.5, 1, 0)\nhead(predicted.classes.full2)\n\n\n 2  5  6 11 12 13 \n 0  0  1  0  1  1 \n\n\nCode\nModel_data_test2 = Model_data2 %&gt;% \n  filter(sanctioned_state!=\"United States\",\n         !is.na(`Trade_%`))\n\nmean(predicted.classes.full2 == Model_data_test2$Success)\n\n\n[1] 0.6513514\n\n\n\n\nCode\nplot_model(LogitFull2,\n           type = \"pred\",\n           terms = c(\"duration[all]\",\n                     \"regime\",\n                     \"end_war\",\n                     \"comprehensive\"),\n           facet.grid = list(scale = \"free\")\n)\n\n\n\n\n\n\n\nCode\nMDLogitLess2 = MDLogitFull2 %&gt;% \n  filter(!is.na(`Trade_%`))\n\n\n\n\nCode\nLogit2 = glm(Success ~ other+duration+comprehensive+regime+GDP_san+end_war+democracy+other_obj, family = binomial(link = \"logit\"), MDLogitLess2)\nsummary(Logit2)\n\n\n\nCall:\nglm(formula = Success ~ other + duration + comprehensive + regime + \n    GDP_san + end_war + democracy + other_obj, family = binomial(link = \"logit\"), \n    data = MDLogitLess2)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1711  -1.1472   0.5195   1.1081   1.9362  \n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -1.8872639  0.4667804  -4.043 5.27e-05 ***\nother          0.6501514  0.3105503   2.094  0.03630 *  \nduration       0.0876814  0.0273611   3.205  0.00135 ** \ncomprehensive  1.4275922  0.6201075   2.302  0.02133 *  \nregimeNF       0.9605548  0.4035809   2.380  0.01731 *  \nregimePF       1.0308482  0.4195122   2.457  0.01400 *  \nGDP_san        0.0008285  0.0003309   2.504  0.01228 *  \nend_war        1.1540550  0.4617573   2.499  0.01245 *  \ndemocracy      0.7794541  0.2824456   2.760  0.00579 ** \nother_obj      0.7724959  0.3396457   2.274  0.02294 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 512.66  on 369  degrees of freedom\nResidual deviance: 474.89  on 360  degrees of freedom\nAIC: 494.89\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nCode\nprobabilities2 &lt;- Logit2 %&gt;% predict(type = \"response\")\nhead(probabilities2)\n\n\n        1         2         3         4         5         6 \n0.3036010 0.4711828 0.5234767 0.5230554 0.6533678 0.6533678 \n\n\nCode\npredicted.classes2 &lt;- ifelse(probabilities2 &gt; 0.5, 1, 0)\nhead(predicted.classes2)\n\n\n1 2 3 4 5 6 \n0 0 1 1 1 1 \n\n\nCode\nmean(predicted.classes2 == MDLogitLess2$Success)\n\n\n[1] 0.6486486\n\n\n\n\nCode\nexport_summs(LogitFull2,\n             Logit2,\n             model.names = c(\"Model 1\",\n                             \"Model 2\")) -&gt; sum_table2\nsum_table2\n\n\n\n\nModel 1Model 2\n\n(Intercept)-2.49 * -1.89 ***\n\n(1.04)  (0.47)   \n\ntrade0.44          \n\n(0.29)         \n\nfinancial-0.18          \n\n(0.26)         \n\narms-0.18          \n\n(0.34)         \n\nmilitary0.39          \n\n(0.30)         \n\ntravel-0.03          \n\n(0.33)         \n\nother0.58   0.65 *  \n\n(0.35)  (0.31)   \n\ncomprehensive1.23   1.43 *  \n\n(0.67)  (0.62)   \n\nmultilateral0.39          \n\n(0.27)         \n\ndemocracy1.34   0.78 ** \n\n(0.97)  (0.28)   \n\nhuman_rights0.53          \n\n(0.97)         \n\npolicy_change0.26          \n\n(0.96)         \n\nother_obj1.17   0.77 *  \n\n(0.96)  (0.34)   \n\nprevent_war0.45          \n\n(0.96)         \n\nend_war1.44   1.15 *  \n\n(1.02)  (0.46)   \n\ndestab_regime0.92          \n\n(1.13)         \n\nterrorism0.07          \n\n(1.17)         \n\n`Trade_%`-0.00          \n\n(0.00)         \n\nGDP_san0.00 * 0.00 *  \n\n(0.00)  (0.00)   \n\nduration0.09 **0.09 ** \n\n(0.03)  (0.03)   \n\nregimeNF0.96 * 0.96 *  \n\n(0.43)  (0.40)   \n\nregimePF1.11 * 1.03 *  \n\n(0.44)  (0.42)   \n\nN370      370       \n\nAIC511.49   494.89    \n\nBIC597.59   534.03    \n\nPseudo R20.15   0.13    \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05."
  },
  {
    "objectID": "projects.html#economic-research",
    "href": "projects.html#economic-research",
    "title": "Projects",
    "section": "Economic Research",
    "text": "Economic Research\n\nEconometrics Analysis of Sanctions (Code)\n\nThe study used a logistic regression and marginal effect analysis to find what factors lead to the increase in the probability of the success of sanctions."
  }
]