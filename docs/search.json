[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dinmukhamed (Dimash) Alchinbayev",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Dengue\nHydropower"
  },
  {
    "objectID": "projects.html#forecasting",
    "href": "projects.html#forecasting",
    "title": "Projects",
    "section": "",
    "text": "Dengue\nHydropower"
  },
  {
    "objectID": "projects.html#market-research",
    "href": "projects.html#market-research",
    "title": "Projects",
    "section": "Market Research",
    "text": "Market Research\n\nCluster Analysis"
  },
  {
    "objectID": "ETS.html",
    "href": "ETS.html",
    "title": "ETS",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nlibrary(fpp3)\n\n── Attaching packages ────────────────────────────────────────────── fpp3 0.5 ──\n\n\n✔ tibble      3.1.8     ✔ tsibble     1.1.3\n✔ dplyr       1.1.0     ✔ tsibbledata 0.4.1\n✔ tidyr       1.2.0     ✔ feasts      0.3.1\n✔ lubridate   1.8.0     ✔ fable       0.3.3\n✔ ggplot2     3.3.5     ✔ fabletools  0.3.3\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()"
  },
  {
    "objectID": "ETS.html#r-markdown",
    "href": "ETS.html#r-markdown",
    "title": "ETS",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nlibrary(fpp3)\n\n── Attaching packages ────────────────────────────────────────────── fpp3 0.5 ──\n\n\n✔ tibble      3.1.8     ✔ tsibble     1.1.3\n✔ dplyr       1.1.0     ✔ tsibbledata 0.4.1\n✔ tidyr       1.2.0     ✔ feasts      0.3.1\n✔ lubridate   1.8.0     ✔ fable       0.3.3\n✔ ggplot2     3.3.5     ✔ fabletools  0.3.3\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()"
  },
  {
    "objectID": "ETS.html#naive-snaive-drift-ets-for-net-energy-generation",
    "href": "ETS.html#naive-snaive-drift-ets-for-net-energy-generation",
    "title": "ETS",
    "section": "Naive, SNaive, Drift, ETS for net energy generation",
    "text": "Naive, SNaive, Drift, ETS for net energy generation\n\nlibrary(readr)\nnet_gen &lt;- read_csv(\"Net_generation_United_States_all_sectors_monthly.csv\", \n    skip = 4)\n\nRows: 268 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Month\ndbl (1): conventional hydroelectric thousand megawatthours\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nI import the data on net energy generation of electricity from the conventional hydroelectric sources.\n\nhead(net_gen)\n\n# A tibble: 6 × 2\n  Month    `conventional hydroelectric thousand megawatthours`\n  &lt;chr&gt;                                                  &lt;dbl&gt;\n1 Apr 2023                                              17917.\n2 Mar 2023                                              20630.\n3 Feb 2023                                              19338.\n4 Jan 2023                                              22954.\n5 Dec 2022                                              21870.\n6 Nov 2022                                              18764.\n\n\n\nnet_gen = net_gen %&gt;% \n  rename(MWH = \"conventional hydroelectric thousand megawatthours\")\n\n\nnet_gen=net_gen[order(nrow(net_gen):1),]\n\nI reorder the rows, so that the last observation (earliest date) is now the first one.\n\nnet_gen = net_gen %&gt;% \n  mutate(Date = yearmonth(Month)) |&gt;\n  as_tsibble(index = Date)\n\n\nwhich(net_gen$Month==\"Jan 2018\")\n\n[1] 205\n\nwhich(net_gen$Month==\"Jan 2022\")\n\n[1] 253\n\n\n\nnet_gen %&gt;% \n  autoplot(MWH) +\n  labs(title = \"Net Conventional Hydroelectric Power Generation\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\nnet_gen |&gt;\n  gg_season(MWH, labels = \"right\")\n\n\n\n\n\nnet_gen |&gt;\n  gg_subseries(MWH)\n\n\n\n\n\nnet_gen |&gt;\n  model(stl = STL(MWH)) |&gt;\n  components() |&gt;\n  autoplot()\n\nWarning: Ignoring unknown parameters: linewidth\n\n\n\n\n\n\nnet_gen |&gt;\n  ACF(MWH) |&gt;\n  autoplot()\n\n\n\n\n\nnet_gen |&gt;\n  model(stl = STL(MWH)) |&gt;\n  components() |&gt; \n  gg_subseries(season_year) +\n  theme(axis.text.x = element_text(size = 5))\n\n\n\n\n\nnet_gen2 = net_gen[(205:268),]\n\nI take only last five years of the data.\n\nnet_gen2 %&gt;% \n  autoplot(MWH) +\n  labs(title = \"Net Conventional Hydroelectric Generation\",\n       subtitle = \"Jan 2018 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n  #theme(title = element_text(size = 10))\n\n\nnet_gen2 |&gt;\n  model(stl = STL(MWH)) |&gt;\n  components() |&gt;\n  autoplot()\n\nWarning: Ignoring unknown parameters: linewidth\n\n\n\n\n\nI use STL model to decompose the time series. We can observe seasonality and a small negative trend.\n\nnet_gen.train = net_gen2[(1:48),]\nnet_gen.test = net_gen2[(49:64),]\n\nI split the last and the first four years into the testing and training sets respectively.\n\nnet_gen.fit = net_gen.train |&gt;\n  model(Naive = NAIVE(MWH),\n        Season_Naive = SNAIVE(MWH),\n        Drift = RW(MWH ~ drift()),\n        ETSopt = ETS(MWH),\n        ETS = ETS(MWH ~ error(\"A\")\n                      + trend(\"Ad\")\n                      + season(\"M\")))\n\nI estimate Naive, Seasonal Naive, Drift, and two ETS models. There are two ETS models because the optimal ETS() (the default one) seems to be underperforming, since it chooses no trend. I add ETS(A,Ad,M) since it seems to increase forecast accuracy.\nA mable: 1 x 5 Naive Season_Naive Drift ETSopt ETS      1    &lt;ETS(M,N,A)&gt; &lt;ETS(A,Ad,M)&gt;\n\naccuracy(net_gen.fit)\n\n# A tibble: 5 × 10\n  .model       .type           ME  RMSE   MAE    MPE  MAPE  MASE RMSSE    ACF1\n  &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Naive        Training -3.20e+ 1 2665. 2253. -0.784  9.70 1.25  1.08   0.147 \n2 Season_Naive Training -1.14e+ 3 2478. 1809. -5.51   8.19 1     1      0.523 \n3 Drift        Training -1.39e-12 2665. 2255. -0.643  9.71 1.25  1.08   0.147 \n4 ETSopt       Training  1.07e+ 1 1312.  943. -0.118  3.98 0.521 0.529 -0.0589\n5 ETS          Training  1.84e+ 0 1295. 1027. -0.143  4.34 0.568 0.522  0.131 \n\n\n\nnet_gen.fc = net_gen.fit |&gt;\n  forecast(h = \"16 months\")\n\n\nnet_gen.fc |&gt;\n  autoplot(net_gen.train, level = NULL) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation Forecast\",\n       subtitle = \"Jan 2018 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal() +\n  theme(plot.caption = element_text(hjust = 6.5))\n\n\n\n\nHere is the plot with the training data and the forecast of the unseen 16 months.\n\nnet_gen.fc |&gt;\n  autoplot(net_gen2, level = NULL) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation Forecast\",\n       subtitle = \"Jan 2018 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal() +\n  theme(plot.caption = element_text(hjust = 6.5))\n\n\n\n\nHere is the plot of the forecast and the actual data.\n\nnet_gen.fc |&gt;\n  autoplot(net_gen.test, level = NULL) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation Forecast\",\n       subtitle = \"Jan 2022 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal() +\n  theme(plot.caption = element_text(hjust = 6.5))\n\n\n\n\nHere is a closer look at the actual observations and the forecast. As we can see, visually, SNaive and ETS models fit better. I use accuracy() function to check which model fitted unseen data the best.\n\nnet_gen.fc %&gt;% \n  accuracy(net_gen2)\n\n# A tibble: 5 × 10\n  .model       .type     ME  RMSE   MAE     MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;        &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Drift        Test  -1863. 3756. 2981. -11.5   15.8  1.65  1.52  0.412\n2 ETS          Test  -2260. 3323. 2561. -11.9   13.1  1.42  1.34  0.198\n3 ETSopt       Test  -3538. 4312. 3600. -18.1   18.4  1.99  1.74  0.235\n4 Naive        Test  -2134. 3964. 3185. -12.9   16.9  1.76  1.60  0.427\n5 Season_Naive Test    372. 1952. 1585.   0.685  7.25 0.876 0.788 0.542\n\n\nUsing RMSE and MAE as metrics, we can observe that Seasonal Naive, surprisingly, has the best predictions, outperforming both ETS models. It is worth mentioning that the default ETS has the worst result.\nPerhaps, had I used the whole dataset, the ETS would have done a better job and see the negative trend, but given only five years, Seasonal Naive is the best choice."
  },
  {
    "objectID": "ETS.html#three-models-for-the-whole-ts-and-enslemble-model",
    "href": "ETS.html#three-models-for-the-whole-ts-and-enslemble-model",
    "title": "ETS",
    "section": "Three models for the whole ts and enslemble model",
    "text": "Three models for the whole ts and enslemble model\n\n#net_gen %&gt;% \n#  mutate(year = year(Date), #lubridate::year to exract year\n#         month = month(Date), #to extract month\n#         index = 1:nrow(net_gen)) -&gt; net_gen\n\n\n#net_gen$month = month.name[(net_gen$month)]\n#month.name[] is not a function\n\n\n#library(fastDummies)\n#net_gen |&gt; \n#  dummy_cols(select_columns = \"month\") -&gt; net_gen\n\n\nround(268*0.8)\n\n[1] 214\n\ntotal_obs.net_gen = dim(net_gen)[1] #puts n of obs into total_obs\ntrain_obs = total_obs.net_gen * 0.8\ntest_obs = total_obs.net_gen - train_obs\nnet_gen.train2 = head(net_gen, train_obs)\nnet_gen.test2 = tail(net_gen, test_obs)"
  },
  {
    "objectID": "ETS.html#testing-regression",
    "href": "ETS.html#testing-regression",
    "title": "ETS",
    "section": "Testing Regression",
    "text": "Testing Regression\n\nnet_gen.train2 |&gt;\n  model(TSLM(MWH ~ trend() + season())) |&gt;\n  forecast() |&gt;\n  autoplot(net_gen.train2)"
  },
  {
    "objectID": "ETS.html#arima-and-other-models",
    "href": "ETS.html#arima-and-other-models",
    "title": "ETS",
    "section": "ARIMA and other models",
    "text": "ARIMA and other models\n\nnet_gen |&gt; gg_tsdisplay(MWH,\n                     plot_type='partial', lag_max = 24)\n\n\n\n\n\nnet_gen.train2 %&gt;% \n  autoplot(log(MWH))\n\n\n\n\n\nlambda &lt;- net_gen.train2 |&gt;\n  features(MWH, features = guerrero) |&gt;\n  pull(lambda_guerrero)\nnet_gen.train2 |&gt;\n  autoplot(box_cox(MWH, lambda)) +\n  labs(y = \"\",\n       title = \"Transformed gas production with $\\\\lambda$ = \",\n         round(lambda,2))\n\n\n\nnet_gen.train2 |&gt;\n  autoplot()\n\nPlot variable not specified, automatically selected `.vars = MWH`\n\n\n\n\n\n\nnet_gen.train.arima = net_gen.train2 %&gt;% \n  mutate(bcMWH = box_cox(MWH, lambda))\n\n\nnet_gen |&gt; gg_tsdisplay(difference(MWH, 12),\n                     plot_type='partial', lag_max = 24)\n\nWarning: Removed 12 row(s) containing missing values (geom_path).\n\n\nWarning: Removed 12 rows containing missing values (geom_point).\n\n\n\n\n\n\nnet_gen |&gt; gg_tsdisplay(difference(MWH, 12) |&gt; difference(),\n                     plot_type='partial', lag_max = 24)\n\nWarning: Removed 13 row(s) containing missing values (geom_path).\n\n\nWarning: Removed 13 rows containing missing values (geom_point).\n\n\n\n\n\n\nnet_gen.train2 |&gt;\n  model(auto = ARIMA(MWH,\n                     stepwise = FALSE,\n                     approx = FALSE)) -&gt; arima.fit\n\nWarning in sqrt(diag(best$var.coef)): NaNs produced\n\n\n\narima.fit |&gt;\n  gg_tsresiduals(lag_max=36)\n\n\n\n\n\narima.fit\n\n# A mable: 1 x 1\n                       auto\n                    &lt;model&gt;\n1 &lt;ARIMA(2,0,0)(2,1,1)[12]&gt;\n\n\nA mable: 1 x 1 auto  1 &lt;ARIMA(2,0,0)(2,1,1)[12]&gt;\n\naugment(arima.fit) |&gt;\n  features(.innov, ljung_box, lag = 36, dof = 5)\n\n# A tibble: 1 × 3\n  .model lb_stat lb_pvalue\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 auto      28.9     0.575\n\n\n\nnet_gen.train.arima |&gt;\n  model(auto = ARIMA(bcMWH,\n                     stepwise = FALSE,\n                     approx = FALSE)) -&gt; arima.fit2\n\n\narima.fit2 |&gt;\n  gg_tsresiduals(lag_max=36)\n\n\n\n\n\narima.fit2\n\n# A mable: 1 x 1\n                       auto\n                    &lt;model&gt;\n1 &lt;ARIMA(1,0,0)(2,1,0)[12]&gt;\n\n\nA mable: 1 x 1 auto  1 &lt;ARIMA(1,0,0)(2,1,0)[12]&gt;\n\naugment(arima.fit) |&gt;\n  features(.innov, ljung_box, lag = 36, dof = 3)\n\n# A tibble: 1 × 3\n  .model lb_stat lb_pvalue\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 auto      28.9     0.672\n\n\n\nnet_gen.fit2 = net_gen.train2 |&gt;\n  model(Season_Naive = SNAIVE(MWH),\n        Regression = TSLM(MWH ~ trend() + season()),\n        ETSopt = ETS(MWH),\n        ETS = ETS(MWH ~ error(\"A\")\n                      + trend(\"Ad\")\n                      + season(\"M\")))\n\n\nnet_gen.fit2\n\n# A mable: 1 x 4\n  Season_Naive Regression        ETSopt           ETS\n       &lt;model&gt;    &lt;model&gt;       &lt;model&gt;       &lt;model&gt;\n1     &lt;SNAIVE&gt;     &lt;TSLM&gt; &lt;ETS(M,Ad,M)&gt; &lt;ETS(A,Ad,M)&gt;\n\n\nA mable: 1 x 4 Season_Naive Regression ETSopt ETS     1   &lt;ETS(M,Ad,M)&gt; &lt;ETS(A,Ad,M)&gt;\n\naccuracy(net_gen.fit2) |&gt;\n  select(.model, RMSE, MAE)\n\n# A tibble: 4 × 3\n  .model        RMSE   MAE\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 Season_Naive 3201. 2437.\n2 Regression   2421. 1856.\n3 ETSopt       1687. 1296.\n4 ETS          1685. 1290.\n\n\n\naccuracy(arima.fit) |&gt;\n  select(.model, RMSE, MAE)\n\n# A tibble: 1 × 3\n  .model  RMSE   MAE\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 auto   1698. 1243.\n\n\n\naccuracy(arima.fit2) |&gt;\n  select(.model, RMSE, MAE)\n\n# A tibble: 1 × 3\n  .model    RMSE     MAE\n  &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 auto   0.00570 0.00448\n\n\n\nnet_gen.fc2 = net_gen.fit2 |&gt;\n  forecast(h = 54)\nnet_gen.fc2.arima = arima.fit |&gt;\n  forecast(h = 54)\nnet_gen.fc2.arima2 = arima.fit2 |&gt;\n  forecast(h = 54)\n\n\nnet_gen.fc2 |&gt;\n  autoplot(net_gen.train2, level = NULL) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation Forecast\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal() +\n  theme(plot.caption = element_text(hjust = 6.5))\n\n\n\n\n\nnet_gen.fc2 |&gt;\n  #filter(.model = \"\") |&gt;\n  autoplot(net_gen.test2, level = NULL) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation Forecast\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal() +\n  theme(plot.caption = element_text(hjust = 5))\n\n\n\n\n\nnet_gen.fc2.arima |&gt;\n  autoplot(net_gen.train2, level = NULL) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation\nARIMA Forecast\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\nnet_gen.fc2.arima2 |&gt;\n  autoplot(net_gen.train.arima, level = NULL) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation\nARIMA2 Forecast\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\nnet_gen.fc2 |&gt;\n  accuracy(net_gen) |&gt;\n  select(.model, RMSE, ME, MPE)\n\n# A tibble: 4 × 4\n  .model        RMSE     ME    MPE\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 ETS          3999. -2990. -14.2 \n2 ETSopt       3863. -2877. -13.7 \n3 Regression   2928. -1518.  -7.79\n4 Season_Naive 3323. -1733.  -8.80\n\n\n\nnet_gen.fc2.arima |&gt;\n  accuracy(net_gen) |&gt;\n  select(.model, RMSE, ME, MPE)\n\n# A tibble: 1 × 4\n  .model  RMSE    ME   MPE\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 auto   2638. -567. -3.41\n\n\n\nnet_gen.fc2 |&gt;\n  accuracy(net_gen) |&gt;\n  select(.model, RMSE, ME, MPE) |&gt;\n  as.data.frame() -&gt; acc.metric\n\nnet_gen.fc2.arima |&gt;\n  accuracy(net_gen) |&gt;\n  select(.model, RMSE, ME, MPE) |&gt;\n  as.data.frame() |&gt;\n  bind_rows(acc.metric) -&gt; acc.metric\n\nacc.metric[1,1] = \"ARIMA\"\n\n\nacc.metric %&gt;% \n  group_by(.model, RMSE) %&gt;% \n  ggplot(aes(x = RMSE, \n             xend = 2500, \n             y = reorder(.model, desc(RMSE)), \n             yend=.model,\n             label=round(RMSE, 0))) +\n  theme_minimal() +\n  geom_segment() +\n  geom_point() +\n  geom_text(nudge_x = -50, nudge_y = 0.2,\n            size = 3) +\n  theme(axis.ticks.y = element_blank(),\n        axis.text.y = element_blank()) +\n  labs(x = \"RMSE\", y = element_blank()) -&gt; rmse.plot\n\n\nacc.metric %&gt;% \n  group_by(.model, ME) %&gt;% \n  ggplot(aes(x = ME, \n             xend = 0, \n             y = reorder(.model, desc(abs(ME))), \n             yend=.model,\n             label=round(ME, 2))) +\n  theme_minimal() +\n  geom_segment() +\n  geom_point() +\n  geom_text(nudge_x = 0, nudge_y = 0.2) +\n  #theme(axis.text.y=element_text(margin=margin(r=0))) +\n  labs(title = \"Mean Error by Forecast Model\", x = \"ME\", y = \"Models\")\n\n\n\n\n\nacc.metric %&gt;% \n  group_by(.model, MPE) %&gt;% \n  ggplot(aes(x = MPE, \n             xend = 0, \n             y = reorder(.model, desc(abs(MPE))), \n             yend=.model,\n             label=round(MPE, 2))) +\n  theme_minimal() +\n  geom_segment() +\n  geom_point() +\n  geom_text(nudge_x = 2, nudge_y = 0.2,\n            size = 3) +\n  labs(y = \"\", x = \"MPE\") -&gt; mpe.plot\n\n\nlibrary(ggpubr)\n\n\nggarrange(mpe.plot, rmse.plot,\n                    ncol = 2, nrow = 1) |&gt;\n  annotate_figure(top = text_grob(\"Forecast Accuracy Metrics\"))\n\n\n\n\n\n#net_gen.test2\n#net_gen.fc2.arima2 |&gt;\n#  accuracy()\n\n\nnet_gen.fc2.arima |&gt;\n  autoplot(net_gen.test2, level = NULL) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation\nARIMA Forecast\",\n       subtitle = \"Nov 2018 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\nnet_gen.fc2.arima |&gt;\n  autoplot(net_gen.test2) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation\nARIMA Forecast\",\n       subtitle = \"Nov 2018 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\nnet_gen.fc2 |&gt;\n  filter(.model == \"Regression\") |&gt;\n  autoplot(net_gen.test2, level = NULL) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation\nRegression Forecast\",\n       subtitle = \"Nov 2018 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()"
  },
  {
    "objectID": "ETS.html#arima-with-external-regressors",
    "href": "ETS.html#arima-with-external-regressors",
    "title": "ETS",
    "section": "ARIMA with external Regressors",
    "text": "ARIMA with external Regressors\n\nSPI &lt;- read_csv(\"SPI.csv\")\n\nRows: 1542 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): DATE\ndbl (12): 0, D0, D1, D2, D3, D4, -9, W0, W1, W2, W3, W4\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nwhich(SPI$DATE==\"d_20010101\")\n\n[1] 1273\n\n\n\nspi = SPI[1273:nrow(SPI), c(-1)]\nspi = spi[1:268,]\n\n\nnet_gen = net_gen %&gt;% \n  bind_cols(spi)\n\n\nhead(net_gen)\n\n# A tsibble: 6 x 15 [1M]\n  Month       MWH     Date DATE     D0    D1    D2    D3    D4  `-9`    W0    W1\n  &lt;chr&gt;     &lt;dbl&gt;    &lt;mth&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Jan 2001 18852. 2001 Jan d_20…  40.8  28.2  12.7   7.3   2.8     0  18.4  11.3\n2 Feb 2001 17473. 2001 Feb d_20…  37.3  27.1  13.9   8.5   4.5     0  23.9  14.5\n3 Mar 2001 20477. 2001 Mar d_20…  37.6  25.9  11.8   7     3.8     0  17.5   8.6\n4 Apr 2001 18013. 2001 Apr d_20…  37.5  26.8  13     7.3   3.1     0  22.8  13.6\n5 May 2001 19176. 2001 May d_20…  36.4  27.6  14.4   8.2   3.4     0  29.7  20.6\n6 Jun 2001 20728. 2001 Jun d_20…  36.8  29.5  16.4   9.3   3.3     0  36.2  25.1\n# … with 3 more variables: W2 &lt;dbl&gt;, W3 &lt;dbl&gt;, W4 &lt;dbl&gt;\n\n\n\ntotal_obs.net_gen = dim(net_gen)[1] #puts n of obs into total_obs\ntrain_obs = total_obs.net_gen * 0.8\ntest_obs = total_obs.net_gen - train_obs\nnet_gen.train2 = head(net_gen, train_obs)\nnet_gen.test2 = tail(net_gen, test_obs)\n\n\nnet_gen |&gt; \n  autoplot(D0)\n\n\n\n\n\nnet_gen |&gt; \n  autoplot(D4)\n\n\n\nnet_gen |&gt;\n  autoplot(MWH)\n\n\n\n\n\nnet_gen |&gt;\n  pivot_longer(c(MWH, D4)) |&gt;\n  ggplot(aes(x = Date, y = value)) +\n  geom_line() +\n  facet_grid(name ~ ., scales = \"free_y\") + ylab(\"\")\n\n\n\n\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\n\nnet_gen |&gt;\n  GGally::ggpairs(columns = c(\"MWH\",\"W3\",\"W4\"))\n\n\n\n\n\nnet_gen.fit3 = net_gen.train2 |&gt;\n  model(ARIMA.reg = ARIMA(MWH ~ D0+D1+D2+D3+D4+W0+W1+W2+W3+W4+season()))\n\n\nnet_gen.fit3 |&gt; gg_tsresiduals()\n\n\n\n\n\nnet_gen_future &lt;- new_data(net_gen.train2, 54) |&gt;\n  mutate(D0 = mean(net_gen$D0),\n         D1 = mean(net_gen$D1),\n         D2 = mean(net_gen$D2),\n         D3 = mean(net_gen$D3),\n         D4 = mean(net_gen$D4),\n         W0 = mean(net_gen$W0),\n         W1 = mean(net_gen$W1),\n         W2 = mean(net_gen$W2),\n         W3 = mean(net_gen$W3),\n         W4 = mean(net_gen$W4))\n\n\nnet_gen.fc3 = net_gen.fit3 |&gt;\n  forecast(new_data = net_gen_future)\n\n\nnet_gen.fc3 |&gt;\n  autoplot(net_gen.train2, level = NULL) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation Forecast\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n  #theme(plot.caption = element_text(hjust = 6.5))\n\n\nnet_gen.fc3 |&gt;\n  autoplot(net_gen.test2, level = NULL) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation\nARIMA with Regressors Forecast\",\n       subtitle = \"Nov 2018 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\nnet_gen.fc3 |&gt;\n  accuracy(net_gen) |&gt;\n  select(.model, RMSE, ME, MPE)\n\n# A tibble: 1 × 4\n  .model     RMSE    ME   MPE\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 ARIMA.reg 2367. -112. -1.29\n\n\n\nnet_gen.test2 |&gt;\n  autoplot(MWH) +\n  autolayer(net_gen.fc3, level = NULL) +\n  autolayer(net_gen.fc2.arima, level = NULL) +\n  labs(title = \"Net Conventional Hydroelectric Generation\nARIMA with Regressors Forecast\",\n       subtitle = \"Nov 2018 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\n  ggplot() + \n  geom_line(data=net_gen.test2,\n            aes(x=Date, y=MWH),\n            color='black') +\n  geom_line(data=net_gen.fc3,\n            aes(x=Date, y=.mean, color = \"blue\"),\n            color='blue') + \n  geom_line(data=net_gen.fc2.arima,\n            aes(x=Date, y=.mean, color = \"red\"),\n            color='red') +\n  labs(title = \"Net Conventional Hydroelectric Generation\nARIMA with Regressors Forecast\",\n       subtitle = \"Nov 2018 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  scale_color_manual(values = c(\"red\", \"blue\"), labels = c(\"Data Frame 1\", \"Data Frame 2\"))\n\nWarning: The output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package.\nIf you're using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values.\nThe output of `fortify(&lt;fable&gt;)` has changed to better suit usage with the ggdist package.\nIf you're using it to extract intervals, consider using `hilo()` to compute intervals, and `unpack_hilo()` to obtain values."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Dinmukhamed (Dimash) Alchinbayev",
    "section": "EDUCATION",
    "text": "EDUCATION\nBOSTON COLLEGE, Woods College of Advancing Studies | Boston, MA.\nM.S. Applied Economics; M.S. Applied Analytics | Jan. 2023 – May 2024\nSUFFOLK UNIVERSITY, College of Arts and Sciences | Boston, MA\nB.A. in Economics; B.A. in Politics, Philosophy, and Econ (Honors) | Sep.2018–May 2022\nGPA: 3.9"
  },
  {
    "objectID": "index.html#related-courses",
    "href": "index.html#related-courses",
    "title": "Dinmukhamed (Dimash) Alchinbayev",
    "section": "RELATED COURSES",
    "text": "RELATED COURSES\nMachine Learning Algorithms; Econometrics; Market Research and Data Analysis;\nStatistics and Probability; Linear Algebra; Multivariable Calculus;\nPublic Finance; Forecasting; Politics and Data Analysis; Managerial Economics"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "Dinmukhamed (Dimash) Alchinbayev",
    "section": "WORK EXPERIENCE",
    "text": "WORK EXPERIENCE\nAsian Development Bank | Remote | Sep. 2022–Jan. 2023\nResearch Assistant\n\nWorked with the team of economists to identify key hypotheses and metrics to identify the impact of digital finance on agricultural enterprises.\nComposed exhaustive reports on the latest academic and business progress in digitalization of agricultural finance and its effects.\nWorked with and cleaned the WDI and FINDEX survey datasets; visualized the statistics, using Stata and Excel.\nFound relevant data from the official and secondary sources, such as central banks and business reports; cleaned and transformed the data, using Stata, R, and Excel for easier use and analysis.\nDid OLS regression and statistical analyses on the individual characteristics associated with borrowing, among agricultural cultivators using Stata.\nDelivered outcomes within tight deadlines.\n\nBeacon Hill Institute | Boston, MA | Nov. 2021–Aug. 2022\nData Analyst and Research Assistant\n\nCollaborated with the team of researchers to identify the key metrics needed for the research of the effects of taxes on businesses and government spending in Massachusetts.\nComposed literature reviews on the effects of the earmarked tax revenue on the government spending.\nAnalyzed the flow of earmarked taxes revenue across different government agencies and funds.\nFound, cleaned, and organized data from the Federal and State sources, using R and Stata.\nDid regression analysis and visualized earmark tax revenue and relevant spending data using R and Stata.\nPresented the key findings to the Directors of the BHI and stakeholders.\n\nQazindustry, Center of Industry and Export | Astana, Kazakhstan | Jun. 2019–Aug. 2019\nInternational Experience and Export Analyst Intern\n\nTranslated technical information about government subsidies into non-technical presentations for the small and medium domestic exporters.\nConducted interviews with multiple business owners over phone and in person and advised them corresponding to their needs.\nHelped organizing the Conference for the largest non-oil exporters in Astana.\nFound and analyzed successful cases of the international analogs of the government support of the exporters. Communicated key findings of the analysis to the stakeholders.\nDesigned a blueprint of a virtual assistant program that improved clients’ interaction with the service."
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Dinmukhamed (Dimash) Alchinbayev",
    "section": "SKILLS",
    "text": "SKILLS\nSoftware: R, Stata, Excel, PowerPoint, SQL, Python, Tableau\nHard: Regression, Logit, Splines, KNN, SVM, Regression Trees, Tuning, Clustering, Conjoint\nSoft: Analytical Thinking, Communication, Teamwork, Report Writing, Presenting"
  },
  {
    "objectID": "index.html#independent-research",
    "href": "index.html#independent-research",
    "title": "Dinmukhamed (Dimash) Alchinbayev",
    "section": "INDEPENDENT RESEARCH",
    "text": "INDEPENDENT RESEARCH\nApplied Machine Learning Project\nTrained multiple ML models in R (Ridge Regression, Splines, KNN, Regression Trees, Trees Bagging, and Support Vector Machines) to predict the value of Procurement contracts based on their available descriptions and corruption indices. Result: KNN (categorical) and Bagged Tree (continuous) are the most optimal models to predict Procurement Contract values.\nEconomics Senior Thesis on Effectiveness of Economic Sanctions\nPerformed literature review; statistical data analysis; combined multiple databases (GSDB, World Bank, and Freedom House), and used a logistic regression model to find what factors lead to the increase in the probability of the success of sanctions. Result: Sanctions are more likely to succeed if they aim narrowly, ban trade completely, and are aimed towards politically non-free countries.\nSenior Thesis on Political Economy of Corruption in Russia and the Effects of Sanctions on it\nPerformed extensive literature review of Russian sources, data analysis, and application of existing philosophical paradigms to answer multidisciplinary question of the nature of corruption in Russia."
<<<<<<< HEAD
=======
  },
  {
    "objectID": "ETS.html#abstract",
    "href": "ETS.html#abstract",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US",
    "section": "",
    "text": "In the efforts of reaching net zero emissions, renewable energy sources get more and more important. One of the cheapest and most sustainable sources of electricity is conventional hydroelectric power generation.\nEven though, the US has been using conventional hydropower since 1950s, the industry is far from reaching its full potential. With thousands of yet to be used dams and an increasing pressure to switch to the renewable energy, the demand and investments are going to rise significantly. Meanwhile, high seasonality and annual variation of the hydropower output can produce more accurate forecasting models. Here Seasonal Naïve, Regression, ETS, ARIMA, and ARIMA with external regressors are tested using RMSE and MPE to see which model better predicts unseen data. Using the net monthly conventional hydropower generation dataset, I identified ARIMA as the best one for the role.\nTechnical Note: A megawatt is a unit for measuring instantaneous power equivalent to one million watts or thousand kilowatts. A megawatt hour (MWh) is equal to 1,000 kilowatt hours (kWh). It is equal to 1,000 kilowatts of energy consumed or produced in one hour. It is a bit more than the amount of electricity used by an average US residential utility customer in one month (886 kWh), according to the EIA.\n\n\nCode\nlibrary(fpp3)\n\n\n── Attaching packages ────────────────────────────────────────────── fpp3 0.5 ──\n\n\n✔ tibble      3.1.8     ✔ tsibble     1.1.3\n✔ dplyr       1.1.0     ✔ tsibbledata 0.4.1\n✔ tidyr       1.2.0     ✔ feasts      0.3.1\n✔ lubridate   1.8.0     ✔ fable       0.3.3\n✔ ggplot2     3.3.5     ✔ fabletools  0.3.3\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()"
  },
  {
    "objectID": "Dengue_fc.html",
    "href": "Dengue_fc.html",
    "title": "Dengue Fever",
    "section": "",
    "text": "Dengue fever is a virus that is spread to people through the bite of an infected mosquito. A severe dengue fever case can lead to circulatory system failure, shock, and even death. Almost half of the world’s population, about 4 billion people, live in areas with a risk of dengue. Studying and forecasting the total number of infection incidents can help to tame this disease.\nI am using the fpp3 package to forecast the next total weekly cases of Dengue fever two cities in Peru and Puerto Rico.\nThe dataset is from drivendata.org competition.\n\n\nCode\nlibrary(fpp3)\n\n\n── Attaching packages ────────────────────────────────────────────── fpp3 0.5 ──\n\n\n✔ tibble      3.1.8     ✔ tsibble     1.1.3\n✔ dplyr       1.1.0     ✔ tsibbledata 0.4.1\n✔ tidyr       1.2.0     ✔ feasts      0.3.1\n✔ lubridate   1.8.0     ✔ fable       0.3.3\n✔ ggplot2     3.3.5     ✔ fabletools  0.3.3\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\n\nI am importing and combining the data on the number of cases and 20 possible predictor variables.\n\n\nCode\nlibrary(readr)\ndengue_fts &lt;- read_csv(\"DengAI_Predicting_Disease_Spread_-_Training_Data_Features.csv\")\n\n\nRows: 1456 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): city\ndbl  (22): year, weekofyear, ndvi_ne, ndvi_nw, ndvi_se, ndvi_sw, precipitati...\ndate  (1): week_start_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\ndengue_label &lt;- read_csv(\"DengAI_Predicting_Disease_Spread_-_Training_Data_Labels.csv\")\n\n\nRows: 1456 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): city\ndbl (3): year, weekofyear, total_cases\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\ndengue_label |&gt;\n  left_join(dengue_fts,\n            by = c(\"city\",\n                   \"year\",\n                   \"weekofyear\")) -&gt; dengue\n\n\nI am performing a quick random test to see if everything matches up.\n\n\nCode\ndengue_fts$reanalysis_dew_point_temp_k[256]==dengue$reanalysis_dew_point_temp_k[256]\n\n\n[1] TRUE\n\n\nI am transforming the combined dataset into a tsibble, the format for fpp3 package. The newly created year_week is an index variable and the two cities are keys to treat their observations as separate time series.\n\n\nCode\ndengue |&gt;\n  mutate(year_week = yearweek(week_start_date)) |&gt;\n  as_tsibble(key = city,\n             index = year_week) |&gt;\n  arrange(desc(city)) -&gt; dengue_ts\n\n\nHere is the plot fro both cities.\n\n\nCode\ndengue_ts |&gt;\n  #filter(city==\"sj\") |&gt;\n  autoplot(total_cases) +\n  labs(title = \"Total Cases of Dengue fever in San Juan and Iquitos\",\n       subtitle = \"April 1990 - June 2010, Weekly\",\n       y = \"Total Cases\",\n       x = \"Date\",\n       caption = \"Source: U.S. Centers for Disease Control and prevention\") +\n  theme_minimal()\n\n\n\n\n\nWe can see that the data for San Juan and Iquitos span different time periods but still overlap. We can also observe that most of the time the total number of cases is less than 50, however there are numerous spikes.\n\n\nCode\nplot(density(dengue_ts$total_cases))\nabline(v = 50, col = \"red\", lty = \"dashed\")\n\n\n\n\n\n\n\nCode\ndengue_ts |&gt;\n  filter(city==\"sj\") |&gt;\n  tail(1)\n\n\n# A tsibble: 1 x 26 [1W]\n# Key:       city [1]\n  city   year weekofyear total_cases week_start_date ndvi_ne ndvi_nw ndvi_se\n  &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 sj     2008         17           5 2008-04-22       -0.037 -0.0104  0.0773\n# … with 18 more variables: ndvi_sw &lt;dbl&gt;, precipitation_amt_mm &lt;dbl&gt;,\n#   reanalysis_air_temp_k &lt;dbl&gt;, reanalysis_avg_temp_k &lt;dbl&gt;,\n#   reanalysis_dew_point_temp_k &lt;dbl&gt;, reanalysis_max_air_temp_k &lt;dbl&gt;,\n#   reanalysis_min_air_temp_k &lt;dbl&gt;, reanalysis_precip_amt_kg_per_m2 &lt;dbl&gt;,\n#   reanalysis_relative_humidity_percent &lt;dbl&gt;,\n#   reanalysis_sat_precip_amt_mm &lt;dbl&gt;,\n#   reanalysis_specific_humidity_g_per_kg &lt;dbl&gt;, reanalysis_tdtr_k &lt;dbl&gt;, …\n\n\nCode\ndengue_ts |&gt;\n  filter(city==\"iq\") |&gt;\n  tail(1)\n\n\n# A tsibble: 1 x 26 [1W]\n# Key:       city [1]\n  city   year weekofyear total_cases week_start_date ndvi_ne ndvi_nw ndvi_se\n  &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 iq     2010         25           4 2010-06-25        0.298   0.233   0.274\n# … with 18 more variables: ndvi_sw &lt;dbl&gt;, precipitation_amt_mm &lt;dbl&gt;,\n#   reanalysis_air_temp_k &lt;dbl&gt;, reanalysis_avg_temp_k &lt;dbl&gt;,\n#   reanalysis_dew_point_temp_k &lt;dbl&gt;, reanalysis_max_air_temp_k &lt;dbl&gt;,\n#   reanalysis_min_air_temp_k &lt;dbl&gt;, reanalysis_precip_amt_kg_per_m2 &lt;dbl&gt;,\n#   reanalysis_relative_humidity_percent &lt;dbl&gt;,\n#   reanalysis_sat_precip_amt_mm &lt;dbl&gt;,\n#   reanalysis_specific_humidity_g_per_kg &lt;dbl&gt;, reanalysis_tdtr_k &lt;dbl&gt;, …\n\n\nMost of the models cannot work with gaps in time, so I am checking for explicit gaps.\n\n\nCode\nhas_gaps(dengue_ts, .full = T)\n\n\n# A tibble: 2 × 2\n  city  .gaps\n  &lt;chr&gt; &lt;lgl&gt;\n1 iq    TRUE \n2 sj    TRUE \n\n\nBoth cities have gaps, so I am filling gaps and using the previous value to fill them.\n\n\nCode\nlibrary(zoo)\n\n\n\nAttaching package: 'zoo'\n\n\nThe following object is masked from 'package:tsibble':\n\n    index\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\n\nCode\ndengue_ts |&gt;\n  fill_gaps() %&gt;%\n  mutate_all( ~ na.locf(.x, na.rm = FALSE)) -&gt; dengue_ts_c\n\n\nI am checking if there are any missing values left.\n\n\nCode\ncolSums(is.na(dengue_ts_c))\n\n\n                                 city                                  year \n                                    0                                     0 \n                           weekofyear                           total_cases \n                                    0                                     0 \n                      week_start_date                               ndvi_ne \n                                    0                                     0 \n                              ndvi_nw                               ndvi_se \n                                    0                                     0 \n                              ndvi_sw                  precipitation_amt_mm \n                                    0                                     0 \n                reanalysis_air_temp_k                 reanalysis_avg_temp_k \n                                    0                                     0 \n          reanalysis_dew_point_temp_k             reanalysis_max_air_temp_k \n                                    0                                     0 \n            reanalysis_min_air_temp_k       reanalysis_precip_amt_kg_per_m2 \n                                    0                                     0 \n reanalysis_relative_humidity_percent          reanalysis_sat_precip_amt_mm \n                                    0                                     0 \nreanalysis_specific_humidity_g_per_kg                     reanalysis_tdtr_k \n                                    0                                     0 \n                   station_avg_temp_c               station_diur_temp_rng_c \n                                    0                                     0 \n                   station_max_temp_c                    station_min_temp_c \n                                    0                                     0 \n                    station_precip_mm                             year_week \n                                    0                                     0 \n\n\nThere are no missing values in the dengue_ts_c.\n\n\nCode\ndengue_ts_c |&gt;\n  #filter(city==\"sj\") |&gt;\n  autoplot(total_cases)\n\n\n\n\n\nIt seems that there is some seasonality present. The seasonal plot shows that most of the spikes happen in the same months.\n\n\nCode\ndengue_ts_c |&gt;\n  gg_season(total_cases)\n\n\n\n\n\nI am performing an STL decomposition to identify seasonality and trend, if any.\n\n\nCode\ndengue_ts_c |&gt;\n  filter(city==\"sj\") |&gt;\n  model(\n    STL(total_cases)\n  ) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"STL decomposition for San Juan\")\n\n\nWarning: Ignoring unknown parameters: linewidth\n\n\n\n\n\n\n\nCode\ndengue_ts_c |&gt;\n  filter(city==\"iq\") |&gt;\n  model(\n    STL(total_cases)\n  ) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"STL decomposition for Iquitos\")\n\n\nWarning: Ignoring unknown parameters: linewidth\n\n\n\n\n\nIt seems that there is some seasonality when all the spikes are in the remainder. There also might be a slight increase in the trend for Iquitos.\nI am creating an ordered list of all the climate-related variables, descending from the highest correlation with the total cases to lowest.\n\n\nCode\ncor(dengue_ts_c[,c(6:25)],\n    dengue_ts_c[,4]) |&gt;\n  as.table() |&gt;\n  as.data.frame() |&gt;\n  select(-Var2) |&gt;\n  arrange(desc(Freq))\n\n\n                                    Var1        Freq\n1              reanalysis_min_air_temp_k  0.32528700\n2                     station_min_temp_c  0.26445735\n3                  reanalysis_air_temp_k  0.26295412\n4                  reanalysis_avg_temp_k  0.14823702\n5            reanalysis_dew_point_temp_k  0.13943398\n6  reanalysis_specific_humidity_g_per_kg  0.12666319\n7                     station_avg_temp_c  0.11350812\n8        reanalysis_precip_amt_kg_per_m2 -0.01146877\n9                     station_max_temp_c -0.04013282\n10                  precipitation_amt_mm -0.04243715\n11          reanalysis_sat_precip_amt_mm -0.04243715\n12                     station_precip_mm -0.07344970\n13  reanalysis_relative_humidity_percent -0.13335667\n14                               ndvi_sw -0.14512247\n15                               ndvi_nw -0.16844856\n16             reanalysis_max_air_temp_k -0.19310534\n17                               ndvi_ne -0.20513633\n18                               ndvi_se -0.21315790\n19               station_diur_temp_rng_c -0.23704112\n20                     reanalysis_tdtr_k -0.27946425\n\n\nThere seems to be no really strong correlation between the total cases and any individual predictor. Nevertheless, I am going to use this ranking when choosing the variables for time-consuming models.\nHere are plots of the variables with the highest correlations (in absolute values).\n\n\nCode\ndengue_ts_c %&gt;% \n  #filter(total_cases&lt;=100) %&gt;% \n  ggplot(aes(total_cases,\n             reanalysis_min_air_temp_k,\n             color = year)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\nCode\ndengue_ts_c %&gt;% \n  #filter(total_cases&lt;=100) %&gt;% \n  ggplot(aes(total_cases,\n             reanalysis_tdtr_k,\n             color = year)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nHere is the plot of all three variables stacked. There seems to be no obvious relationship between the predictors and the spikes.\n\n\nCode\ndengue_ts |&gt;\n  filter(city==\"sj\") |&gt;\n  pivot_longer(c(total_cases,\n                 reanalysis_min_air_temp_k,\n                 reanalysis_tdtr_k)) |&gt;\n  ggplot(aes(x = year_week, y = value)) +\n  geom_line() +\n  facet_grid(name ~ ., scales = \"free_y\") + ylab(\"\")\n\n\n\n\n\nWe can also observe relatively high correlation between the cases in one week and the week before.\n\n\nCode\ndengue_ts |&gt;\n  filter(city==\"sj\",\n         total_cases&lt;=100) |&gt;\n  gg_lag(y = total_cases,\n         lags = 1:8,\n         geom = \"point\")"
  },
  {
    "objectID": "Dengue_fc.html#significance",
    "href": "Dengue_fc.html#significance",
    "title": "Dengue Fever",
    "section": "",
    "text": "Dengue fever is a virus that is spread to people through the bite of an infected mosquito. A severe dengue fever case can lead to circulatory system failure, shock, and even death. Almost half of the world’s population, about 4 billion people, live in areas with a risk of dengue. Studying and forecasting the total number of infection incidents can help to tame this disease.\nI am using the fpp3 package to forecast the next total weekly cases of Dengue fever two cities in Peru and Puerto Rico.\nThe dataset is from drivendata.org competition.\n\n\nCode\nlibrary(fpp3)\n\n\n── Attaching packages ────────────────────────────────────────────── fpp3 0.5 ──\n\n\n✔ tibble      3.1.8     ✔ tsibble     1.1.3\n✔ dplyr       1.1.0     ✔ tsibbledata 0.4.1\n✔ tidyr       1.2.0     ✔ feasts      0.3.1\n✔ lubridate   1.8.0     ✔ fable       0.3.3\n✔ ggplot2     3.3.5     ✔ fabletools  0.3.3\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\n\nI am importing and combining the data on the number of cases and 20 possible predictor variables.\n\n\nCode\nlibrary(readr)\ndengue_fts &lt;- read_csv(\"DengAI_Predicting_Disease_Spread_-_Training_Data_Features.csv\")\n\n\nRows: 1456 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): city\ndbl  (22): year, weekofyear, ndvi_ne, ndvi_nw, ndvi_se, ndvi_sw, precipitati...\ndate  (1): week_start_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\ndengue_label &lt;- read_csv(\"DengAI_Predicting_Disease_Spread_-_Training_Data_Labels.csv\")\n\n\nRows: 1456 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): city\ndbl (3): year, weekofyear, total_cases\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\ndengue_label |&gt;\n  left_join(dengue_fts,\n            by = c(\"city\",\n                   \"year\",\n                   \"weekofyear\")) -&gt; dengue\n\n\nI am performing a quick random test to see if everything matches up.\n\n\nCode\ndengue_fts$reanalysis_dew_point_temp_k[256]==dengue$reanalysis_dew_point_temp_k[256]\n\n\n[1] TRUE\n\n\nI am transforming the combined dataset into a tsibble, the format for fpp3 package. The newly created year_week is an index variable and the two cities are keys to treat their observations as separate time series.\n\n\nCode\ndengue |&gt;\n  mutate(year_week = yearweek(week_start_date)) |&gt;\n  as_tsibble(key = city,\n             index = year_week) |&gt;\n  arrange(desc(city)) -&gt; dengue_ts\n\n\nHere is the plot fro both cities.\n\n\nCode\ndengue_ts |&gt;\n  #filter(city==\"sj\") |&gt;\n  autoplot(total_cases) +\n  labs(title = \"Total Cases of Dengue fever in San Juan and Iquitos\",\n       subtitle = \"April 1990 - June 2010, Weekly\",\n       y = \"Total Cases\",\n       x = \"Date\",\n       caption = \"Source: U.S. Centers for Disease Control and prevention\") +\n  theme_minimal()\n\n\n\n\n\nWe can see that the data for San Juan and Iquitos span different time periods but still overlap. We can also observe that most of the time the total number of cases is less than 50, however there are numerous spikes.\n\n\nCode\nplot(density(dengue_ts$total_cases))\nabline(v = 50, col = \"red\", lty = \"dashed\")\n\n\n\n\n\n\n\nCode\ndengue_ts |&gt;\n  filter(city==\"sj\") |&gt;\n  tail(1)\n\n\n# A tsibble: 1 x 26 [1W]\n# Key:       city [1]\n  city   year weekofyear total_cases week_start_date ndvi_ne ndvi_nw ndvi_se\n  &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 sj     2008         17           5 2008-04-22       -0.037 -0.0104  0.0773\n# … with 18 more variables: ndvi_sw &lt;dbl&gt;, precipitation_amt_mm &lt;dbl&gt;,\n#   reanalysis_air_temp_k &lt;dbl&gt;, reanalysis_avg_temp_k &lt;dbl&gt;,\n#   reanalysis_dew_point_temp_k &lt;dbl&gt;, reanalysis_max_air_temp_k &lt;dbl&gt;,\n#   reanalysis_min_air_temp_k &lt;dbl&gt;, reanalysis_precip_amt_kg_per_m2 &lt;dbl&gt;,\n#   reanalysis_relative_humidity_percent &lt;dbl&gt;,\n#   reanalysis_sat_precip_amt_mm &lt;dbl&gt;,\n#   reanalysis_specific_humidity_g_per_kg &lt;dbl&gt;, reanalysis_tdtr_k &lt;dbl&gt;, …\n\n\nCode\ndengue_ts |&gt;\n  filter(city==\"iq\") |&gt;\n  tail(1)\n\n\n# A tsibble: 1 x 26 [1W]\n# Key:       city [1]\n  city   year weekofyear total_cases week_start_date ndvi_ne ndvi_nw ndvi_se\n  &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 iq     2010         25           4 2010-06-25        0.298   0.233   0.274\n# … with 18 more variables: ndvi_sw &lt;dbl&gt;, precipitation_amt_mm &lt;dbl&gt;,\n#   reanalysis_air_temp_k &lt;dbl&gt;, reanalysis_avg_temp_k &lt;dbl&gt;,\n#   reanalysis_dew_point_temp_k &lt;dbl&gt;, reanalysis_max_air_temp_k &lt;dbl&gt;,\n#   reanalysis_min_air_temp_k &lt;dbl&gt;, reanalysis_precip_amt_kg_per_m2 &lt;dbl&gt;,\n#   reanalysis_relative_humidity_percent &lt;dbl&gt;,\n#   reanalysis_sat_precip_amt_mm &lt;dbl&gt;,\n#   reanalysis_specific_humidity_g_per_kg &lt;dbl&gt;, reanalysis_tdtr_k &lt;dbl&gt;, …\n\n\nMost of the models cannot work with gaps in time, so I am checking for explicit gaps.\n\n\nCode\nhas_gaps(dengue_ts, .full = T)\n\n\n# A tibble: 2 × 2\n  city  .gaps\n  &lt;chr&gt; &lt;lgl&gt;\n1 iq    TRUE \n2 sj    TRUE \n\n\nBoth cities have gaps, so I am filling gaps and using the previous value to fill them.\n\n\nCode\nlibrary(zoo)\n\n\n\nAttaching package: 'zoo'\n\n\nThe following object is masked from 'package:tsibble':\n\n    index\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\n\nCode\ndengue_ts |&gt;\n  fill_gaps() %&gt;%\n  mutate_all( ~ na.locf(.x, na.rm = FALSE)) -&gt; dengue_ts_c\n\n\nI am checking if there are any missing values left.\n\n\nCode\ncolSums(is.na(dengue_ts_c))\n\n\n                                 city                                  year \n                                    0                                     0 \n                           weekofyear                           total_cases \n                                    0                                     0 \n                      week_start_date                               ndvi_ne \n                                    0                                     0 \n                              ndvi_nw                               ndvi_se \n                                    0                                     0 \n                              ndvi_sw                  precipitation_amt_mm \n                                    0                                     0 \n                reanalysis_air_temp_k                 reanalysis_avg_temp_k \n                                    0                                     0 \n          reanalysis_dew_point_temp_k             reanalysis_max_air_temp_k \n                                    0                                     0 \n            reanalysis_min_air_temp_k       reanalysis_precip_amt_kg_per_m2 \n                                    0                                     0 \n reanalysis_relative_humidity_percent          reanalysis_sat_precip_amt_mm \n                                    0                                     0 \nreanalysis_specific_humidity_g_per_kg                     reanalysis_tdtr_k \n                                    0                                     0 \n                   station_avg_temp_c               station_diur_temp_rng_c \n                                    0                                     0 \n                   station_max_temp_c                    station_min_temp_c \n                                    0                                     0 \n                    station_precip_mm                             year_week \n                                    0                                     0 \n\n\nThere are no missing values in the dengue_ts_c.\n\n\nCode\ndengue_ts_c |&gt;\n  #filter(city==\"sj\") |&gt;\n  autoplot(total_cases)\n\n\n\n\n\nIt seems that there is some seasonality present. The seasonal plot shows that most of the spikes happen in the same months.\n\n\nCode\ndengue_ts_c |&gt;\n  gg_season(total_cases)\n\n\n\n\n\nI am performing an STL decomposition to identify seasonality and trend, if any.\n\n\nCode\ndengue_ts_c |&gt;\n  filter(city==\"sj\") |&gt;\n  model(\n    STL(total_cases)\n  ) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"STL decomposition for San Juan\")\n\n\nWarning: Ignoring unknown parameters: linewidth\n\n\n\n\n\n\n\nCode\ndengue_ts_c |&gt;\n  filter(city==\"iq\") |&gt;\n  model(\n    STL(total_cases)\n  ) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"STL decomposition for Iquitos\")\n\n\nWarning: Ignoring unknown parameters: linewidth\n\n\n\n\n\nIt seems that there is some seasonality when all the spikes are in the remainder. There also might be a slight increase in the trend for Iquitos.\nI am creating an ordered list of all the climate-related variables, descending from the highest correlation with the total cases to lowest.\n\n\nCode\ncor(dengue_ts_c[,c(6:25)],\n    dengue_ts_c[,4]) |&gt;\n  as.table() |&gt;\n  as.data.frame() |&gt;\n  select(-Var2) |&gt;\n  arrange(desc(Freq))\n\n\n                                    Var1        Freq\n1              reanalysis_min_air_temp_k  0.32528700\n2                     station_min_temp_c  0.26445735\n3                  reanalysis_air_temp_k  0.26295412\n4                  reanalysis_avg_temp_k  0.14823702\n5            reanalysis_dew_point_temp_k  0.13943398\n6  reanalysis_specific_humidity_g_per_kg  0.12666319\n7                     station_avg_temp_c  0.11350812\n8        reanalysis_precip_amt_kg_per_m2 -0.01146877\n9                     station_max_temp_c -0.04013282\n10                  precipitation_amt_mm -0.04243715\n11          reanalysis_sat_precip_amt_mm -0.04243715\n12                     station_precip_mm -0.07344970\n13  reanalysis_relative_humidity_percent -0.13335667\n14                               ndvi_sw -0.14512247\n15                               ndvi_nw -0.16844856\n16             reanalysis_max_air_temp_k -0.19310534\n17                               ndvi_ne -0.20513633\n18                               ndvi_se -0.21315790\n19               station_diur_temp_rng_c -0.23704112\n20                     reanalysis_tdtr_k -0.27946425\n\n\nThere seems to be no really strong correlation between the total cases and any individual predictor. Nevertheless, I am going to use this ranking when choosing the variables for time-consuming models.\nHere are plots of the variables with the highest correlations (in absolute values).\n\n\nCode\ndengue_ts_c %&gt;% \n  #filter(total_cases&lt;=100) %&gt;% \n  ggplot(aes(total_cases,\n             reanalysis_min_air_temp_k,\n             color = year)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\nCode\ndengue_ts_c %&gt;% \n  #filter(total_cases&lt;=100) %&gt;% \n  ggplot(aes(total_cases,\n             reanalysis_tdtr_k,\n             color = year)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nHere is the plot of all three variables stacked. There seems to be no obvious relationship between the predictors and the spikes.\n\n\nCode\ndengue_ts |&gt;\n  filter(city==\"sj\") |&gt;\n  pivot_longer(c(total_cases,\n                 reanalysis_min_air_temp_k,\n                 reanalysis_tdtr_k)) |&gt;\n  ggplot(aes(x = year_week, y = value)) +\n  geom_line() +\n  facet_grid(name ~ ., scales = \"free_y\") + ylab(\"\")\n\n\n\n\n\nWe can also observe relatively high correlation between the cases in one week and the week before.\n\n\nCode\ndengue_ts |&gt;\n  filter(city==\"sj\",\n         total_cases&lt;=100) |&gt;\n  gg_lag(y = total_cases,\n         lags = 1:8,\n         geom = \"point\")"
  },
  {
    "objectID": "Dengue_fc.html#outliers",
    "href": "Dengue_fc.html#outliers",
    "title": "Dengue Fever",
    "section": "Outliers",
    "text": "Outliers\n\n\nCode\ndengue_ts_c |&gt;\n  filter(city==\"iq\") |&gt;\n  ggplot(aes(total_cases)) +\n  geom_density()\n\n\n\n\n\nI am creating a dummy identifying if there was a spike in weekly cases.\n\n\nCode\ndengue_ts_c |&gt;\n  mutate(outlier = ifelse(total_cases&gt;=100&city==\"sj\",\n                          1,\n                          ifelse(total_cases&gt;=15&city==\"iq\",\n                                 1,\n                                 0)\n                          )) -&gt; dengue_100up\n\n\nHere is a visual representation.\n\n\nCode\ndengue_100up %&gt;% \n  filter(city==\"sj\") %&gt;% \n  ggplot(aes(x = year_week, y = total_cases)) +\n  geom_rect(aes(xmin = lag(year_week),\n                  xmax = year_week,\n                  ymin = 0, \n                  ymax = Inf, \n                  fill = outlier == 1), \n                  alpha = 0.5) +\n  geom_line() +\n  scale_fill_manual(values = c(\"transparent\", \n                               \"red\"),\n                    guide = \"none\")\n\n\nWarning: Removed 1 rows containing missing values (geom_rect).\n\n\n\n\n\nI am then creating a new list based on the variables’ correlation with the spike incidents.\n\n\nCode\ncor(dengue_100up[,c(6:25)],\n    dengue_100up[,27]) |&gt;\n  as.table() |&gt;\n  as.data.frame() |&gt;\n  select(-Var2) |&gt;\n  arrange(desc(Freq))\n\n\n                                    Var1         Freq\n1                     station_max_temp_c  0.198169569\n2  reanalysis_specific_humidity_g_per_kg  0.184571781\n3                     station_avg_temp_c  0.180714057\n4            reanalysis_dew_point_temp_k  0.174252235\n5              reanalysis_max_air_temp_k  0.157736465\n6                station_diur_temp_rng_c  0.145657342\n7   reanalysis_relative_humidity_percent  0.137849906\n8                  reanalysis_avg_temp_k  0.130503427\n9                   precipitation_amt_mm  0.117053029\n10          reanalysis_sat_precip_amt_mm  0.117053029\n11                     reanalysis_tdtr_k  0.107245826\n12                               ndvi_nw  0.101885734\n13       reanalysis_precip_amt_kg_per_m2  0.099191371\n14                               ndvi_ne  0.092681542\n15                               ndvi_sw  0.091356494\n16                    station_min_temp_c  0.080883428\n17                     station_precip_mm  0.071557679\n18                 reanalysis_air_temp_k  0.066923290\n19                               ndvi_se  0.006051806\n20             reanalysis_min_air_temp_k -0.018410823\n\n\n\n\nCode\ndengue_ts |&gt;\n  filter(city==\"sj\") |&gt;\n  pivot_longer(c(total_cases,\n                 station_max_temp_c,\n                 reanalysis_specific_humidity_g_per_kg)) |&gt;\n  ggplot(aes(x = year_week, y = value)) +\n  geom_line() +\n  facet_grid(name ~ ., scales = \"free_y\") + ylab(\"\")\n\n\n\n\n\nNo obvious relationship between the spikes and the two variables.\nAfter looking at the data, I am proceeding to create the forecast models."
  },
  {
    "objectID": "Dengue_fc.html#stl-decomposition-and-non-seasonal-method",
    "href": "Dengue_fc.html#stl-decomposition-and-non-seasonal-method",
    "title": "Dengue Fever",
    "section": "STL decomposition and non-seasonal method",
    "text": "STL decomposition and non-seasonal method\nAccording to Hyndman, et al., the simplest method is STL decomposition with a non-seasonal model applied to the seasonally adjusted data. I am creating a decomposition model with STL and non-seasonal ETS.\n\n\nCode\ndecomposition_model(\n  STL(total_cases),\n  ETS(season_adjust ~ season(\"N\"))\n) -&gt; den_dcmp_envm\n\ndengue_ts_c |&gt;\n  model(stl_ets = den_dcmp_envm) |&gt;\n  forecast(h = \"5 years\") -&gt; den_dcmp_fc\n\n\n\n\nCode\nden_dcmp_fc |&gt;\n  autoplot(dengue_ts_c) +\n  labs(title = \"Decomposition and STL forecast\",\n       y = \"Total Cases\",\n       x = \"Date\")\n\n\n\n\n\nI am forecasting 5 years for both of the cities to then cut the last two years of Iquitos.\nDue to its simplicity, it is similar to Seasonal Naive."
  },
  {
    "objectID": "Dengue_fc.html#arima-w-fourier",
    "href": "Dengue_fc.html#arima-w-fourier",
    "title": "Dengue Fever",
    "section": "ARIMA w/ Fourier",
    "text": "ARIMA w/ Fourier\nHere, I am creating an ARIMA model where seasonality is captured by Fourier series.\nI am creating several models with increasing number of K to then choose one based on the AICc.\n\n\nCode\ndengue_ts_c |&gt;\n  model(\n    K1 = ARIMA(total_cases ~ PDQ(0, 0, 0) + fourier(K = 1)),\n    K2 = ARIMA(total_cases ~ PDQ(0, 0, 0) + fourier(K = 2)),\n    K3 = ARIMA(total_cases ~ PDQ(0, 0, 0) + fourier(K = 3)),\n    K4 = ARIMA(total_cases ~ PDQ(0, 0, 0) + fourier(K = 4)),\n    K5 = ARIMA(total_cases ~ PDQ(0, 0, 0) + fourier(K = 5)),\n    K6 = ARIMA(total_cases ~ PDQ(0, 0, 0) + fourier(K = 6)),\n    K12 = ARIMA(total_cases ~ PDQ(0, 0, 0) + fourier(K = 12)),\n    K15 = ARIMA(total_cases ~ PDQ(0, 0, 0) + fourier(K = 15)),\n    K26 = ARIMA(total_cases ~ PDQ(0, 0, 0) + fourier(K = 26)),\n        ) -&gt; dengue_arima3\n\n\n\n\nCode\nglance(dengue_arima3) |&gt;\n  select(city, .model, AICc)\n\n\n# A tibble: 18 × 3\n   city  .model  AICc\n   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1 iq    K1     3539.\n 2 iq    K2     3543.\n 3 iq    K3     3545.\n 4 iq    K4     3546.\n 5 iq    K5     3548.\n 6 iq    K6     3549.\n 7 iq    K12    3545.\n 8 iq    K15    3549.\n 9 iq    K26    3571.\n10 sj    K1     7521.\n11 sj    K2     7520.\n12 sj    K3     7523.\n13 sj    K4     7525.\n14 sj    K5     7529.\n15 sj    K6     7532.\n16 sj    K12    7543.\n17 sj    K15    7548.\n18 sj    K26    7565.\n\n\nThe larger numbers of K worsen the AICc. Only the model for San Juan is slightly better at K = 2. Nevertheless, since the improvement is small, to stay consistent, I keep K = 1.\n\n\nCode\ndengue_arima3 |&gt;\n  select(K1) |&gt;\n  forecast(h = \"5 years\") -&gt; den_arima_fc\n\n\n\n\nCode\nden_arima_fc |&gt;\n  autoplot(dengue_ts_c) +\n  labs(title = \"ARIMA with Fourier forecast\",\n       y = \"Total Cases\",\n       x = \"Date\")"
  },
  {
    "objectID": "Dengue_fc.html#nnetar",
    "href": "Dengue_fc.html#nnetar",
    "title": "Dengue Fever",
    "section": "NNETAR",
    "text": "NNETAR\nThe third model is Neural Networks. This is the most time consuming model, but the results can be very promising.\n\n\nCode\nden_nn_model = dengue_ts_c |&gt;\n  model(nnetar = NNETAR(total_cases))\n\n\n\n\nCode\nstart_time = Sys.time()\n\nden_nn_model |&gt;\n  forecast(h = \"3 years\") -&gt; den_nn_fc\n\nprint(Sys.time() - start_time)\n\n\nTime difference of 19.68229 mins\n\n\n\n\nCode\nden_nn_fc |&gt;\n  autoplot(dengue_ts_c) +\n  labs(title = \"NNETAR with no external regs forecast\",\n       y = \"Total Cases\",\n       x = \"Date\")\n\n\n\n\n\nHere we can see that the model for Iquitos picked up the trend but then flattened out, which doesn’t look good. The model for San Juan, however looks promising but also looses all seasonality later on.\nI am no trying NNETAR with external regressors: station_max_temp_c\nreanalysis_specific_humidity_g_per_kg\nreanalysis_min_air_temp_k reanalysis_air_temp_k.\nHowever, in order to use them, I need features from the test data.\n\n\nCode\nden_test_fts &lt;- read_csv(\"DengAI_Predicting_Disease_Spread_-_Test_Data_Features.csv\")\n\n\nRows: 416 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): city\ndbl  (22): year, weekofyear, ndvi_ne, ndvi_nw, ndvi_se, ndvi_sw, precipitati...\ndate  (1): week_start_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe test dataset also has some gaps, so I am filling them.\n\n\nCode\nden_test_fts |&gt;\n  mutate(year_week = yearweek(week_start_date)) |&gt;\n  as_tsibble(key = city,\n             index = year_week) |&gt;\n  fill_gaps() %&gt;% \n  mutate_all( ~ na.locf(.x, na.rm = FALSE)) %&gt;% \n  arrange(desc(city)) -&gt; den_test_ts2\n\n\n\n\nCode\nden_nn_model2 = dengue_ts_c |&gt;\n  model(nnetar = NNETAR(total_cases ~ station_max_temp_c +\n                          reanalysis_specific_humidity_g_per_kg +\n                          reanalysis_min_air_temp_k +\n                          reanalysis_air_temp_k))\n\n\n\n\nCode\nstart_time = Sys.time()\n\nden_nn_model2 |&gt;\n  forecast(new_data = den_test_ts2) -&gt; den_nn_fc2\n\nprint(Sys.time() - start_time)\n\n\nTime difference of 26.75402 mins\n\n\n\n\nCode\nden_nn_fc2 |&gt;\n  autoplot(dengue_ts_c) +\n  labs(title = \"NNETAR w/ Regressors forecast\",\n       y = \"Total Cases\",\n       x = \"Date\")\n\n\n\n\n\nThe resulting forecast looks very promising, indeed. The prediction intervals also seem to cover most of the probable observations, without getting into negative territory."
  },
  {
    "objectID": "Dengue_fc.html#arima-w-regressors",
    "href": "Dengue_fc.html#arima-w-regressors",
    "title": "Dengue Fever",
    "section": "ARIMA w/ Regressors",
    "text": "ARIMA w/ Regressors\nTo incorporate more predictor variables from the dataset, I am using another ARIMA model with external regressors.\n\n\nCode\ndengue_ts_c |&gt;\n  model(ARIMA.reg = ARIMA(total_cases ~ PDQ(0, 0, 0) + \n                            reanalysis_min_air_temp_k +\n                            reanalysis_air_temp_k +\n                            station_min_temp_c)) -&gt; den_arreg_fit\n\n\n\n\nCode\nden_arreg_fit |&gt;\n  forecast(new_data = den_test_ts2) -&gt; den_arreg_fc2\n\n\n\n\nCode\nden_arreg_fc2 |&gt;\n  autoplot(dengue_ts_c)\n\n\n\n\n\nUsing only the variables with highest correlation doesn’t seem to improve the forecast, all that much. Therefore, I am going to use all of them. I am also adding Fourier for seasonality.\n\n\nCode\ncolnames(dengue_ts_c)\n\n\n [1] \"city\"                                 \n [2] \"year\"                                 \n [3] \"weekofyear\"                           \n [4] \"total_cases\"                          \n [5] \"week_start_date\"                      \n [6] \"ndvi_ne\"                              \n [7] \"ndvi_nw\"                              \n [8] \"ndvi_se\"                              \n [9] \"ndvi_sw\"                              \n[10] \"precipitation_amt_mm\"                 \n[11] \"reanalysis_air_temp_k\"                \n[12] \"reanalysis_avg_temp_k\"                \n[13] \"reanalysis_dew_point_temp_k\"          \n[14] \"reanalysis_max_air_temp_k\"            \n[15] \"reanalysis_min_air_temp_k\"            \n[16] \"reanalysis_precip_amt_kg_per_m2\"      \n[17] \"reanalysis_relative_humidity_percent\" \n[18] \"reanalysis_sat_precip_amt_mm\"         \n[19] \"reanalysis_specific_humidity_g_per_kg\"\n[20] \"reanalysis_tdtr_k\"                    \n[21] \"station_avg_temp_c\"                   \n[22] \"station_diur_temp_rng_c\"              \n[23] \"station_max_temp_c\"                   \n[24] \"station_min_temp_c\"                   \n[25] \"station_precip_mm\"                    \n[26] \"year_week\"                            \n\n\n\n\nCode\ndengue_ts_c |&gt;\n  model(ARIMA.reg = ARIMA(total_cases ~ PDQ(0, 0, 0) + \n                            ndvi_ne + ndvi_nw + ndvi_se + ndvi_sw +\n                            precipitation_amt_mm + \n                            reanalysis_air_temp_k + \n                            reanalysis_avg_temp_k +\n                            reanalysis_dew_point_temp_k +\n                            reanalysis_max_air_temp_k +\n                            reanalysis_min_air_temp_k +\n                            reanalysis_relative_humidity_percent +\n                            reanalysis_specific_humidity_g_per_kg +\n                            reanalysis_precip_amt_kg_per_m2 +\n                            reanalysis_tdtr_k +\n                            station_avg_temp_c +\n                            station_diur_temp_rng_c +\n                            station_max_temp_c +\n                            station_min_temp_c +\n                            station_precip_mm +\n                            fourier(K = 1)))-&gt;den_arreg_fit2\n\n#Removed reanalysis_sat_precip_amt_mm to avoid rank deficiency\n\n\n\n\nCode\nden_arreg_fit2 |&gt;\n  forecast(new_data = den_test_ts2) -&gt; den_arreg_fc3\n\n\n\n\nCode\nden_arreg_fc3 |&gt;\n  autoplot(dengue_ts_c)\n\n\n\n\n\nThe forecast looks better now, but it looks too clean compared to the previous values.\n\n\nCode\ndengue_ts_c |&gt;\n  model(ARIMA.reg = ARIMA(total_cases ~ PDQ(0, 0, 0) + \n                            ndvi_ne + ndvi_nw + ndvi_se + ndvi_sw +\n                            precipitation_amt_mm + \n                            reanalysis_air_temp_k + \n                            reanalysis_avg_temp_k +\n                            reanalysis_dew_point_temp_k +\n                            reanalysis_max_air_temp_k +\n                            reanalysis_min_air_temp_k +\n                            reanalysis_relative_humidity_percent +\n                            reanalysis_specific_humidity_g_per_kg +\n                            reanalysis_precip_amt_kg_per_m2 +\n                            reanalysis_tdtr_k +\n                            station_avg_temp_c +\n                            station_diur_temp_rng_c +\n                            station_max_temp_c +\n                            station_min_temp_c +\n                            station_precip_mm +\n                            season()))-&gt;den_arreg_fit3\n\n#Removed reanalysis_sat_precip_amt_mm to avoid rank deficiency\n\n\n\n\nCode\nden_arreg_fit3 |&gt;\n  forecast(new_data = den_test_ts2) -&gt; den_arreg_fc4\n\n\n\n\nCode\nden_arreg_fc4 |&gt;\n  autoplot(dengue_ts_c) +\n  labs(title = \"ARIMA w/ Regressors and Seasonality forecast\",\n       y = \"Total Cases\",\n       x = \"Date\")\n\n\n\n\n\nThe same ARIMA model with seasonality included looks better than that with Fourier but worse than NNETAR."
  },
  {
    "objectID": "Dengue_fc.html#results",
    "href": "Dengue_fc.html#results",
    "title": "Dengue Fever",
    "section": "Results",
    "text": "Results\nI had to upload the forecasts to the drivendata.org to obtain the evaluation of the models. The forecast used the observations at time T to forecast the number of cases at time T, because it was a forecast for the internally missing data.\nI also had to manually remove some 3 observations from the predictions, since the evaluation dataset didn’t have some dates.\nGiven the Score and the Rank, NNETAR model is by far the best one. Decomposition model’s performance was quite low, ARIMA with seasonality and external regressors got a score of 25.93 and rank of 2042. NNETAR did better by a wide margin: the MAE score of 22.48 and rank of 512. This is out of approximately 12 thousand participants.\nThe respective submission IDs are id-240847, id-240852, and id-240853."
  },
  {
    "objectID": "Dengue_fc.html#further-work",
    "href": "Dengue_fc.html#further-work",
    "title": "Dengue Fever",
    "section": "Further work",
    "text": "Further work\nAdding more external variables will most probably increase the performance of NNETAR model. There also might be some calendar variables that should be included. For example, in the beginning we saw that the spikes occur in the same months. Adding dummies for high number months might improve the forecasts."
  },
  {
    "objectID": "ETS.html#data-preparation-and-analysis",
    "href": "ETS.html#data-preparation-and-analysis",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US",
    "section": "Data Preparation and Analysis",
    "text": "Data Preparation and Analysis\nI import the data on net energy generation of electricity from the conventional hydroelectric sources from the EIA website. The units are measured in thousand megawatt hours. The timeframe is from January 2001 through April 2023.\n\n\nCode\nlibrary(readr)\nnet_gen &lt;- read_csv(\"Net_generation_United_States_all_sectors_monthly.csv\", \n    skip = 4)\n\n\nRows: 268 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Month\ndbl (1): conventional hydroelectric thousand megawatthours\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHere is a preview of the dataset:\n\n\nCode\nhead(net_gen)\n\n\n# A tibble: 6 × 2\n  Month    `conventional hydroelectric thousand megawatthours`\n  &lt;chr&gt;                                                  &lt;dbl&gt;\n1 Apr 2023                                              17917.\n2 Mar 2023                                              20630.\n3 Feb 2023                                              19338.\n4 Jan 2023                                              22954.\n5 Dec 2022                                              21870.\n6 Nov 2022                                              18764.\n\n\n\n\nCode\nlibrary(summarytools)\n\n\n\nAttaching package: 'summarytools'\n\n\nThe following object is masked from 'package:tibble':\n\n    view\n\n\nI am changing the name of the variable to MWH for brevity.\n\n\nCode\n# rename the variable to MWH\nnet_gen = net_gen %&gt;% \n  rename(MWH = \"conventional hydroelectric thousand megawatthours\")\n\n\nHere is a table of some descriptive statistics:\n\n\nCode\ndescr(net_gen,\n      stats = c(\"mean\", \"sd\",\"med\", \"min\", \"max\"), \n      transpose = TRUE)\n\n\nNon-numerical variable(s) ignored: Month\n\n\nDescriptive Statistics  \nnet_gen$MWH  \nN: 268  \n\n                Mean   Std.Dev     Median        Min        Max\n--------- ---------- --------- ---------- ---------- ----------\n      MWH   22469.85   3972.41   22187.54   14638.18   32607.12\n\n\nI reorder the rows, so that the last observation (earliest date) is now the first one.\n\n\nCode\nnet_gen=net_gen[order(nrow(net_gen):1),]\n\n\n\n\nCode\n#changing to yearmonth format, naming Date, and indexing\nnet_gen = net_gen %&gt;% \n  mutate(Date = yearmonth(Month)) |&gt;\n  as_tsibble(index = Date)\n\n\n\n\nCode\nnet_gen %&gt;% \n  autoplot(MWH) +\n  labs(title = \"Net Conventional Hydroelectric Power Generation\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\nHere we can observe a general plot of the Hydropower time series. There seems to be no significant trend and high seasonality.\n\n\nCode\nnet_gen |&gt;\n  gg_season(MWH, labels = \"both\")\n\n\n\n\n\nAs we can see here, there are spikes in the same months across all years, indicating high seasonality.\n\n\nCode\nnet_gen |&gt;\n  gg_subseries(MWH)\n\n\n\n\n\nThis plot, where the blue line is the average across the same months, shows that on average there is a spike in the late spring to early summer. The trough is in fall.\n\n\nCode\nnet_gen |&gt;\n  model(stl = STL(MWH)) |&gt;\n  components() |&gt; \n  gg_subseries(season_year) +\n  theme(axis.text.x = element_text(size = 5))\n\n\n\n\n\nIn this plot we can see the average and the trend by month across different years. This plot shows the same spikes and troughs, but we can also observe that the months with the highest values have a downward trend. This means that even though the energy output in those months is still the highest, it diminishes.\n\n\nCode\nnet_gen |&gt;\n  model(stl = STL(MWH)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\nWarning: Ignoring unknown parameters: linewidth\n\n\n\n\n\nHere we see the STL decomposition components: trend, seasonality, and the remainder. There is high seasonality and some downward trend at the end of the series. The gray bars on the left show us that seasonality is very significant, whereas the general trend is less so.\n\n\nCode\nnet_gen |&gt;\n  ACF(MWH) |&gt;\n  autoplot()\n\n\n\n\n\nThe plot of Autocorrelation Figure also indicates high seasonality, since the the values 12 months apart have the highest correlation.\nI estimate Naive, Seasonal Naive, Drift, and two ETS models. There are two ETS models because the optimal ETS() (the default one) seems to be underperforming, since it chooses no trend. I add ETS(A,Ad,M) since it seems to increase forecast accuracy.\nA mable: 1 x 5 Naive Season_Naive Drift ETSopt ETS      1    &lt;ETS(M,N,A)&gt; &lt;ETS(A,Ad,M)&gt;"
  },
  {
    "objectID": "ETS.html#creating-training-and-testing-sets.",
    "href": "ETS.html#creating-training-and-testing-sets.",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US",
    "section": "Creating Training and Testing sets.",
    "text": "Creating Training and Testing sets.\n\n\nCode\n#creating the var with the number of ~80% and ~20% of the observations and then splitting them accordingly.\nround(268*0.8)\n\n\n[1] 214\n\n\nCode\ntotal_obs.net_gen = dim(net_gen)[1] #puts n of obs into total_obs\ntrain_obs = total_obs.net_gen * 0.8\ntest_obs = total_obs.net_gen - train_obs\nnet_gen.train2 = head(net_gen, train_obs)\nnet_gen.test2 = tail(net_gen, test_obs)"
  },
  {
    "objectID": "ETS.html#arima-estimation",
    "href": "ETS.html#arima-estimation",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US",
    "section": "ARIMA Estimation",
    "text": "ARIMA Estimation\n\n\nCode\nnet_gen |&gt; gg_tsdisplay(MWH,\n                     plot_type='partial', lag_max = 24)\n\n\n\n\n\n\n\nCode\nnet_gen |&gt; gg_tsdisplay(difference(MWH, 12),\n                     plot_type='partial', lag_max = 24)\n\n\nWarning: Removed 12 row(s) containing missing values (geom_path).\n\n\nWarning: Removed 12 rows containing missing values (geom_point).\n\n\n\n\n\n\n\nCode\nnet_gen |&gt; gg_tsdisplay(difference(MWH, 12) |&gt; difference(),\n                     plot_type='partial', lag_max = 24)\n\n\nWarning: Removed 13 row(s) containing missing values (geom_path).\n\n\nWarning: Removed 13 rows containing missing values (geom_point).\n\n\n\n\n\n\n\nCode\n#initial ARIMA estimation\nnet_gen.train2 |&gt;\n  model(auto = ARIMA(MWH,\n                     stepwise = FALSE, #gives ARIMA more time to calculate\n                     approx = FALSE)) -&gt; arima.fit\n\n\nWarning in sqrt(diag(best$var.coef)): NaNs produced\n\n\nWhen estimating ARIMA models, it is important to check if the residuals look like white noise and have no patterns.\n\n\nCode\narima.fit |&gt;\n  gg_tsresiduals(lag_max=36)\n\n\n\n\n\nThe graphic of the residuals shows that residuals look pretty random, but there is a spike in acf that is statistically significant (crossed the blue line). I have to perform Ljung-Box test.\n\n\nCode\narima.fit\n\n\n# A mable: 1 x 1\n                       auto\n                    &lt;model&gt;\n1 &lt;ARIMA(2,0,0)(2,1,1)[12]&gt;\n\n\nA mable: 1 x 1 auto  1 &lt;ARIMA(2,0,0)(2,1,1)[12]&gt;\n\n\nCode\n#Ljung-Box test\naugment(arima.fit) |&gt;\n  features(.innov, ljung_box, lag = 36, dof = 5)\n\n\n# A tibble: 1 × 3\n  .model lb_stat lb_pvalue\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 auto      28.9     0.575\n\n\nCode\n#dof=p+q+P+Q from ARIMA(p,d,q)(P,D,Q)\n\n\nThe p-value is very large, which is exactly what we need to proceed and not care about autocorrelation in the residuals."
  },
  {
    "objectID": "ETS.html#seasonal-naive-ts-regression-and-ets",
    "href": "ETS.html#seasonal-naive-ts-regression-and-ets",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US",
    "section": "Seasonal Naive, TS Regression, and ETS",
    "text": "Seasonal Naive, TS Regression, and ETS\n\n\nCode\nnet_gen.fit2 = net_gen.train2 |&gt;\n  model(\n        Season_Naive = SNAIVE(MWH),\n        Regression = TSLM(MWH ~ trend() + season()),\n        ETSopt = ETS(MWH), #automatic\n        ETS = ETS(MWH ~ error(\"A\")\n                      + trend(\"Ad\")\n                      + season(\"M\"))  #industry standard\n        )\n\n\n\n\nCode\nnet_gen.fit2\n\n\n# A mable: 1 x 4\n  Season_Naive Regression        ETSopt           ETS\n       &lt;model&gt;    &lt;model&gt;       &lt;model&gt;       &lt;model&gt;\n1     &lt;SNAIVE&gt;     &lt;TSLM&gt; &lt;ETS(M,Ad,M)&gt; &lt;ETS(A,Ad,M)&gt;\n\n\nA mable: 1 x 4 Season_Naive Regression ETSopt ETS     1   &lt;ETS(M,Ad,M)&gt; &lt;ETS(A,Ad,M)&gt;\n\n\nCode\naccuracy(net_gen.fit2) |&gt;\n  select(.model, RMSE, MAE)\n\n\n# A tibble: 4 × 3\n  .model        RMSE   MAE\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 Season_Naive 3201. 2437.\n2 Regression   2421. 1856.\n3 ETSopt       1687. 1296.\n4 ETS          1685. 1290.\n\n\n\n\nCode\naccuracy(arima.fit) |&gt;\n  select(.model, RMSE, MAE)\n\n\n# A tibble: 1 × 3\n  .model  RMSE   MAE\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 auto   1698. 1243.\n\n\nWe can see how closely the corresponding models predict the data within the Training set.\nNow using these models I predict the next 54 months and compare the predictions to the withheld observations.\n\n\nCode\n#generating forecast\nnet_gen.fc2 = net_gen.fit2 |&gt;\n  forecast(h = 54)\nnet_gen.fc2.arima = arima.fit |&gt;\n  forecast(h = 54)\n\n\n\n\nCode\nnet_gen.fc2 |&gt;\n  autoplot(net_gen.train2, level = NULL) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation Forecast\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal() +\n  theme(plot.caption = element_text(hjust = 6.5))\n\n\n\n\n\n\n\nCode\nnet_gen.fc2 |&gt;\n  #filter(.model = \"\") |&gt;\n  autoplot(net_gen.test2, level = NULL) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation Forecast\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\nCode\n  #theme(plot.caption = element_text(hjust = 5))\n\n\n\n\nCode\nnet_gen.fc2.arima |&gt;\n  autoplot(net_gen.train2) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation\nARIMA Forecast\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\nnet_gen.fc2.arima |&gt;\n  #filter(.model = \"\") |&gt;\n  autoplot(net_gen.test2) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation\nARIMA Forecast\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\nCode\n  #theme(plot.caption = element_text(hjust = 5))\n\n\n\n\nCode\nnet_gen.fc2 |&gt;\n  accuracy(net_gen) |&gt;\n  select(.model, RMSE, ME, MPE)\n\n\n# A tibble: 4 × 4\n  .model        RMSE     ME    MPE\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 ETS          3800. -2797. -13.3 \n2 ETSopt       3863. -2877. -13.7 \n3 Regression   2928. -1518.  -7.79\n4 Season_Naive 3323. -1733.  -8.80\n\n\n\n\nCode\nnet_gen.fc2.arima |&gt;\n  accuracy(net_gen) |&gt;\n  select(.model, RMSE, ME, MPE)\n\n\n# A tibble: 1 × 4\n  .model  RMSE    ME   MPE\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 auto   2638. -567. -3.41\n\n\n\n\nCode\nnet_gen.fc2 |&gt;\n  accuracy(net_gen) |&gt;\n  select(.model, RMSE, ME, MPE) |&gt;\n  as.data.frame() -&gt; acc.metric\n\nnet_gen.fc2.arima |&gt;\n  accuracy(net_gen) |&gt;\n  select(.model, RMSE, ME, MPE) |&gt;\n  as.data.frame() |&gt;\n  bind_rows(acc.metric) -&gt; acc.metric\n\nacc.metric[1,1] = \"ARIMA\"\n\n\n\n\nCode\nacc.metric %&gt;% \n  group_by(.model, RMSE) %&gt;% \n  ggplot(aes(x = RMSE, \n             xend = 2500, \n             y = reorder(.model, desc(RMSE)), \n             yend=.model,\n             label=round(RMSE, 0))) +\n  theme_minimal() +\n  geom_segment() +\n  geom_point() +\n  geom_text(nudge_x = -50, nudge_y = 0.2,\n            size = 3) +\n  theme(axis.ticks.y = element_blank(),\n        axis.text.y = element_blank()) +\n  labs(x = \"RMSE\", y = element_blank()) -&gt; rmse.plot\n\n\n\n\nCode\nacc.metric %&gt;% \n  group_by(.model, ME) %&gt;% \n  ggplot(aes(x = ME, \n             xend = 0, \n             y = reorder(.model, desc(abs(ME))), \n             yend=.model,\n             label=round(ME, 2))) +\n  theme_minimal() +\n  geom_segment() +\n  geom_point() +\n  geom_text(nudge_x = 0, nudge_y = 0.2) +\n  #theme(axis.text.y=element_text(margin=margin(r=0))) +\n  labs(title = \"Mean Error by Forecast Model\", x = \"ME\", y = \"Models\")\n\n\n\n\n\n\n\nCode\nacc.metric %&gt;% \n  group_by(.model, MPE) %&gt;% \n  ggplot(aes(x = MPE, \n             xend = 0, \n             y = reorder(.model, desc(abs(MPE))), \n             yend=.model,\n             label=round(MPE, 2))) +\n  theme_minimal() +\n  geom_segment() +\n  geom_point() +\n  geom_text(nudge_x = 2, nudge_y = 0.2,\n            size = 3) +\n  labs(y = \"\", x = \"MPE\") -&gt; mpe.plot\n\n\n\n\nCode\nlibrary(ggpubr)\n\n\n\n\nCode\nggarrange(mpe.plot, rmse.plot,\n                    ncol = 2, nrow = 1) |&gt;\n  annotate_figure(top = text_grob(\"Forecast Accuracy Metrics\"))\n\n\n\n\n\n\n\nCode\nnet_gen.fc2.arima |&gt;\n  autoplot(net_gen.test2, level = NULL) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation\nARIMA Forecast\",\n       subtitle = \"Nov 2018 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\nnet_gen.fc2.arima |&gt;\n  autoplot(net_gen.test2) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation\nARIMA Forecast\",\n       subtitle = \"Nov 2018 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\nnet_gen.fc2 |&gt;\n  filter(.model == \"Regression\") |&gt;\n  autoplot(net_gen.test2) +\n  guides(colour = guide_legend(title = \"Forecast\")) +\n  labs(title = \"Net Conventional Hydroelectric Generation\nRegression Forecast\",\n       subtitle = \"Nov 2018 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()"
  },
  {
    "objectID": "ml_procurement.html",
    "href": "ml_procurement.html",
    "title": "Procurement Contracts",
    "section": "",
    "text": "I use ML techniques to predict and classify the value of the Procurement Contracts."
  },
  {
    "objectID": "ml_procurement.html#data",
    "href": "ml_procurement.html#data",
    "title": "Procurement Contracts",
    "section": "Data",
    "text": "Data\n\n\nCode\nProcurement &lt;- read.csv(\"Procurement.csv\", header=TRUE, sep=\";\")\nView(Procurement)"
  },
  {
    "objectID": "ml_procurement.html#plot",
    "href": "ml_procurement.html#plot",
    "title": "Procurement Contracts",
    "section": "Plot",
    "text": "Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`summarise()` has grouped output by 'cri_wb.ch'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\n\n\nWarning: Removed 13 rows containing non-finite values (stat_density).\n\n\n\n\n\nWe can observe some differences, for example between regions like Sub-Saharan Africa and Latin America.\n\n\nWarning: Removed 13 rows containing non-finite values (stat_density).\n\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\nAgain some differences are seen between for example Energy&Mining and Law, which makes sense, since Mining requires more costly equipment. The sector without a name seems to be an outlier almost.\n\n\nWarning: Removed 13 rows containing non-finite values (stat_summary).\n\n\n\n\n\nGenerally increasing trend of average value per contract."
  },
  {
    "objectID": "ml_procurement.html#regression",
    "href": "ml_procurement.html#regression",
    "title": "Procurement Contracts",
    "section": "Regression",
    "text": "Regression\nI run a simple regression with variables of interest to see if anything interesting happens\n\n\n\nCall:\nlm(formula = lca_contract_value ~ region + ca_type + ca_bids_all + \n    ca_procedure + singleb + corr_signp2 + corr_signp3 + corr_signp4 + \n    nrc + taxhav_fixed + cri_wb, data = Procurement2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.4957  -1.2318   0.0867   1.2818   8.4553 \n\nCoefficients: (1 not defined because of singularities)\n                                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      1.358e+01  3.544e-02 383.279  &lt; 2e-16 ***\nregionEAP                       -5.604e-02  1.851e-02  -3.028  0.00247 ** \nregionECA                       -2.158e-01  1.546e-02 -13.959  &lt; 2e-16 ***\nregionLCR                       -5.246e-01  1.536e-02 -34.153  &lt; 2e-16 ***\nregionMNA                        8.887e-02  2.102e-02   4.227 2.37e-05 ***\nregionSAR                       -5.903e-02  2.073e-02  -2.848  0.00441 ** \nca_typeConsultant Services      -1.443e+00  3.071e-02 -46.986  &lt; 2e-16 ***\nca_typeGoods                    -7.299e-01  1.279e-02 -57.051  &lt; 2e-16 ***\nca_bids_all                      6.982e-04  3.591e-04   1.945  0.05184 .  \nca_procedureconsultancy,quality -9.478e-01  2.372e-02 -39.965  &lt; 2e-16 ***\nca_procedureopen                -6.071e-01  3.322e-02 -18.277  &lt; 2e-16 ***\nca_procedurerestricted          -1.573e+00  3.941e-02 -39.906  &lt; 2e-16 ***\nca_proceduresingle source       -7.534e-01  1.824e-02 -41.302  &lt; 2e-16 ***\nsingleb                         -1.965e-01  1.232e-02 -15.949  &lt; 2e-16 ***\ncorr_signp2                     -2.161e-01  1.201e-02 -17.986  &lt; 2e-16 ***\ncorr_signp3                      2.122e-01  1.592e-02  13.331  &lt; 2e-16 ***\ncorr_signp4                     -5.635e-01  1.590e-02 -35.452  &lt; 2e-16 ***\nnrc                              4.125e-05  1.467e-06  28.117  &lt; 2e-16 ***\ntaxhav_fixedNO tax haven         8.181e-01  1.238e-02  66.078  &lt; 2e-16 ***\ntaxhav_fixedtax haven            8.470e-01  3.503e-02  24.176  &lt; 2e-16 ***\ncri_wb                                  NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.935 on 161432 degrees of freedom\n  (13 observations deleted due to missingness)\nMultiple R-squared:  0.1659,    Adjusted R-squared:  0.1658 \nF-statistic:  1690 on 19 and 161432 DF,  p-value: &lt; 2.2e-16\n\n\nAdjusted R-squared is about 0.17, whereas cri_wb is dropped because of singularity.\n\n\n[1] 604717.7\n\n\nRSS is quite big.\nThe variable for cri_wb was omitted due to singularities. Perhaps creating a dummy might change it.\nI am creating several dummies for categorical vars.\n\n\n [1] \"Public admin, Law\"    \"Health & social serv\" \"Education\"           \n [4] \"Water/sanit/fld prot\" \"Industry and trade\"   \"Transportation\"      \n [7] \"Finance\"              \"Agriculture\"          \"Energy & mining\"     \n[10] \"Info & communication\" \"(H)\"                  \"(H)Multisector\"      \n[13] \"\"                     \"(H)Priv Sector Dev\"  \n\n\nFor regression of the value per contract I am selecting the following variables: 1. Dummy variables for region, because there was a slight difference between the distributions of value by regions. 4. Procurement Sector, thinking some sectors tend to attract more valuable tender contracts. 2. Dummy for ca_type (Procurement Category) since the type of services can affect the value 3. Dummy for for n of bidders since higher n of bidders should indicate a higher price of the contract. 4. Dummy for contract award procedure associated with corruption. 5. Dummy for single bidder might indicate corrupted case, which can imply high value of the contract. 6. Period between when the contract was awarded and signed. 7. Number of contracts per country. 8. Whether the supplier is a domestic one or a tax haven. Foreign tax havens might indicate corruption and thus higher price. 9. The Corruption Index might be associated with the higher value of the contract.\n\n\n\nCall:\nlm(formula = lca_contract_value ~ ., data = Procurement3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.7818  -0.6101  -0.0397   0.5108   7.0744 \n\nCoefficients: (2 not defined because of singularities)\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                       9.252e+00  1.382e-01  66.933  &lt; 2e-16 ***\nca_contract_valuec200.000-        4.692e+00  7.916e-03 592.686  &lt; 2e-16 ***\nca_contract_valuec25.000-49.999   1.543e+00  1.066e-02 144.820  &lt; 2e-16 ***\nca_contract_valuec50.000-199.999  2.570e+00  8.370e-03 307.066  &lt; 2e-16 ***\nAFR                               5.303e-02  1.071e-02   4.951 7.40e-07 ***\nEAP                               1.428e-01  9.106e-03  15.681  &lt; 2e-16 ***\nECA                               6.290e-02  1.089e-02   5.778 7.57e-09 ***\nLCR                               1.050e-01  1.050e-02  10.000  &lt; 2e-16 ***\nMNA                               1.890e-01  1.336e-02  14.146  &lt; 2e-16 ***\nSAR                                      NA         NA      NA       NA    \nca_Consul.Service                -3.435e-01  1.593e-02 -21.562  &lt; 2e-16 ***\nca_Goods                         -2.349e-01  6.761e-03 -34.744  &lt; 2e-16 ***\nca_bids_all                       1.069e-03  1.850e-04   5.782 7.40e-09 ***\nca_proc_open                      2.515e-01  4.851e-02   5.184 2.17e-07 ***\nca_proc_quality                  -1.505e-01  1.232e-02 -12.215  &lt; 2e-16 ***\nca_proc_source                   -6.125e-02  9.609e-03  -6.374 1.85e-10 ***\nca_proc_restricted               -2.147e-01  2.046e-02 -10.492  &lt; 2e-16 ***\nsingleb                          -2.465e-01  4.567e-02  -5.398 6.74e-08 ***\ncorr_signp1                       3.224e-01  4.605e-02   7.001 2.55e-12 ***\ncorr_signp2                       4.846e-02  8.781e-03   5.519 3.42e-08 ***\ncorr_signp3                       2.146e-01  1.054e-02  20.355  &lt; 2e-16 ***\nnrc                               1.587e-05  7.593e-07  20.903  &lt; 2e-16 ***\ntaxhaven                          1.119e-01  4.103e-02   2.728 0.006376 ** \nnot.taxhaven                      2.798e-01  6.462e-03  43.304  &lt; 2e-16 ***\ncri_wb.d0                        -6.546e-01  1.366e-01  -4.791 1.66e-06 ***\ncri_wb.d25                       -5.129e-01  9.156e-02  -5.602 2.12e-08 ***\ncri_wb.d50                       -2.489e-01  4.666e-02  -5.334 9.61e-08 ***\ncri_wb.d75                               NA         NA      NA       NA    \nP                                -2.145e-01  1.278e-01  -1.678 0.093383 .  \nHealth                           -9.958e-02  1.279e-01  -0.779 0.436187    \nEdu                              -9.031e-02  1.280e-01  -0.706 0.480466    \nW                                 2.929e-02  1.280e-01   0.229 0.819010    \nI                                -1.220e-01  1.282e-01  -0.952 0.341306    \nTran                              3.261e-01  1.280e-01   2.549 0.010814 *  \nFin                              -9.101e-02  1.285e-01  -0.708 0.478748    \nA                                -4.242e-02  1.280e-01  -0.331 0.740364    \nEner                              3.031e-01  1.281e-01   2.366 0.017991 *  \nInfo                             -1.693e-01  1.296e-01  -1.306 0.191443    \nH                                -6.562e-02  1.771e-01  -0.371 0.710976    \nMulti                            -8.459e-01  2.276e-01  -3.717 0.000202 ***\nPriv                              5.614e-01  1.005e+00   0.559 0.576349    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9966 on 161413 degrees of freedom\nMultiple R-squared:  0.7789,    Adjusted R-squared:  0.7788 \nF-statistic: 1.496e+04 on 38 and 161413 DF,  p-value: &lt; 2.2e-16\n\n\nTwo variables were dropped because of singularity still. Most of the remaining vars have high statistical significance."
  },
  {
    "objectID": "projects.html#machine-learning",
    "href": "projects.html#machine-learning",
    "title": "Projects",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nProcurement Contracts\n\nUsed various Machine Learning techniques to predict Procurement Contracts value."
  },
  {
    "objectID": "cluster_ipl.html",
    "href": "cluster_ipl.html",
    "title": "Cluster Analysis IPL",
    "section": "",
    "text": "Here I explore the performance of the Indian Premier League batsmen in cricket. The dataset includes the popular performance metrics for 92 players: Average, Total Runs, Strike Rate, Hundreds, Fifties, Fours, Sixes, and salaries. In short, the higher the value of the metrics, the better a batsman is. The Average reflects the total number of runs scored divided by the number of innings in a season. The SR is calculated by dividing the number of runs scored by the number of balls faced and then multiplying by 100. It shows if a batsman scores runs at a faster pace. The rest of the metrics reflect the numbers of times a player scored, for example, fifty or more runs per inning. An inning is a team’s turn to bat.\nI am analyzing the correspondence of the players’ pay and their performance. I am going to look at SR and Avg. These two metrics should be of critical meaning since they signal the relative performance. In other words, if we compare two player’s SR (Strikes Rate) rather than absolute scores, we can observe their ability to score high (runs per 100 balls faced). This is more clear than absolute numbers of runs, since batsmen have to be called to face the ball, which can skew the final number. The same could be said about Avg."
  },
  {
    "objectID": "cluster_ipl.html#instroduction",
    "href": "cluster_ipl.html#instroduction",
    "title": "Cluster Analysis IPL",
    "section": "",
    "text": "Here I explore the performance of the Indian Premier League batsmen in cricket. The dataset includes the popular performance metrics for 92 players: Average, Total Runs, Strike Rate, Hundreds, Fifties, Fours, Sixes, and salaries. In short, the higher the value of the metrics, the better a batsman is. The Average reflects the total number of runs scored divided by the number of innings in a season. The SR is calculated by dividing the number of runs scored by the number of balls faced and then multiplying by 100. It shows if a batsman scores runs at a faster pace. The rest of the metrics reflect the numbers of times a player scored, for example, fifty or more runs per inning. An inning is a team’s turn to bat.\nI am analyzing the correspondence of the players’ pay and their performance. I am going to look at SR and Avg. These two metrics should be of critical meaning since they signal the relative performance. In other words, if we compare two player’s SR (Strikes Rate) rather than absolute scores, we can observe their ability to score high (runs per 100 balls faced). This is more clear than absolute numbers of runs, since batsmen have to be called to face the ball, which can skew the final number. The same could be said about Avg."
  },
  {
    "objectID": "cluster_ipl.html#data-exploration",
    "href": "cluster_ipl.html#data-exploration",
    "title": "Cluster Analysis IPL",
    "section": "Data Exploration",
    "text": "Data Exploration\n\n\nCode\nlibrary(readxl)\nCricket_Data &lt;- read_excel(\"The Indian Premier League Supplemental Data.xlsx\", \n    sheet = \"Sheet1\", col_types = c(\"numeric\", \n        \"skip\", \"text\", \"text\", \"numeric\", \n        \"numeric\", \"numeric\", \"numeric\", \n        \"numeric\", \"numeric\", \"numeric\", \n        \"numeric\"))\n\n\nNew names:\n• `` -&gt; `...1`\n\n\n\n\nCode\nlibrary(psych) #describe()\nlibrary(summarytools) #descr()\n\n\nHere is a quick preview of the dataset:\n\n\nCode\nhead(Cricket_Data)\n\n\n# A tibble: 6 × 11\n   ...1 Player       Team   Runs   Avg    SR Hundreds Fifties Fours Sixes Salary\n  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1 AB de Villi… Roya…   442  44.2 154          0       5    31    26  1.72 \n2     2 Ajinkya Rah… Raja…   393  32.8 138.         1       1    45     9  0.625\n3     3 Akshdeep Na… Roya…    61  12.2 107.         0       0     5     2  0.514\n4     4 Ambati Rayu… Chen…   282  23.5  93.1        0       1    20     7  0.344\n5     5 Andre Russe… Kolk…   510  56.7 205.         0       4    31    52  1.33 \n6     6 Axar Patel   Roya…   110  18.3 125          0       0    10     3  0.714\n\n\nHere we can see a table of descriptive statistics for the performance metrics:\n\n\nCode\ndescr(Cricket_Data[, -c(1,2,3)],  #without ...1, Player, Team\n      stats = c(\"mean\", \"sd\",\"med\", \"min\", \"max\"), \n      transpose = TRUE)\n\n\nDescriptive Statistics  \nCricket_Data  \nN: 70  \n\n                   Mean   Std.Dev   Median     Min      Max\n-------------- -------- --------- -------- ------- --------\n           Avg    28.58     14.89    27.07    3.60    83.20\n       Fifties     1.49      1.73     1.00    0.00     8.00\n         Fours    22.40     16.84    19.00    1.00    64.00\n      Hundreds     0.09      0.28     0.00    0.00     1.00\n          Runs   249.26    168.75   216.50   12.00   692.00\n        Salary     0.91      0.67     0.70    0.03     2.66\n         Sixes    10.51      9.57     8.00    0.00    52.00\n            SR   133.12     25.18   133.31   63.15   204.81\n\n\n\n\nCode\nlibrary(ggplot2)\n\n\n\nAttaching package: 'ggplot2'\n\n\nThe following objects are masked from 'package:psych':\n\n    %+%, alpha\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n✔ purrr   1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()   masks psych::%+%()\n✖ ggplot2::alpha() masks psych::alpha()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ tibble::view()   masks summarytools::view()\n\n\nHere I plot the Salary against the Avg.\n\n\nCode\nCricket_Data %&gt;% \n  ggplot(aes(x=Avg, y=Salary)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\nWe can see that generally a higher Avg is associated with higher salary.\nI now plot SR against the Salary.\n\n\nCode\nCricket_Data %&gt;% \n  ggplot(aes(x=SR, y=Salary)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\nHere we can observe almost no visual relationship between the two.\nI then build a simple correlation matrix to see which metrics are highly correlated with salary:\n\n\nCode\nCricket_Data[, -c(1,2,3)] %&gt;% \n  cor() %&gt;% \n  as.data.frame() %&gt;% \n  dplyr::select(Salary) |&gt;\n  arrange(desc(Salary))\n\n\n            Salary\nSalary   1.0000000\nRuns     0.3234302\nAvg      0.2919955\nFifties  0.2900521\nFours    0.2550323\nSixes    0.2468059\nHundreds 0.2331481\nSR       0.1055668\n\n\nWe observe a positive but weak correlation between Salary and most performance metrics. SR, however, has the lowest correlation.\nI generate boxplots for both metrics with outliers named, so we can see the distribution of the data and the names of the players who score the highest. The limitation would be that we do not see other players. This can be overcome by sorting the data by our metrics and see who ends up above the mean or another threshold.\n\n\nCode\nCricket_Data %&gt;% \n  ggplot(aes(x=\"\", y=Avg)) +\n  geom_boxplot() +\n    geom_jitter(color=\"black\", size=0.4, alpha=0.9) +\n    geom_text(data = subset(Cricket_Data, Avg&gt;60), aes(label = Player)) +\n    ggtitle(\"Boxplot for Avg\")\n\n\n\n\n\nWe can see that these two players have the highest Avg.\n\n\nCode\nCricket_Data %&gt;% \n  ggplot(aes(x=\"\", y=SR)) +\n  geom_boxplot() +\n    geom_jitter(color=\"black\", size=0.4, alpha=0.9) +\n    geom_text(data = subset(Cricket_Data, SR&gt;180), aes(label = Player))+\n    ggtitle(\"Boxplot for SR\")\n\n\n\n\n\nThe other two have the highest SR.\nI know plot Avg vs SR, to see the distribution of players according to these metrics.\n\n\nCode\nCricket_Data %&gt;% \n  ggplot(aes(x=SR, y=Avg)) +\n  geom_text(aes(label = Player), size = 2) +\n  labs(title = \"Plot of the Players' Avg vs SR\")\n\n\n\n\n\nHere we can identify the players with the highest combination of SR and Avg metrics. The closer the player is to the top right corner, the better performance he has, and reversely the closer he is to the bottom left, the worse their performance is. We can also observe that relatively few players have Avg above 40."
  },
  {
    "objectID": "cluster_ipl.html#clustering",
    "href": "cluster_ipl.html#clustering",
    "title": "Cluster Analysis IPL",
    "section": "Clustering",
    "text": "Clustering\nI then perform cluster analysis to discover any subgroups within the dataset. I use a clustering method called kmeans, which uses the Euclidean distances to subset our datapoints.\n\n\nCode\n#library(tidyverse)  # data manipulation\nlibrary(cluster)    # clustering algorithms\nlibrary(factoextra) # clustering algorithms & visualization (fviz...)\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n\nI remove the variables Team and “…1” in the data.\n\n\nCode\ndf = select(Cricket_Data, -Team, -\"...1\")\n\n\nI set the Players’ names as rownames to label the datapoints for better vizualization.\n\n\nCode\ndf2 = column_to_rownames(df, \"Player\")\n\n\nI check clustering without scaling the data first.\n\n\nCode\nset.seed(123)\n\nk4.2 &lt;- kmeans(df2, centers = 4, nstart = 25)\nfviz_cluster(k4.2, data = df2)\n\n\n\n\n\nAs we can see, unscaled data overlaps signficantly.\nI standardize the data for better clustering using scale() function.\n\n\nCode\ndf3 = as.data.frame(scale(df2))\n\n\nI check the scaling:\n\n\nCode\n#check scaling. Mean should be 0 and Std 1.\ndescr(df3, \n      stats = c(\"mean\", \"sd\",\"med\", \"min\", \"max\"), \n      transpose = TRUE)\n\n\nDescriptive Statistics  \ndf3  \nN: 70  \n\n                 Mean   Std.Dev   Median     Min    Max\n-------------- ------ --------- -------- ------- ------\n           Avg   0.00      1.00    -0.10   -1.68   3.67\n       Fifties   0.00      1.00    -0.28   -0.86   3.76\n         Fours   0.00      1.00    -0.20   -1.27   2.47\n      Hundreds   0.00      1.00    -0.30   -0.30   3.24\n          Runs   0.00      1.00    -0.19   -1.41   2.62\n        Salary   0.00      1.00    -0.31   -1.32   2.60\n         Sixes   0.00      1.00    -0.26   -1.10   4.33\n            SR   0.00      1.00     0.01   -2.78   2.85\n\n\n\n\nCode\nk2 &lt;- kmeans(df3, centers = 2, nstart = 25)\n\n\nI visualize initial kmeans clustering with 2 clusters.\n\n\nCode\nfviz_cluster(k2, data = df3)\n\n\n\n\n\nI plot Clusters by Teams to see if it produces any insight.\n\n\nCode\nfviz_cluster(object = list(data = df3, \n                           cluster = Cricket_Data$Team),\n             geom = \"point\",\n             show.clust.cent = FALSE)\n\n\n\n\n\nWe can see that Players cannot be meaningfully clustered by Teams. That is, the teams do not differ from each other based on the metrics presented.\nI perform Elbow method to identify optimal number of Clusters.\n\n\n\n\n\nI use factoextra functions for better visualization.\n\n\nCode\nset.seed(123)\n\nfviz_nbclust(df3, kmeans, method = \"wss\") +\n  geom_hline(yintercept = 190, \n             linetype = \"dashed\", \n             color = \"red\") +\n  geom_hline(yintercept = 230, \n             linetype = \"dashed\", \n             color = \"brown\")\n\n\n\n\n\nCode\n#should check if k=5 is good too\n\n\nThis chart shows that Total Within Sum of Squares drop rate decreases after 4 and even more so after 5 clusters. We can evaluate both cases and use Silhouette Method for further investigation.\n\n\nCode\nk4 &lt;- kmeans(df3, centers = 4, nstart = 25)\n\n\n\n\nCode\nfviz_cluster(k4, data = df3)\n\n\n\n\n\nClusters are almost not overlapping. Cluster 1 is quite big compared to others.\nHere are the cluster centers (means) and sizes of clusters.\n\n\nCode\nk4$centers\n\n\n        Runs        Avg          SR   Hundreds    Fifties      Fours      Sixes\n1 -0.8138651 -0.5593812 -0.32277994 -0.3039913 -0.7165009 -0.7826572 -0.6495237\n2  1.2370084  1.5176928  1.33963042 -0.3039913  1.0654740  0.5896937  2.2273208\n3  0.6760145  0.2087239  0.06149259 -0.3039913  0.6810246  0.8186736  0.3293470\n4  1.4157758  1.2012910  0.43562184  3.2425739  0.9693617  1.3713348  0.6253612\n       Salary\n1 -0.26225776\n2  1.06586416\n3 -0.05845941\n4  0.75599997\n\n\nCode\nk4$size\n\n\n[1] 37  6 21  6\n\n\nHere is the plot for Average Silhouette Method. ASM determines how well each object lies within its cluster. A high average silhouette width signals a good clustering.\n\n\nCode\nfviz_nbclust(df3, kmeans, method = \"silhouette\") +\n  geom_vline(xintercept = 5, \n             linetype = \"dashed\", \n             color = \"red\")\n\n\n\n\n\nCode\n#abline(v = 5, col = \"red\", lty = \"dashed\")\n#seems k=5 is the second highest \n\n\nAccording to the ASM plot, k=5 should be more optimal than k=4. I perform a new cluster analysis using 5 centers.\n\n\nCode\nk5 &lt;- kmeans(df3, centers = 5, nstart = 25)\nfviz_cluster(k5, data = df3)\n\n\n\n\n\nHere are the Cluster centers which we can interpret as means for each cluster (data is scaled).\n\n\nCode\nk5$centers\n\n\n        Runs        Avg          SR   Hundreds    Fifties      Fours      Sixes\n1  0.7470225  0.2122767 -0.04890149 -0.3039913  0.7518443  0.9510921  0.2651953\n2 -0.5874998 -0.0488647  0.61959027 -0.3039913 -0.6193189 -0.6313657 -0.2934097\n3  1.2370084  1.5176928  1.33963042 -0.3039913  1.0654740  0.5896937  2.2273208\n4  1.4157758  1.2012910  0.43562184  3.2425739  0.9693617  1.3713348  0.6253612\n5 -0.9146652 -0.8871118 -0.92070090 -0.3039913 -0.7257107 -0.8683501 -0.7803108\n        Salary\n1 -0.008588271\n2 -0.590968548\n3  1.065864161\n4  0.755999969\n5 -0.032797378\n\n\nI create a combined dataset with the cluster numbers as variables.\n\n\nCode\n#creating df with cluster indices as variable\nclusterdf = as.data.frame(k5$cluster)\n\n\n\n\nCode\n#Adding cluster indices and creating df4\ndf4 = df2 %&gt;% \n  bind_cols(clusterdf) %&gt;% \n  bind_cols(Cricket_Data$Team) %&gt;% \n  rename(cluster = `k5$cluster`) %&gt;% \n  rename(Team = \"...10\")\n\n\nNew names:\n• `` -&gt; `...10`\n\n\n\n\nCode\n#structure of df4\nstr(df4)\n\n\n'data.frame':   70 obs. of  10 variables:\n $ Runs    : num  442 393 61 282 510 110 123 12 490 405 ...\n $ Avg     : num  44.2 32.8 12.2 23.5 56.7 ...\n $ SR      : num  154 137.9 107 93.1 204.8 ...\n $ Hundreds: num  0 1 0 0 0 0 0 0 0 0 ...\n $ Fifties : num  5 1 0 1 4 0 0 0 4 4 ...\n $ Fours   : num  31 45 5 20 31 10 8 1 45 41 ...\n $ Sixes   : num  26 9 2 7 52 3 4 0 34 22 ...\n $ Salary  : num  1.719 0.625 0.514 0.344 1.328 ...\n $ cluster : int  3 4 5 5 3 5 5 5 3 1 ...\n $ Team    : chr  \"Royal Challengers Bangalore\" \"Rajasthan Royals\" \"Royal Challengers Bangalore\" \"Chennai Super Kings\" ...\n\n\nHere are the descriptive statistics for each cluster (data is unscaled).\n\n\nCode\nstby(\n  data = df4,\n  INDICES = df4$cluster, # by clusters\n  FUN = descr, # descriptive statistics from summarytools\n  stats = c(\"mean\", \"sd\",\"med\", \"min\", \"max\"), \n  transpose = TRUE\n)\n\n\nNon-numerical variable(s) ignored: Team\n\n\nDescriptive Statistics  \ndf4  \nGroup: cluster = 1  \nN: 19  \n\n                   Mean   Std.Dev   Median      Min      Max\n-------------- -------- --------- -------- -------- --------\n           Avg    31.74      5.73    31.62    22.06    43.00\n       cluster     1.00      0.00     1.00     1.00     1.00\n       Fifties     2.79      0.92     3.00     1.00     5.00\n         Fours    38.42     11.04    41.00    21.00    64.00\n      Hundreds     0.00      0.00     0.00     0.00     0.00\n          Runs   375.32     74.26   373.00   253.00   529.00\n        Salary     0.91      0.66     0.69     0.16     2.34\n         Sixes    13.05      5.56    11.00     4.00    25.00\n            SR   131.89     10.48   130.86   115.10   151.70\n\nGroup: cluster = 2  \nN: 17  \n\n                   Mean   Std.Dev   Median      Min      Max\n-------------- -------- --------- -------- -------- --------\n           Avg    27.85     10.89    26.62    14.60    52.75\n       cluster     2.00      0.00     2.00     2.00     2.00\n       Fifties     0.41      0.62     0.00     0.00     2.00\n         Fours    11.76      5.17    13.00     2.00    19.00\n      Hundreds     0.00      0.00     0.00     0.00     0.00\n          Runs   150.12     68.37   160.00    63.00   279.00\n        Salary     0.52      0.52     0.29     0.03     1.95\n         Sixes     7.71      5.76     7.00     1.00    22.00\n            SR   148.72     17.69   151.31   125.87   175.00\n\nGroup: cluster = 3  \nN: 6  \n\n                   Mean   Std.Dev   Median      Min      Max\n-------------- -------- --------- -------- -------- --------\n           Avg    51.18     16.97    44.43    37.53    83.20\n       cluster     3.00      0.00     3.00     3.00     3.00\n       Fifties     3.33      1.37     3.50     1.00     5.00\n         Fours    32.33      7.89    31.00    22.00    45.00\n      Hundreds     0.00      0.00     0.00     0.00     0.00\n          Runs   458.00     44.24   465.00   402.00   510.00\n        Salary     1.63      0.76     1.72     0.31     2.34\n         Sixes    31.83     10.53    28.00    23.00    52.00\n            SR   166.85     26.24   158.33   134.62   204.81\n\nGroup: cluster = 4  \nN: 6  \n\n                   Mean   Std.Dev   Median      Min      Max\n-------------- -------- --------- -------- -------- --------\n           Avg    46.47     15.31    44.05    32.75    69.20\n       cluster     4.00      0.00     4.00     4.00     4.00\n       Fifties     3.17      3.13     2.00     0.00     8.00\n         Fours    45.50      9.57    47.00    28.00    57.00\n      Hundreds     1.00      0.00     1.00     1.00     1.00\n          Runs   488.17    130.70   454.50   342.00   692.00\n        Salary     1.42      0.87     1.48     0.31     2.66\n         Sixes    16.50      5.92    15.50     9.00    25.00\n            SR   144.09      7.94   142.66   135.38   157.24\n\nGroup: cluster = 5  \nN: 22  \n\n                   Mean   Std.Dev   Median     Min      Max\n-------------- -------- --------- -------- ------- --------\n           Avg    15.37      7.49    16.31    3.60    35.33\n       cluster     5.00      0.00     5.00    5.00     5.00\n       Fifties     0.23      0.43     0.00    0.00     1.00\n         Fours     7.77      6.51     5.50    1.00    20.00\n      Hundreds     0.00      0.00     0.00    0.00     0.00\n          Runs    94.91     68.50    85.00   12.00   282.00\n        Salary     0.89      0.48     0.81    0.07     1.95\n         Sixes     3.05      1.91     3.00    0.00     7.00\n            SR   109.93     21.24   116.50   63.15   150.00\n\n\nWe can observe the means of the produced clusters and check if the high average performance corresponds with higher or lower average salaries. This would help to identify clusters which contain underpaid or overpaid players.\nIn the given tables, we can observe that Cluster 3 has the top average salary of 1.63. The second highest earning cluster is Cluster 4. We can also observe that these clusters contain the highest performing players since their average performing metrics are the highest among the cluster. The exceptions are SR, where Cluster 2 is performing better than Cluster 4, taking second place, and Fours, where Cluster 1 performs better than Cluster 3, coming second. Otherwise, we can see that they are the top performers and thus the top earners. Their performance in the key metrics is, on average, in the top 3 (even top 2, mostly) and have the highest average salaries. Even their minimums in Avg and SR are as high as average values of most other clusters, if not higher. The 6 players in Cluster 4 are the only ones scoring Hundreds, separating them from the 6 players of Cluster 3. We can therefore identify them as 12 top performers and top earners. Therefore, we can call the Cluster 3 “Top Performers” and Cluster 4 “Hundred Scorers”.\nThe other cluster is Cluster 1. We can call them as the “Average Player”. They are the second most populous cluster and perform average. Their metrics is in the middle between most of the Clusters, except SR (they are 3rd) and Fours (2nd). We can also identify them as average since they get the average pay. We can see that in the scaled cluster centers, where the closer the value is to 0, the closer it is to the mean.\nThe remaining two clusters are 2 and 5. Interestingly, we can see that Cluster 2 overperforms Cluster 5 by all metrics but gets the lowest salary on average (0.52 vs 0.89). Not only that, Cluster 5, on average, performs worse than any other cluster but the salary is very close to the pay of Cluster “Average” (0.89 and 0.91). It is also worth noticing that Cluster 2 performs the 2nd best by SR, one of the key metrics Mitra is interested in. All this indicates that Cluster 2 is underpaid. Thus, I call cluster 5 “Overpaid” and Cluster 2 “Underpaid”."
  },
  {
    "objectID": "cluster_ipl.html#parallel-coordinate-plot-for-cluster-interpretation",
    "href": "cluster_ipl.html#parallel-coordinate-plot-for-cluster-interpretation",
    "title": "Cluster Analysis IPL",
    "section": "Parallel Coordinate Plot for Cluster interpretation",
    "text": "Parallel Coordinate Plot for Cluster interpretation\n\n\nCode\nk5centers = k5$centers %&gt;% \n  as.data.frame() %&gt;% \n  rownames_to_column(var = \"cluster\")\n\n\n\n\nCode\nlibrary(GGally) #ggparcoord\nlibrary(plotly)\nlibrary(MASS) #parcoord\n\n\nTo better analyze and compare the clusters I use Parallel Coordinate Plot to see the Cluster centers (means) all at the same time. In this plot the higher values of the metrics are, the better. It means that high metrics values should be followed by high salary.\n\n\nCode\nggparcoord(data = k5centers,\n           columns = c(2:9),\n           groupColumn = \"cluster\",\n           scale = \"globalminmax\") +\n  geom_line(size = 1) +\n  scale_color_discrete(name = \"Cluster\",\n                       labels = c(\"Average Player\", \n                                  \"Underpaid\", \n                                  \"Top Performers\", \n                                  \"Hundred Scorers\",\n                                  \"Overpaid\")) +\n  labs(title = \"Parallel Coordinate Plot of Cluster Centers by Variables\")\n\n\n\n\n\nHundreds Variable skews the view a little, so I remove it from the plot but keep in mind that only “Hundred Scorers” cluster (Blue) has players who scored Hundreds.\n\n\nCode\nggparcoord(data = dplyr::select(k5centers, -Hundreds), #MASS masks select\n           columns = c(2:8),\n           groupColumn = \"cluster\",\n           scale = \"globalminmax\") +\n  geom_line(size = 1) +\n  scale_color_discrete(name = \"Cluster\",\n                       labels = c(\"Average Player\", \n                                  \"Underpaid\", \n                                  \"Top Performers\", \n                                  \"Hundred Scorers\",\n                                  \"Overpaid\")) +\n  labs(title = \"Parallel Coordinate Plot of Cluster Centers by Variables\")\n\n\n\n\n\nWe can observe that Clusters Top Performers and Hundred Scorers perform generally well and earn higher than average salaries. Cluster Average Player is somewhat in the middle and earns average salary. Most interestingly, Clusters Underpaid and Overpaid on average perform the worst but the Underpaid plays better than the Overpaid by all metrics and even has the second highest SR yet earns the least. This happens while the Overpaid earns as much as Average Player. It should also be noticed that the scaled average salaries of the Clusters Average Player, Top Performers, and Hundred Scorers are in the vicinity of the scaled Avg and SR, whereas the Clusters Underpaid and Overpaid are not. This might again indicate that players are underpaid and overpaid in the respective clusters, on average.\nI also show the cluster centers separately side by side.\n\n\nCode\nggparcoord(data = k5centers,\n           columns = c(2:9),\n           groupColumn = \"cluster\",\n           scale = \"globalminmax\") +\n  facet_wrap(~ cluster) +\n  labs(title = \"Cluster Centers by Variables\")\n\n\n\n\n\n\n\nCode\nggparcoord(data = dplyr::select(k5centers, -Hundreds),\n           columns = c(2:8),\n           groupColumn = \"cluster\",\n           scale = \"globalminmax\") +\n  facet_wrap(~ cluster) +\n  labs(title = \"Cluster Centers by Variables without Hundreds\")\n\n\n\n\n\n\n\nCode\nk4centers = k4$centers %&gt;% \n  as.data.frame() %&gt;% \n  rownames_to_column(var = \"cluster\")\n\n\nI create Parallel Coordinate Plot to identify the differences when we use 4 clusters instead of 5.\n\n\nCode\nggparcoord(data = dplyr::select(k4centers, -Hundreds),\n           columns = c(2:8),\n           groupColumn = \"cluster\",\n           scale = \"globalminmax\") +\n  geom_line(size = 1) +\n  labs(title = \"Parallel Coordinate Plot of Cluster Centers by Variables\")\n\n\n\n\n\nHere we can observe that Cluster Analysis with only 4 centers merges two worst performing Clusters together, so we do not see underpaid players. I, therefore, suggest sticking with 5 clusters.\nThese are average values, so we might want to look at the individual data.\n\n\nCode\ndf5 = df3 %&gt;% \n  bind_cols(clusterdf) %&gt;% \n  rename(cluster = `k5$cluster`)\n\n\n\n\nCode\ndplyr::select(df5, -Hundreds) %&gt;% #MASS masks select\n  mutate_at(vars(\"cluster\"), as.factor) %&gt;% #needs to factor cluster\n  ggparcoord(columns = c(1:7),\n           groupColumn = \"cluster\",\n           scale = \"globalminmax\") +\n  geom_line(size = 0.5) +\n  scale_color_manual(values = c(\"red\", \"green\", \"blue\", \"orange\", \"black\")) +\n  labs(title = \"Parallel Coordinate Plot of Cluster Centers by Variables\")\n\n\n\n\n\n\n\nCode\ndplyr::select(df5, -Hundreds) %&gt;% #MASS masks select\n  mutate_at(vars(\"cluster\"), as.factor) %&gt;% \n  ggparcoord(columns = c(1:7),\n           groupColumn = \"cluster\",\n           scale = \"globalminmax\") +\n  geom_line(size = 0.5) +\n  scale_color_manual(values = c(\"red\", \"green\", \"blue\", \"orange\", \"black\")) +\n  facet_wrap(~ cluster) +\n  labs(title = \"Parallel Coordinate Plot of Cluster Centers by Variables\")\n\n\n\n\n\nThe lines intertwine in some places but generally stay within its cluster. Most intertwining happens in the Salary. There is also some between Clusters 1 and 2 in Avg and SR metrics.\n\n\nCode\nCricket_Data %&gt;% \n  bind_cols(as.factor(clusterdf$`k5$cluster`)) %&gt;% \n  rename(cluster = ...12) %&gt;% #`k5$cluster` is renamed into ...12\n  ggplot(aes(x=SR, y=Avg)) +\n  geom_text(aes(label = Player, \n                color = cluster), \n            size = 2) +\n  scale_color_discrete(name = \"Cluster\",\n                       labels = c(\"Average Player\", \n                                  \"Underpaid\", \n                                  \"Top Performers\", \n                                  \"Hundred Scorers\",\n                                  \"Overpaid\")) +\n  guides(color = guide_legend(override.aes = list(size = 5))) + #size=5 overrides the size of colored letters in legend\n  ggtitle(\"Plot of the Players' Avg vs SR clustered\")\n\n\nNew names:\n• `` -&gt; `...12`\n\n\n\n\n\nHere we can also observe separation of the key metrics by clusters and that Cluster Underpaid is closer to Average Player than Overpaid Cluster. In other words, it has higher SR and Avg than Overpaid clusters, but their salary does not reflect that."
  },
  {
    "objectID": "cluster_ipl.html#conclusion",
    "href": "cluster_ipl.html#conclusion",
    "title": "Cluster Analysis IPL",
    "section": "Conclusion",
    "text": "Conclusion\nI suggest reevaluating the pay of the players from Cluster 2. The Teams might also reconsider the salaries of the players from Cluster 5 who perform the worst but get paid the average rate. The cluster centers showed the average performance of the players from the same subgroups but a more individual evaluation should take place before the final decision."
  },
  {
    "objectID": "NNETAR.html",
    "href": "NNETAR.html",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US using NNETAR",
    "section": "",
    "text": "This is a part 2 to the Hydropower Project I did before."
  },
  {
    "objectID": "NNETAR.html#data-preparation",
    "href": "NNETAR.html#data-preparation",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US using NNETAR",
    "section": "Data Preparation",
    "text": "Data Preparation\nI am importing my data on the total number of Conventional Hydropower Plants in the US and Monthly Precipitation.\n\n\nCode\nlibrary(readxl)\nplants &lt;- read_excel(\"EHA_Monthly_Net_Generation.xlsx\", \n    sheet = \"MonthlyNet\")\n\n\n\n\nCode\nlibrary(fpp3)\n\n\n── Attaching packages ────────────────────────────────────────────── fpp3 0.5 ──\n\n\n✔ tibble      3.1.8     ✔ tsibble     1.1.3\n✔ dplyr       1.1.0     ✔ tsibbledata 0.4.1\n✔ tidyr       1.2.0     ✔ feasts      0.3.1\n✔ lubridate   1.8.0     ✔ fable       0.3.3\n✔ ggplot2     3.3.5     ✔ fabletools  0.3.3\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\n\nCode\nplants %&gt;% \n  select(Year, Month, Net_Generation_MWh, EHA_PtID) -&gt; plants\n\n\n\n\nCode\nhead(plants)\n\n\n# A tibble: 6 × 4\n   Year Month Net_Generation_MWh EHA_PtID  \n  &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;     \n1  2003 APR                    0 hc0115_p01\n2  2003 AUG                    0 hc0115_p01\n3  2003 DEC                    0 hc0115_p01\n4  2003 FEB                    0 hc0115_p01\n5  2003 JAN                    0 hc0115_p01\n6  2003 JUL                  122 hc0115_p01\n\n\n\n\nCode\nlength(unique(plants$EHA_PtID))\n\n\n[1] 1519\n\n\n\n\nCode\nplants %&gt;%                              \n  group_by(Year, Month) %&gt;%\n  summarise(Count = n_distinct(EHA_PtID)) %&gt;% \n  mutate(Date = yearmonth(paste(Year, Month))) %&gt;% \n  arrange(Date) |&gt;\n  as_tsibble(index = Date) -&gt; plants2\n\n\n`summarise()` has grouped output by 'Year'. You can override using the\n`.groups` argument.\n\n\nCode\nplants2\n\n\n# A tsibble: 240 x 4 [1M]\n# Groups:    Year [20]\n    Year Month Count     Date\n   &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;    &lt;mth&gt;\n 1  2003 JAN    1386 2003 Jan\n 2  2003 FEB    1386 2003 Feb\n 3  2003 MAR    1386 2003 Mar\n 4  2003 APR    1386 2003 Apr\n 5  2003 MAY    1386 2003 May\n 6  2003 JUN    1386 2003 Jun\n 7  2003 JUL    1386 2003 Jul\n 8  2003 AUG    1386 2003 Aug\n 9  2003 SEP    1386 2003 Sep\n10  2003 OCT    1386 2003 Oct\n# … with 230 more rows\n\n\n\n\nCode\nplants2 |&gt;\n  autoplot(Count)\n\n\n\n\n\n\n\nCode\nlibrary(readr)\nprcpt &lt;- read_csv(\"Contiguous_us_Precipitation_data.csv\", \n    skip = 4)\n\n\nRows: 283 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): Date, Value, Anomaly\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nprcpt |&gt;\n  mutate(date = seq(as.Date(\"2000-01-01\"),\n                    as.Date(\"2023-07-01\"),\n                    by = \"month\"),\n         Date = yearmonth(date)) |&gt;\n  as_tsibble(index = Date) |&gt;\n  select(Date, Value) -&gt; prcpt2\nprcpt2\n\n\n# A tsibble: 283 x 2 [1M]\n       Date Value\n      &lt;mth&gt; &lt;dbl&gt;\n 1 2000 Jan  2.14\n 2 2000 Feb  2.12\n 3 2000 Mar  2.44\n 4 2000 Apr  2.32\n 5 2000 May  2.66\n 6 2000 Jun  3.45\n 7 2000 Jul  2.3 \n 8 2000 Aug  1.91\n 9 2000 Sep  2.18\n10 2000 Oct  2.26\n# … with 273 more rows\n\n\n\n\nCode\nprcpt2 |&gt;\n  autoplot() +\n  labs(title = \"Contiguous U.S. Precipitation\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Inches\",\n       x = \"Date\",\n       caption = \"Source: National Centers for Environmental Information\") +\n  theme_minimal()\n\n\nPlot variable not specified, automatically selected `.vars = Value`\n\n\n\n\n\n\n\nCode\nplants3 &lt;- read_csv(\"RectifHyd_v1.0.csv\", \n    skip = 27)\n\n\nRows: 324024 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): plant, state, month, EIA_obs_freq, RectifHyd_method, recommended_data\ndbl (6): EIA_ID, year, EIA_fraction, EIA_MWh, RectifHyd_fraction, RectifHyd_MWh\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nplants3 %&gt;%    \n  select(EIA_ID, plant, year, month) %&gt;% \n  group_by(year, month) %&gt;%\n  summarise(Count = n_distinct(plant), .groups = \"drop\") %&gt;% \n  mutate(Date = yearmonth(paste(year, month))) %&gt;% \n  arrange(Date) |&gt;\n  as_tsibble(index = Date) -&gt; plants3\nplants3\n\n\n# A tsibble: 240 x 4 [1M]\n    year month Count     Date\n   &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;    &lt;mth&gt;\n 1  2001 Jan    1337 2001 Jan\n 2  2001 Feb    1337 2001 Feb\n 3  2001 Mar    1337 2001 Mar\n 4  2001 Apr    1337 2001 Apr\n 5  2001 May    1337 2001 May\n 6  2001 Jun    1337 2001 Jun\n 7  2001 Jul    1337 2001 Jul\n 8  2001 Aug    1337 2001 Aug\n 9  2001 Sep    1337 2001 Sep\n10  2001 Oct    1337 2001 Oct\n# … with 230 more rows\n\n\n\n\nCode\nwhich(plants3$year==2003)[1]\n\n\n[1] 25\n\n\n\n\nCode\nplants3[1:24,] |&gt;\n  bind_rows(plants2) |&gt;\n  select(Count, Date) -&gt; plants_t\nplants_t\n\n\n# A tsibble: 264 x 2 [1M]\n   Count     Date\n   &lt;int&gt;    &lt;mth&gt;\n 1  1337 2001 Jan\n 2  1337 2001 Feb\n 3  1337 2001 Mar\n 4  1337 2001 Apr\n 5  1337 2001 May\n 6  1337 2001 Jun\n 7  1337 2001 Jul\n 8  1337 2001 Aug\n 9  1337 2001 Sep\n10  1337 2001 Oct\n# … with 254 more rows\n\n\n\n\nCode\nplants_t |&gt;\n  autoplot() +\n  labs(title = \"The Total Number of Operational Conventional\nHydropower Plants\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Plants\",\n       x = \"Date\",\n       caption = \"Source: Existing Hydropower Assets (EHA) Net\nGeneration Plant Database, 2003-2022; RectifHyd\") +\n  theme_minimal()\n\n\nPlot variable not specified, automatically selected `.vars = Count`\n\n\n\n\n\n\n\nCode\nhydro &lt;- read_csv(\"Net_generation_United_States_all_sectors_monthly.csv\", \n    skip = 4)\n\n\nRows: 268 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Month\ndbl (1): conventional hydroelectric thousand megawatthours\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nI am importing the data on Net Energy Generation.\n\n\nCode\nhydro = hydro %&gt;% \n  rename(MWH = \"conventional hydroelectric thousand megawatthours\")\n\n\n\n\nCode\nhydro=hydro[order(nrow(hydro):1),]\n\n\n\n\nCode\nhydro = hydro %&gt;% \n  mutate(Date = yearmonth(Month)) |&gt;\n  as_tsibble(index = Date)\n\n\n\n\nCode\nhydro %&gt;% \n  autoplot(MWH) +\n  labs(title = \"Net Conventional Hydroelectric Power Generation\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\nhydro |&gt;\n  left_join(plants_t) |&gt;\n  select(Date, MWH, Count) |&gt;\n  left_join(prcpt2) |&gt;\n  mutate(Count = ifelse(is.na(Count),\n                        1351,\n                        Count)) |&gt; #inputting missing 4 obs with the last ob\n  rename(Precipitation = Value) -&gt; hydro_comp\n\n\nJoining with `by = join_by(Date)`\nJoining with `by = join_by(Date)`\n\n\nCode\nhead(hydro_comp)\n\n\n# A tsibble: 6 x 4 [1M]\n      Date    MWH Count Precipitation\n     &lt;mth&gt;  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 2001 Jan 18852.  1337          1.85\n2 2001 Feb 17473.  1337          2.21\n3 2001 Mar 20477.  1337          2.57\n4 2001 Apr 18013.  1337          2.1 \n5 2001 May 19176.  1337          2.78\n6 2001 Jun 20728.  1337          3.12\n\n\nI am combining all the data into one tsibble.\n\n\nCode\nhydro_comp |&gt;\n  pivot_longer(c(MWH, Count, Precipitation)) |&gt;\n  ggplot(aes(x = Date, y = value)) +\n  geom_line() +\n  facet_grid(name ~ ., scales = \"free_y\") + ylab(\"\")\n\n\n\n\n\n\n\nCode\nhydro |&gt; gg_tsdisplay(MWH,\n                     plot_type='partial', lag_max = 24)\n\n\n\n\n\n\n\nCode\nhydro |&gt;\n  model(stl = STL(MWH)) |&gt;\n  components() |&gt; \n  gg_subseries(season_year) +\n  theme(axis.text.x = element_text(size = 5))\n\n\n\n\n\n\n\nCode\nhydro |&gt;\n  model(stl = STL(MWH)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\nWarning: Ignoring unknown parameters: linewidth\n\n\n\n\n\nSTL decomposition shows high seasonality of the data."
  },
  {
    "objectID": "NNETAR.html#preliminary-model-estimation",
    "href": "NNETAR.html#preliminary-model-estimation",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US using NNETAR",
    "section": "Preliminary Model Estimation",
    "text": "Preliminary Model Estimation\n\n\nCode\ntotal_obs.hydro = dim(hydro_comp)[1] #puts n of obs into total_obs\ntrain_obs = total_obs.hydro * 0.8\ntest_obs = total_obs.hydro - train_obs\nhydro_train = head(hydro_comp, train_obs)\nhydro_test = tail(hydro_comp, test_obs)\n\n\n\n\nCode\nhydro_train |&gt;\n  pivot_longer(c(MWH, Count, Precipitation)) |&gt;\n  ggplot(aes(x = Date, y = value)) +\n  geom_line() +\n  facet_grid(name ~ ., scales = \"free_y\") + ylab(\"\")\n\n\n\n\n\n\n\nCode\nnn_model = hydro_train |&gt;\n  model(nnetar = NNETAR(MWH ~ Count + Precipitation),\n        Arima.reg = ARIMA(MWH ~ Count + Precipitation))\n\n\n\n\nCode\nnn_model\n\n\n# A mable: 1 x 2\n             nnetar                              Arima.reg\n            &lt;model&gt;                                &lt;model&gt;\n1 &lt;NNAR(1,1,2)[12]&gt; &lt;LM w/ ARIMA(1,0,0)(1,1,0)[12] errors&gt;\n\n\nA mable: 1 x 2 nnetar Arima.reg   1 &lt;NNAR(1,1,2)[12]&gt; &lt;LM w/ ARIMA(1,0,0)(1,1,0)[12] errors&gt;\n\n\nCode\nhydro_train[214,\"Count\"]\n\n\n# A tibble: 1 × 1\n  Count\n  &lt;dbl&gt;\n1  1440\n\n\nCode\nmean_prcpt = mean(hydro_train$Precipitation)\n\nhydro_train2 = hydro_train[-214,]\n\n\n\n\nCode\nf_scenarios_hydro &lt;- scenarios(\n  Mean = new_data(hydro_train, 54) |&gt;\n    mutate(Count = 1440,\n           Precipitation = mean_prcpt))\n\n\n\n\nCode\nstart_time = Sys.time()\n\nnn_fit = nn_model |&gt;\n  forecast(new_data = f_scenarios_hydro)\n\nprint(Sys.time() - start_time)\n\n\nTime difference of 3.74252 mins\n\n\n\n\nCode\nhydro_train |&gt;\n  autoplot(MWH) +\n  autolayer(nn_fit) +\n  labs(title = \"Net Conventional Hydroelectric Power Generation\nNN Forecast\",\n       subtitle = \"Jan 2001 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\nhydro_test |&gt;\n  autoplot(MWH) +\n  autolayer(nn_fit) +\n  geom_line(aes(y = MWH)) +\n  labs(title = \"Net Conventional Hydroelectric Power Generation\nNN Forecast\",\n       subtitle = \"Nov 2018 - Apr 2023, Monthly\",\n       y = \"Thousand Megawatthours\",\n       x = \"Date\",\n       caption = \"Source: U.S. Energy Information Administration\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\nnn_fit |&gt;\n  accuracy(hydro_comp) |&gt;\n  select(.model, RMSE, MPE)\n\n\n# A tibble: 2 × 3\n  .model     RMSE    MPE\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 Arima.reg 3688. -11.4 \n2 nnetar    2929.  -2.86\n\n\nNeural Networks predict better."
  },
  {
    "objectID": "NNETAR.html#comparing-nnetar-arima-and-ets",
    "href": "NNETAR.html#comparing-nnetar-arima-and-ets",
    "title": "Forecasting Conventional Hydroelectric Power Generation in the US using NNETAR",
    "section": "Comparing NNETAR, ARIMA, and ETS",
    "text": "Comparing NNETAR, ARIMA, and ETS\nFor convenience I am using only the last five years of observation.\n\n\nCode\nhydro_train2 = hydro_comp[208:256,]\nhydro_test2 = tail(hydro_comp, 12)\nhydro_comp2 = hydro_comp[208:268,]\n\n\n\n\nCode\nlagged_arima &lt;- hydro_train2 |&gt;\n  # Restrict data so models use same fitting period\n  mutate(MWH = c(NA, NA, NA, NA, NA, MWH[6:49])) |&gt;\n  model(\n    lag1 = ARIMA(MWH ~ pdq(d = 0)\n                 + lag(Precipitation)\n                 + lag(Count)),\n    lag2 = ARIMA(MWH ~ pdq(d = 0) + lag(Precipitation) +\n                 lag(Precipitation, 2) + lag(Count) + lag(Count, 2)),\n    lag3 = ARIMA(MWH ~ pdq(d = 0) + lag(Precipitation) +\n                 lag(Precipitation, 2) + lag(Precipitation, 3) + lag(Count) + lag(Count, 2) + lag(Count, 3)),\n    lag4 = ARIMA(MWH ~ pdq(d = 0) + lag(Precipitation) +\n                 lag(Precipitation, 2) + lag(Precipitation, 3) + lag(Precipitation, 4) + lag(Count) + lag(Count, 2) + lag(Count, 3) + lag(Count, 4)),\n    lag5 = ARIMA(MWH ~ pdq(d = 0) + lag(Precipitation) +\n                 lag(Precipitation, 2) + lag(Precipitation, 3) + lag(Precipitation, 4) + lag(Precipitation, 5) + lag(Count) + lag(Count, 2) + lag(Count, 3) + lag(Count, 4) + lag(Count, 5))\n  )\n\nglance(lagged_arima)\n\n\n# A tibble: 5 × 8\n  .model   sigma2 log_lik   AIC  AICc   BIC ar_roots  ma_roots \n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;    &lt;list&gt;   \n1 lag1   3085088.   -286.  579.  581.  586. &lt;cpl [0]&gt; &lt;cpl [1]&gt;\n2 lag2   3207038.   -285.  583.  585.  592. &lt;cpl [0]&gt; &lt;cpl [1]&gt;\n3 lag3   3136314.   -284.  584.  589.  597. &lt;cpl [0]&gt; &lt;cpl [1]&gt;\n4 lag4   2900213.   -282.  583.  592.  599. &lt;cpl [0]&gt; &lt;cpl [1]&gt;\n5 lag5   2973819.   -281.  585.  598.  605. &lt;cpl [0]&gt; &lt;cpl [1]&gt;\n\n\nBased on AICc, I am choosing lag1.\n\n\nCode\nnn_model2 = hydro_train2 |&gt;\n  model(nnetar2 = NNETAR(MWH ~ Count + Precipitation),\n        Arima.reg2 = ARIMA(MWH ~ lag(Count) + lag(Precipitation),\n                           stepwise = FALSE,\n                           approx = FALSE),\n        ETS = ETS(MWH))\n\n\n\n\nCode\nlagged_count = hydro_train2$Count[38:49]\nlagged_prcpt = hydro_train2$Precipitation[38:49]\n#I am creating a list of the last 12 values from the hydro_train2.\nmean_count = rep(mean(mean(hydro_train2$Count)), times=12)\nmean_prcpt = rep(mean(mean(hydro_train2$Precipitation)), times=12)\n#I am creating a list of the 12 repeating values equaling the average observation from the training set.\n\nfuture_scenarios &lt;- scenarios(\n  Lagged = new_data(hydro_train2, 12) |&gt;\n    mutate(Count=lagged_count,\n           Precipitation = lagged_prcpt),\n  Mean = new_data(hydro_train2, 12) |&gt;\n    mutate(Count = mean_count,\n           Precipitation = mean_prcpt),\n  names_to = \"Forecast Scenarios\")\n#By putting lagged_vars, I am using the last 12 values of hydro_train2 as the values in the future_scenarios. This way I am creating new values lagged at 12: April 2023 will equal April 2022, etc. Thus, I can use them as the predictors for my forecast. The same is with the Mean values.\n\n\n\n\nCode\nstart_time = Sys.time()\n\nnn_fit2 = nn_model2 |&gt;\n  forecast(new_data = future_scenarios)\n\nprint(Sys.time() - start_time)\n\n\nTime difference of 2.633055 mins\n\n\n\n\nCode\nhydro_test2 = hydro_test2[,c(\"Date\",\"MWH\")]\n\n\n\n\nCode\nhead(hilo(nn_fit2))\n\n\n# A tibble: 6 × 9\n  `Forecast Scenarios` .model      Date          MWH  .mean Count Precipitation\n  &lt;chr&gt;                &lt;chr&gt;      &lt;mth&gt;       &lt;dist&gt;  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 Lagged               nnetar2 2022 May sample[5000] 21545.  1276          2.98\n2 Lagged               nnetar2 2022 Jun sample[5000] 21850.  1276          2.97\n3 Lagged               nnetar2 2022 Jul sample[5000] 21306.  1276          3.41\n4 Lagged               nnetar2 2022 Aug sample[5000] 20481.  1276          3.14\n5 Lagged               nnetar2 2022 Sep sample[5000] 17878.  1276          2.39\n6 Lagged               nnetar2 2022 Oct sample[5000] 17819.  1276          3.15\n# … with 2 more variables: `80%` &lt;hilo&gt;, `95%` &lt;hilo&gt;\n\n\nHere is the plot of the training data and the forecast.\n\n\nCode\nhydro_train2 |&gt;\n  autoplot(MWH) +\n  autolayer(nn_fit2, level = NULL)\n\n\n\n\n\nHere is the plot of the actual data and the forecast.\n\n\nCode\nhydro_test2 |&gt;\n  autoplot(MWH) +\n  autolayer(nn_fit2, level = NULL)\n\n\n\n\n\n\n\nCode\nnn_fit2 %&gt;% \n  filter(`Forecast Scenarios`==\"Lagged\") |&gt;\n  accuracy(hydro_comp2) |&gt;\n  select(.model, RMSE, MPE)\n\n\n# A tibble: 3 × 3\n  .model      RMSE   MPE\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 Arima.reg2 2585. -5.78\n2 ETS        2181.  4.16\n3 nnetar2    2430. -2.24\n\n\n\n\nCode\nnn_fit2 %&gt;% \n  filter(`Forecast Scenarios`==\"Mean\") |&gt;\n  accuracy(hydro_comp2) |&gt;\n  select(.model, RMSE, MPE)\n\n\n# A tibble: 3 × 3\n  .model      RMSE    MPE\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1 Arima.reg2 3267. -12.9 \n2 ETS        2181.   4.16\n3 nnetar2    2270.  -4.33\n\n\nWe can see that ETS has the lowest RMSE, while NNETAR with external regressors lagged at 12 has the lowest MPE.\nNow I creating an ensemble model that averages the predictions.\n\n\nCode\nstart_time = Sys.time()\n\nnn_ensemble = hydro_train2 |&gt;\n  model(Ensemble = (NNETAR(MWH ~ Count + Precipitation) +\n                     ARIMA(MWH ~ lag(Count) + lag(Precipitation),\n                           stepwise = FALSE,\n                           approx = FALSE) +\n                     ETS(MWH))/3)\n\nprint(Sys.time() - start_time)\n\n\nTime difference of 6.888192 secs\n\n\n\n\nCode\nnn_fit2.nsmbl = nn_ensemble |&gt;\n  forecast(new_data = future_scenarios)\n\n\n\n\nCode\nhydro_test2 |&gt;\n  autoplot(MWH) +\n  autolayer(nn_fit2.nsmbl, level = NULL)\n\n\n\n\n\n\n\nCode\nnn_fit2.nsmbl %&gt;% \n  filter(`Forecast Scenarios`==\"Mean\") |&gt;\n  accuracy(hydro_comp2) |&gt;\n  select(.model, RMSE, MPE)\n\n\n# A tibble: 1 × 3\n  .model    RMSE   MPE\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 Ensemble 2017. -4.57\n\n\nThe average prediction, is the best one, in terms of variance (RMSE). However, NNETAR is still better in terms of bias (MPE)."
  },
  {
    "objectID": "ml_procurement.html#libraries",
    "href": "ml_procurement.html#libraries",
    "title": "Procurement Contracts",
    "section": "Libraries",
    "text": "Libraries\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.8     ✔ purrr   1.0.1\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nLoading required package: Ecfun\n\n\nAttaching package: 'Ecfun'\n\n\nThe following object is masked from 'package:base':\n\n    sign\n\n\n\nAttaching package: 'Ecdat'\n\n\nThe following object is masked from 'package:datasets':\n\n    Orange\n\n\nLoading required package: Matrix\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-6"
  },
  {
    "objectID": "ml_procurement.html#stats",
    "href": "ml_procurement.html#stats",
    "title": "Procurement Contracts",
    "section": "Stats",
    "text": "Stats\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    0.00    0.25    0.34    0.50    1.00   86535 \n\n\n[1] 0.3489315\n\n\n[1] 161465"
  },
  {
    "objectID": "ml_procurement.html#cleaning",
    "href": "ml_procurement.html#cleaning",
    "title": "Procurement Contracts",
    "section": "Cleaning",
    "text": "Cleaning\n\n\n country_wb_string    country             WBCode          country_iso3      \n Length:161465      Length:161465      Length:161465      Length:161465     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n country_iso2           fyear           year         region         \n Length:161465      Min.   :1998   Min.   :1997   Length:161465     \n Class :character   1st Qu.:2000   1st Qu.:1999   Class :character  \n Mode  :character   Median :2002   Median :2001   Mode  :character  \n                    Mean   :2002   Mean   :2002                     \n                    3rd Qu.:2004   3rd Qu.:2004                     \n                    Max.   :2009   Max.   :2009                     \n                                                                    \n    pr_id             pr_name          pr_loannumber      cft_methodtype    \n Length:161465      Length:161465      Length:161465      Length:161465     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  cft_sector          ca_type            ca_title            ca_id          \n Length:161465      Length:161465      Length:161465      Length:161465     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  ca_nrbidsrec    ca_nrbidscons      ca_contract_value_original\n Min.   :  1.00   Length:161465      Length:161465             \n 1st Qu.:  2.00   Class :character   Class :character          \n Median :  3.00   Mode  :character   Mode  :character          \n Mean   :  5.02                                                \n 3rd Qu.:  6.00                                                \n Max.   :824.00                                                \n NA's   :55151                                                 \n ca_contract_value   lca_contract_value ca_contract_valuec ca_signdate       \n Min.   :0.000e+00   Min.   :-3.632     Length:161465      Length:161465     \n 1st Qu.:5.608e+04   1st Qu.:10.935     Class :character   Class :character  \n Median :2.297e+05   Median :12.345     Mode  :character   Mode  :character  \n Mean   :1.876e+06   Mean   :12.287                                          \n 3rd Qu.:8.755e+05   3rd Qu.:13.683                                          \n Max.   :2.132e+09   Max.   :21.480                                          \n NA's   :13          NA's   :13                                              \n   ca_date           ca_bids_all         ca_bids       ca_procedure      \n Length:161465      Min.   :  1.000   Min.   : 1.000   Length:161465     \n Class :character   1st Qu.:  1.000   1st Qu.: 1.000   Class :character  \n Mode  :character   Median :  2.000   Median : 2.000   Mode  :character  \n                    Mean   :  3.918   Mean   : 3.701                     \n                    3rd Qu.:  4.000   3rd Qu.: 4.000                     \n                    Max.   :993.000   Max.   :50.000                     \n                                                                         \n  ca_signper        ca_signper_corr  ca_signper_corr5   ca_signper_corr25\n Length:161465      Min.   :  0.00   Length:161465      Min.   : 1.00    \n Class :character   1st Qu.: 10.00   Class :character   1st Qu.: 8.00    \n Mode  :character   Median : 27.00   Mode  :character   Median :16.00    \n                    Mean   : 48.92                      Mean   :15.13    \n                    3rd Qu.: 57.00                      3rd Qu.:22.00    \n                    Max.   :728.00                      Max.   :26.00    \n                    NA's   :19568                                        \n    w_name           w_country             w_id             anb_name        \n Length:161465      Length:161465      Length:161465      Length:161465     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n    SuppAwd       b2_country         b3_country         b4_country       \n Min.   :1.000   Length:161465      Length:161465      Length:161465     \n 1st Qu.:1.000   Class :character   Class :character   Class :character  \n Median :1.000   Mode  :character   Mode  :character   Mode  :character  \n Mean   :1.049                                                           \n 3rd Qu.:1.000                                                           \n Max.   :7.000                                                           \n                                                                         \n  ppp_original        ppp          country_st            anb_id     \n Min.   :0.108   Min.   :0.1077   Length:161465      Min.   :    1  \n 1st Qu.:0.236   1st Qu.:0.2363   Class :character   1st Qu.: 3775  \n Median :0.306   Median :0.3107   Mode  :character   Median : 6902  \n Mean   :0.330   Mean   :0.3310                      Mean   : 7556  \n 3rd Qu.:0.386   3rd Qu.:0.3865                      3rd Qu.:11454  \n Max.   :1.086   Max.   :1.0864                      Max.   :15600  \n NA's   :6868    NA's   :11                          NA's   :366    \n procedure_type        singleb         prop_bidnr      corr_proc        \n Length:161465      Min.   :0.0000   Min.   :0.0004   Length:161465     \n Class :character   1st Qu.:0.0000   1st Qu.:0.0625   Class :character  \n Mode  :character   Median :0.0000   Median :0.2500   Mode  :character  \n                    Mean   :0.4147   Mean   :0.4676                     \n                    3rd Qu.:1.0000   3rd Qu.:1.0000                     \n                    Max.   :1.0000   Max.   :1.0000                     \n                                                                        \n  corr_proc3         corr_signp        corr_signp1         corr_signp2    \n Length:161465      Length:161465      Length:161465      Min.   :0.0000  \n Class :character   Class :character   Class :character   1st Qu.:0.0000  \n Mode  :character   Mode  :character   Mode  :character   Median :0.0000  \n                                                          Mean   :0.2533  \n                                                          3rd Qu.:1.0000  \n                                                          Max.   :1.0000  \n                                                                          \n  corr_signp3      corr_signp4      corr_proc31     corr_proc32   \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.000  \n Median :0.0000   Median :0.0000   Median :1.000   Median :0.000  \n Mean   :0.1148   Mean   :0.1212   Mean   :0.556   Mean   :0.228  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:1.000   3rd Qu.:0.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.000   Max.   :1.000  \n                                                                  \n  corr_proc33         nrc           w_iso             sec_score    \n Min.   :0.000   Min.   :    2   Length:161465      Min.   :31.38  \n 1st Qu.:0.000   1st Qu.: 1707   Class :character   1st Qu.:50.11  \n Median :0.000   Median : 2752   Mode  :character   Median :53.92  \n Mean   :0.216   Mean   : 4989                      Mean   :55.61  \n 3rd Qu.:0.000   3rd Qu.: 6202                      3rd Qu.:58.00  \n Max.   :1.000   Max.   :15701                      Max.   :92.00  \n                                                    NA's   :79897  \n sec_score_max      taxhav          taxhav_fixed         taxhav3         \n Min.   :31.38   Length:161465      Length:161465      Length:161465     \n 1st Qu.:51.00   Class :character   Class :character   Class :character  \n Median :54.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :56.01                                                           \n 3rd Qu.:60.00                                                           \n Max.   :92.00                                                           \n NA's   :84096                                                           \n  taxhav3bi             fsuppl          taxhav31        taxhav32     \n Length:161465      Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n Class :character   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Mode  :character   Median :0.0000   Median :0.000   Median :0.0000  \n                    Mean   :0.2568   Mean   :0.237   Mean   :0.0148  \n                    3rd Qu.:1.0000   3rd Qu.:0.000   3rd Qu.:0.0000  \n                    Max.   :1.0000   Max.   :1.000   Max.   :1.0000  \n                                                                     \n    taxhav33           taxhav34          cri_wb      \n Min.   :0.000000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.000000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.000000   Median :1.0000   Median :0.2500  \n Mean   :0.005017   Mean   :0.7432   Mean   :0.3419  \n 3rd Qu.:0.000000   3rd Qu.:1.0000   3rd Qu.:0.5000  \n Max.   :1.000000   Max.   :1.0000   Max.   :1.0000"
  },
  {
    "objectID": "ml_procurement.html#analysis-of-variables",
    "href": "ml_procurement.html#analysis-of-variables",
    "title": "Procurement Contracts",
    "section": "Analysis of Variables",
    "text": "Analysis of Variables\nSeeing unique values of categorical vars and descriptive stats of numerical ones.\n\n\n [1] \"National Competitive Bidding\"                 \n [2] \"Single Source Selection\"                      \n [3] \"International Competitive Bidding\"            \n [4] \"Direct Contracting\"                           \n [5] \"Quality And Cost-Based Selection\"             \n [6] \"Limited International Bidding\"                \n [7] \"National Shopping\"                            \n [8] \"International Shopping\"                       \n [9] \"Selection Based On Consultant's Qualification\"\n[10] \"Least Cost Selection\"                         \n[11] \"Quality Based Selection\"                      \n[12] \"Individual\"                                   \n[13] \"Selection Under a Fixed Budget\"               \n[14] \"CQS\"                                          \n[15] \"SHOP\"                                         \n\n\n\n\n [1] \"Public admin, Law\"    \"Health & social serv\" \"Education\"           \n [4] \"Water/sanit/fld prot\" \"Industry and trade\"   \"Transportation\"      \n [7] \"Finance\"              \"Agriculture\"          \"Energy & mining\"     \n[10] \"Info & communication\" \"(H)\"                  \"(H)Multisector\"      \n[13] \"\"                     \"(H)Priv Sector Dev\"  \n\n\n\n                                      (H)       (H)Multisector \n                  61                   66                   28 \n  (H)Priv Sector Dev          Agriculture            Education \n                   1                13494                15424 \n     Energy & mining              Finance Health & social serv \n               11106                 4878                25301 \n  Industry and trade Info & communication    Public admin, Law \n                7501                 2067                46307 \n      Transportation Water/sanit/fld prot \n               19215                16016 \n\n\nSector can be divided into dummies later\n\n\n[1] \"Civil Works\"         \"Consultant Services\" \"Goods\"              \n\n\n\n\n [1]   6   3   1   5   2   4  NA   7  17  19  10  12   8  11  16   9  21  13  14\n[20]  15  23  27  18 250  25  26  20  22  28  24  40 109  35  70  50  38  36 100\n[39]  56  33  31  58  57  48  45  30  41  79  39  67  65  32  47  54  29  76  34\n[58] 133  64  52  55  78  80 105  44  71 230  43  87  82  69 824  37  46 101 170\n[77] 149\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1.00    2.00    3.00    5.02    6.00  824.00   55151 \n\n\n\n\n [1] \"\"    \"1\"   \"2\"   \"3\"   \"5\"   \"4\"   \"7\"   \"6\"   \"88\"  \"13\"  \"8\"   \"12\" \n[13] \"11\"  \"9\"   \"23\"  \"15\"  \"113\" \"143\" \"22\"  \"51\"  \"101\" \"37\"  \"993\" \"102\"\n[25] \"14\"  \"16\"  \"10\"  \"141\" \"886\" \"682\" \"986\" \"20\"  \"21\"  \"31\"  \"26\"  \"17\" \n[37] \"60\" \n\n\n   Length     Class      Mode \n   161465 character character \n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  1.0000  0.5107  1.0000  1.0000 \n\n\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.      NA's \n0.000e+00 5.608e+04 2.297e+05 1.876e+06 8.755e+05 2.132e+09        13 \n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.      NA's \n0.000e+00 5.608e+04 2.297e+05 1.876e+06 8.755e+05 2.132e+09        13 \n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n -3.632  10.935  12.345  12.287  13.683  21.480      13 \n\n\n\n\n[1] \"50.000-199.999\" \"200.000-\"       \"0-24.999\"       \"25.000-49.999\" \n[5] \"\"              \n\n\n   Length     Class      Mode \n   161465 character character \n\n\n\n                     0-24.999       200.000-  25.000-49.999 50.000-199.999 \n            13          23693          85200          14064          38495 \n\n\n\n\n[1] \"open\"                \"single source\"       \"consultancy,cost\"   \n[4] \"restricted\"          \"consultancy,quality\"\n\n\n\n   consultancy,cost consultancy,quality                open          restricted \n              25508                9372               89770                7637 \n      single source \n              29178 \n\n\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.      NA's \n-29188.00      6.00     23.00     34.83     53.00   2076.00      4436 \n\n\n\n\n[1] 1 2 3 4 6 5 7\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.000   1.049   1.000   7.000 \n\n\n\n     1      2      3      4      5      6      7 \n155524   4462   1179    199     46     45     10 \n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  31.38   50.11   53.92   55.61   58.00   92.00   79897 \n\n\n\n\n [1] \"National Competitive Bidding\"                 \n [2] \"Single Source Selection\"                      \n [3] \"International Competitive Bidding\"            \n [4] \"Direct Contracting\"                           \n [5] \"Quality And Cost-Based Selection\"             \n [6] \"Limited International Bidding\"                \n [7] \"National Shopping\"                            \n [8] \"International Shopping\"                       \n [9] \"Selection Based On Consultant's Qualification\"\n[10] \"Least Cost Selection\"                         \n[11] \"Quality Based Selection\"                      \n[12] \"Individual\"                                   \n[13] \"Selection Under a Fixed Budget\"               \n\n\n\n                           Direct Contracting \n                                         5205 \n                                   Individual \n                                         9782 \n            International Competitive Bidding \n                                        48500 \n                       International Shopping \n                                         4004 \n                         Least Cost Selection \n                                          796 \n                Limited International Bidding \n                                         1896 \n                 National Competitive Bidding \n                                        41270 \n                            National Shopping \n                                         1737 \n             Quality And Cost-Based Selection \n                                        23916 \n                      Quality Based Selection \n                                         2354 \nSelection Based On Consultant's Qualification \n                                         7018 \n               Selection Under a Fixed Budget \n                                          796 \n                      Single Source Selection \n                                        14191 \n\n\n\n\n[1] \"domestic supplier\" \"NO tax haven\"      \"tax haven\"        \n\n\n\ndomestic supplier      NO tax haven         tax haven \n           119994             38272              3199 \n\n\n\n\n[1] \"domestic supplier\" \"NO tax haven\"      \"tax haven\"        \n\n\n\n\n[1] \"domestic supplier\"                   \n[2] \"NO tax haven & tax haven,large state\"\n[3] \"tax haven, small state\"              \n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.2500  0.3419  0.5000  1.0000 \n\n\n[1] 0.00 0.25 0.75 0.50 1.00\n\n\n\n    0  0.25   0.5  0.75     1 \n42239 50405 36622 31594   605"
  },
  {
    "objectID": "ml_procurement.html#ridge-regression-with-grid-search",
    "href": "ml_procurement.html#ridge-regression-with-grid-search",
    "title": "Procurement Contracts",
    "section": "Ridge Regression with Grid Search",
    "text": "Ridge Regression with Grid Search\n\n\n ca_contract_valuec lca_contract_value      AFR             EAP        \n Length:161452      Min.   :-3.632     Min.   :0.000   Min.   :0.0000  \n Class :character   1st Qu.:10.935     1st Qu.:0.000   1st Qu.:0.0000  \n Mode  :character   Median :12.345     Median :0.000   Median :0.0000  \n                    Mean   :12.287     Mean   :0.232   Mean   :0.1787  \n                    3rd Qu.:13.683     3rd Qu.:0.000   3rd Qu.:0.0000  \n                    Max.   :21.480     Max.   :1.000   Max.   :1.0000  \n      ECA              LCR              MNA               SAR        \n Min.   :0.0000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.00000   Median :0.0000  \n Mean   :0.1791   Mean   :0.1975   Mean   :0.07035   Mean   :0.1424  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n ca_Consul.Service    ca_Goods       ca_bids_all       ca_proc_open  \n Min.   :0.0000    Min.   :0.0000   Min.   :  1.000   Min.   :0.000  \n 1st Qu.:0.0000    1st Qu.:0.0000   1st Qu.:  1.000   1st Qu.:0.000  \n Median :0.0000    Median :0.0000   Median :  2.000   Median :1.000  \n Mean   :0.3645    Mean   :0.3459   Mean   :  3.918   Mean   :0.556  \n 3rd Qu.:1.0000    3rd Qu.:1.0000   3rd Qu.:  4.000   3rd Qu.:1.000  \n Max.   :1.0000    Max.   :1.0000   Max.   :993.000   Max.   :1.000  \n ca_proc_quality   ca_proc_source   ca_proc_restricted    singleb      \n Min.   :0.00000   Min.   :0.0000   Min.   :0.0000     Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000     1st Qu.:0.0000  \n Median :0.00000   Median :0.0000   Median :0.0000     Median :0.0000  \n Mean   :0.05805   Mean   :0.1806   Mean   :0.0473     Mean   :0.4147  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.0000     3rd Qu.:1.0000  \n Max.   :1.00000   Max.   :1.0000   Max.   :1.0000     Max.   :1.0000  \n  corr_signp1      corr_signp2      corr_signp3          nrc       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :    2  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 1707  \n Median :1.0000   Median :0.0000   Median :0.0000   Median : 2752  \n Mean   :0.5108   Mean   :0.2532   Mean   :0.1148   Mean   : 4990  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.: 6202  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :15701  \n    taxhaven       not.taxhaven     cri_wb.d0        cri_wb.d25    \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.000   Median :0.0000   Median :0.0000  \n Mean   :0.0198   Mean   :0.237   Mean   :0.2616   Mean   :0.3122  \n 3rd Qu.:0.0000   3rd Qu.:0.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n   cri_wb.d50       cri_wb.d75           P              Health      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.2268   Mean   :0.1956   Mean   :0.2868   Mean   :0.1567  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n      Edu                W                 I                Tran      \n Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.000  \n Median :0.00000   Median :0.00000   Median :0.00000   Median :0.000  \n Mean   :0.09553   Mean   :0.09919   Mean   :0.04646   Mean   :0.119  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.000  \n Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   Max.   :1.000  \n      Fin                A               Ener              Info       \n Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.00000   Median :0.0000   Median :0.00000   Median :0.0000  \n Mean   :0.03021   Mean   :0.0835   Mean   :0.06879   Mean   :0.0128  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.0000  \n Max.   :1.00000   Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n       H                 Multi                Priv        \n Min.   :0.0000000   Min.   :0.0000000   Min.   :0.0e+00  \n 1st Qu.:0.0000000   1st Qu.:0.0000000   1st Qu.:0.0e+00  \n Median :0.0000000   Median :0.0000000   Median :0.0e+00  \n Mean   :0.0004088   Mean   :0.0001734   Mean   :6.2e-06  \n 3rd Qu.:0.0000000   3rd Qu.:0.0000000   3rd Qu.:0.0e+00  \n Max.   :1.0000000   Max.   :1.0000000   Max.   :1.0e+00  \n\n\n\n\n35 x 1 sparse Matrix of class \"dgCMatrix\"\n                              s0\n(Intercept)         1.295066e+01\n(Intercept)         .           \nAFR                 1.740151e-01\nEAP                 5.540119e-02\nECA                -5.063949e-03\nLCR                -2.871303e-01\nMNA                 3.080603e-01\nSAR                 1.182342e-01\nca_Consul.Service  -1.243931e+00\nca_Goods           -6.270021e-01\nca_bids_all         9.673877e-04\nca_proc_open       -4.830663e-01\nca_proc_quality    -8.910647e-01\nca_proc_source     -7.194468e-01\nca_proc_restricted -1.445686e+00\nsingleb            -1.378996e-01\ncorr_signp1         5.353419e-01\ncorr_signp2         3.257312e-01\ncorr_signp3         7.423295e-01\nnrc                 3.892206e-05\ntaxhaven            8.405527e-01\nnot.taxhaven        8.169726e-01\nP                  -9.535187e-01\nHealth             -3.615957e-01\nEdu                -7.520389e-02\nW                   9.985832e-03\nI                  -6.340170e-01\nTran                4.970437e-01\nFin                -6.351149e-01\nA                   3.418690e-02\nEner                5.531834e-01\nInfo               -8.652731e-01\nH                  -1.507041e-01\nMulti              -5.725173e-01\nPriv                1.060161e+00\n\n\nCoefficients of this regression did not drop the two from the lr2, which might be because the estimates in glmnet and lm will never be exactly the same and one is the subset of the other.\nRMSE of the first Ridge regression is bigger than that of the unregularized linear regression. Seems that the optimal lambda should be small.\n\n\n[1] 0.05474761\n\n\nThe optimal lambda is very close to 0.\n\n\n[1] 0.01582965\n\n\n[1] 0.01447194\n\n\n[1] 0.01409955\n\n\nThe RMSE for Ridge regression with the optimal parameter is smaller than that with lambda 0 just a little bit. Which means regularized regression is just a little more efficient than the simple linear regression.\n\n\n[1] 56996.77\n\n\n[1] 57044.26\n\n\n\n\n\nCall:\nlm(formula = train_y ~ ., data = train_x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.9811  -1.1631   0.0784   1.2355   8.4591 \n\nCoefficients: (1 not defined because of singularities)\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         1.231e+01  2.672e-01  46.051  &lt; 2e-16 ***\nAFR                 5.563e-02  2.107e-02   2.640 0.008280 ** \nEAP                -6.235e-02  1.788e-02  -3.488 0.000488 ***\nECA                -1.228e-01  2.140e-02  -5.735 9.75e-09 ***\nLCR                -4.057e-01  2.058e-02 -19.717  &lt; 2e-16 ***\nMNA                 1.897e-01  2.630e-02   7.215 5.40e-13 ***\nSAR                        NA         NA      NA       NA    \nca_Consul.Service  -1.254e+00  3.123e-02 -40.153  &lt; 2e-16 ***\nca_Goods           -6.273e-01  1.324e-02 -47.388  &lt; 2e-16 ***\nca_bids_all         9.722e-04  3.647e-04   2.666 0.007686 ** \nca_proc_open       -4.938e-01  3.371e-02 -14.650  &lt; 2e-16 ***\nca_proc_quality    -8.915e-01  2.411e-02 -36.982  &lt; 2e-16 ***\nca_proc_source     -7.207e-01  1.854e-02 -38.863  &lt; 2e-16 ***\nca_proc_restricted -1.456e+00  3.999e-02 -36.409  &lt; 2e-16 ***\nsingleb            -1.378e-01  1.251e-02 -11.009  &lt; 2e-16 ***\ncorr_signp1         5.356e-01  1.612e-02  33.230  &lt; 2e-16 ***\ncorr_signp2         3.258e-01  1.724e-02  18.900  &lt; 2e-16 ***\ncorr_signp3         7.424e-01  2.055e-02  36.121  &lt; 2e-16 ***\nnrc                 3.891e-05  1.493e-06  26.061  &lt; 2e-16 ***\ntaxhaven            8.407e-01  3.551e-02  23.672  &lt; 2e-16 ***\nnot.taxhaven        8.168e-01  1.261e-02  64.770  &lt; 2e-16 ***\nP                  -1.791e-01  2.635e-01  -0.679 0.496886    \nHealth              4.127e-01  2.637e-01   1.565 0.117569    \nEdu                 6.990e-01  2.639e-01   2.649 0.008080 ** \nW                   7.842e-01  2.639e-01   2.972 0.002962 ** \nI                   1.401e-01  2.643e-01   0.530 0.596089    \nTran                1.271e+00  2.638e-01   4.818 1.45e-06 ***\nFin                 1.388e-01  2.648e-01   0.524 0.600193    \nA                   8.082e-01  2.639e-01   3.062 0.002199 ** \nEner                1.327e+00  2.641e-01   5.026 5.03e-07 ***\nInfo               -9.151e-02  2.669e-01  -0.343 0.731706    \nH                   6.233e-01  3.608e-01   1.728 0.084042 .  \nMulti               2.018e-01  4.622e-01   0.437 0.662471    \nPriv                1.833e+00  1.879e+00   0.976 0.329313    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.861 on 145274 degrees of freedom\nMultiple R-squared:  0.2276,    Adjusted R-squared:  0.2275 \nF-statistic:  1338 on 32 and 145274 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nWarning in predict.lm(lr3, test_x): prediction from a rank-deficient fit may be\nmisleading\n\n\n[1] 56998.57\n\n\nResidual Sums Squared are almost identical for both regressions. Given that, I’d suggest sticking to a simpler model, i.e. unpenalized Linear Regression Model."
  },
  {
    "objectID": "ml_procurement.html#corrected-ridge-without-cri_wb",
    "href": "ml_procurement.html#corrected-ridge-without-cri_wb",
    "title": "Procurement Contracts",
    "section": "Corrected Ridge without cri_wb",
    "text": "Corrected Ridge without cri_wb\nLater in the research I discovered that cri_wb caused multicollinearity, so I removed it and performed Ridge regression again\n\n\n[1] 0.01447194\n\n\n[1] 56996.77\n\n\n\n\n[1] 0.05474761\n\n\nThe optimal lambda is very close to 0.\n\n\n[1] 0.01409955\n\n\n[1] 57044.26"
  },
  {
    "objectID": "ml_procurement.html#logit-for-contract-value-categories",
    "href": "ml_procurement.html#logit-for-contract-value-categories",
    "title": "Procurement Contracts",
    "section": "Logit for contract value categories",
    "text": "Logit for contract value categories\n\n\n\nCall:\nglm(formula = value.cat ~ ., family = binomial(link = \"logit\"), \n    data = Procurement4)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1680  -1.0435   0.5946   0.9686   2.4301  \n\nCoefficients: (2 not defined because of singularities)\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         2.173e-01  3.725e-01   0.583 0.559751    \nAFR                -1.557e-01  2.348e-02  -6.632 3.32e-11 ***\nEAP                -2.572e-01  2.025e-02 -12.697  &lt; 2e-16 ***\nECA                -2.847e-01  2.384e-02 -11.941  &lt; 2e-16 ***\nLCR                -6.262e-01  2.302e-02 -27.201  &lt; 2e-16 ***\nMNA                -8.363e-02  2.911e-02  -2.873 0.004065 ** \nSAR                        NA         NA      NA       NA    \nca_Consul.Service  -1.132e+00  3.419e-02 -33.102  &lt; 2e-16 ***\nca_Goods           -4.248e-01  1.462e-02 -29.046  &lt; 2e-16 ***\nca_bids_all         3.636e-04  4.081e-04   0.891 0.372981    \nca_proc_open       -2.588e-01  1.072e-01  -2.414 0.015777 *  \nca_proc_quality    -7.728e-01  2.734e-02 -28.270  &lt; 2e-16 ***\nca_proc_source     -8.616e-01  2.107e-02 -40.888  &lt; 2e-16 ***\nca_proc_restricted -1.399e+00  4.401e-02 -31.799  &lt; 2e-16 ***\nsingleb            -4.436e-01  1.012e-01  -4.381 1.18e-05 ***\ncorr_signp1         8.260e-01  1.021e-01   8.090 5.98e-16 ***\ncorr_signp2         3.143e-01  1.952e-02  16.099  &lt; 2e-16 ***\ncorr_signp3         6.527e-01  2.323e-02  28.100  &lt; 2e-16 ***\nnrc                 2.967e-05  1.669e-06  17.774  &lt; 2e-16 ***\ntaxhaven            4.303e-01  9.095e-02   4.731 2.23e-06 ***\nnot.taxhaven        6.952e-01  1.412e-02  49.228  &lt; 2e-16 ***\ncri_wb.d0          -9.895e-01  3.031e-01  -3.265 0.001095 ** \ncri_wb.d25         -7.253e-01  2.031e-01  -3.572 0.000354 ***\ncri_wb.d50         -3.457e-01  1.035e-01  -3.341 0.000834 ***\ncri_wb.d75                 NA         NA      NA       NA    \nP                   3.103e-01  3.543e-01   0.876 0.381088    \nHealth              9.720e-01  3.544e-01   2.743 0.006092 ** \nEdu                 1.355e+00  3.545e-01   3.822 0.000132 ***\nW                   1.275e+00  3.545e-01   3.596 0.000323 ***\nI                   6.389e-01  3.550e-01   1.800 0.071878 .  \nTran                1.641e+00  3.545e-01   4.629 3.68e-06 ***\nFin                 7.281e-01  3.554e-01   2.049 0.040501 *  \nA                   1.378e+00  3.546e-01   3.885 0.000102 ***\nEner                1.716e+00  3.548e-01   4.837 1.32e-06 ***\nInfo                3.506e-01  3.573e-01   0.981 0.326468    \nH                   1.121e+00  4.383e-01   2.557 0.010551 *  \nMulti               2.200e+00  5.811e-01   3.786 0.000153 ***\nPriv                7.657e+00  2.667e+01   0.287 0.774001    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 223324  on 161451  degrees of freedom\nResidual deviance: 195143  on 161416  degrees of freedom\nAIC: 195215\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n\nCall:\nglm(formula = train_y.log ~ ., family = binomial(link = \"logit\"), \n    data = train_x.log)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1717  -1.0427   0.5951   0.9680   2.4297  \n\nCoefficients: (2 not defined because of singularities)\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         1.446e-01  4.138e-01   0.349 0.726831    \nAFR                -1.496e-01  2.475e-02  -6.045 1.49e-09 ***\nEAP                -2.556e-01  2.132e-02 -11.987  &lt; 2e-16 ***\nECA                -2.766e-01  2.514e-02 -11.005  &lt; 2e-16 ***\nLCR                -6.208e-01  2.426e-02 -25.591  &lt; 2e-16 ***\nMNA                -6.703e-02  3.071e-02  -2.183 0.029056 *  \nSAR                        NA         NA      NA       NA    \nca_Consul.Service  -1.121e+00  3.604e-02 -31.096  &lt; 2e-16 ***\nca_Goods           -4.294e-01  1.541e-02 -27.856  &lt; 2e-16 ***\nca_bids_all         3.483e-04  4.278e-04   0.814 0.415608    \nca_proc_open       -2.637e-01  1.128e-01  -2.338 0.019403 *  \nca_proc_quality    -7.794e-01  2.887e-02 -26.995  &lt; 2e-16 ***\nca_proc_source     -8.612e-01  2.224e-02 -38.728  &lt; 2e-16 ***\nca_proc_restricted -1.396e+00  4.643e-02 -30.069  &lt; 2e-16 ***\nsingleb            -4.285e-01  1.065e-01  -4.025 5.70e-05 ***\ncorr_signp1         8.177e-01  1.073e-01   7.620 2.54e-14 ***\ncorr_signp2         3.158e-01  2.057e-02  15.352  &lt; 2e-16 ***\ncorr_signp3         6.552e-01  2.447e-02  26.772  &lt; 2e-16 ***\nnrc                 2.977e-05  1.760e-06  16.917  &lt; 2e-16 ***\ntaxhaven            4.361e-01  9.553e-02   4.565 5.00e-06 ***\nnot.taxhaven        6.971e-01  1.491e-02  46.765  &lt; 2e-16 ***\ncri_wb.d0          -9.462e-01  3.186e-01  -2.970 0.002982 ** \ncri_wb.d25         -7.003e-01  2.135e-01  -3.280 0.001037 ** \ncri_wb.d50         -3.246e-01  1.088e-01  -2.983 0.002850 ** \ncri_wb.d75                 NA         NA      NA       NA    \nP                   3.490e-01  3.955e-01   0.882 0.377560    \nHealth              1.020e+00  3.956e-01   2.579 0.009897 ** \nEdu                 1.398e+00  3.958e-01   3.533 0.000411 ***\nW                   1.311e+00  3.958e-01   3.313 0.000924 ***\nI                   6.749e-01  3.962e-01   1.704 0.088465 .  \nTran                1.680e+00  3.957e-01   4.245 2.19e-05 ***\nFin                 7.710e-01  3.966e-01   1.944 0.051913 .  \nA                   1.418e+00  3.958e-01   3.582 0.000341 ***\nEner                1.769e+00  3.960e-01   4.467 7.94e-06 ***\nInfo                3.904e-01  3.985e-01   0.979 0.327335    \nH                   1.155e+00  4.832e-01   2.391 0.016820 *  \nMulti               2.246e+00  6.367e-01   3.527 0.000420 ***\nPriv                8.699e+00  4.396e+01   0.198 0.843127    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 200979  on 145306  degrees of freedom\nResidual deviance: 175595  on 145271  degrees of freedom\nAIC: 175667\n\nNumber of Fisher Scoring iterations: 7\n\n\n\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\n\n        8        14        34        51        55        63 \n0.6394443 0.6120216 0.4675069 0.3935432 0.4991478 0.7957945 \n\n\n\n\n 8 14 34 51 55 63 \n 1  1  0  0  0  1 \n\n\n\n\n[1] 0.6716011\n\n\n\n\nModel :\ntrain_y.log ~ AFR + EAP + ECA + LCR + MNA + SAR + ca_Consul.Service + \n    ca_Goods + ca_bids_all + ca_proc_open + ca_proc_quality + \n    ca_proc_source + ca_proc_restricted + singleb + corr_signp1 + \n    corr_signp2 + corr_signp3 + nrc + taxhaven + not.taxhaven + \n    cri_wb.d0 + cri_wb.d25 + cri_wb.d50 + cri_wb.d75 + P + Health + \n    Edu + W + I + Tran + Fin + A + Ener + Info + H + Multi + \n    Priv\n\nComplete :\n           (Intercept) AFR EAP ECA LCR MNA ca_Consul.Service ca_Goods\nSAR         1          -1  -1  -1  -1  -1   0                 0      \ncri_wb.d75  2           0   0   0   0   0   0                 0      \n           ca_bids_all ca_proc_open ca_proc_quality ca_proc_source\nSAR         0           0            0               0            \ncri_wb.d75  0           1            0               0            \n           ca_proc_restricted singleb corr_signp1 corr_signp2 corr_signp3 nrc\nSAR         0                  0       0           0           0           0 \ncri_wb.d75  0                 -1       1           0           0           0 \n           taxhaven not.taxhaven cri_wb.d0 cri_wb.d25 cri_wb.d50 P  Health Edu\nSAR         0        0            0         0          0          0  0      0 \ncri_wb.d75 -1        0           -4        -3         -2          0  0      0 \n           W  I  Tran Fin A  Ener Info H  Multi Priv\nSAR         0  0  0    0   0  0    0    0  0     0  \ncri_wb.d75  0  0  0    0   0  0    0    0  0     0  \n\n\n\n\nNULL\n\n\n\n\n\nCall:\nglm(formula = train_y.log ~ ., family = binomial(link = \"logit\"), \n    data = train_x.log2)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1247  -1.0546   0.5972   0.9890   2.3543  \n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        -4.547e-01  3.980e-01  -1.142 0.253301    \nca_Consul.Service  -1.164e+00  3.587e-02 -32.450  &lt; 2e-16 ***\nca_Goods           -4.829e-01  1.515e-02 -31.880  &lt; 2e-16 ***\nca_proc_open       -5.625e-01  3.864e-02 -14.559  &lt; 2e-16 ***\nca_proc_quality    -7.681e-01  2.861e-02 -26.849  &lt; 2e-16 ***\nca_proc_source     -8.259e-01  2.176e-02 -37.948  &lt; 2e-16 ***\nca_proc_restricted -1.451e+00  4.600e-02 -31.536  &lt; 2e-16 ***\nsingleb            -1.536e-01  1.412e-02 -10.880  &lt; 2e-16 ***\ncorr_signp1         5.438e-01  1.899e-02  28.637  &lt; 2e-16 ***\ncorr_signp2         3.407e-01  2.035e-02  16.740  &lt; 2e-16 ***\ncorr_signp3         6.837e-01  2.421e-02  28.246  &lt; 2e-16 ***\nnrc                 3.793e-05  1.264e-06  30.010  &lt; 2e-16 ***\ntaxhaven            7.224e-01  4.179e-02  17.286  &lt; 2e-16 ***\nnot.taxhaven        7.763e-01  1.460e-02  53.159  &lt; 2e-16 ***\nP                   2.943e-01  3.954e-01   0.744 0.456650    \nHealth              9.874e-01  3.955e-01   2.496 0.012545 *  \nEdu                 1.335e+00  3.957e-01   3.373 0.000743 ***\nW                   1.257e+00  3.957e-01   3.176 0.001496 ** \nI                   6.348e-01  3.961e-01   1.603 0.109024    \nTran                1.625e+00  3.957e-01   4.108 3.99e-05 ***\nFin                 7.117e-01  3.966e-01   1.795 0.072714 .  \nA                   1.386e+00  3.958e-01   3.502 0.000461 ***\nEner                1.728e+00  3.960e-01   4.364 1.28e-05 ***\nInfo                3.181e-01  3.984e-01   0.798 0.424710    \nH                   1.137e+00  4.840e-01   2.349 0.018802 *  \nMulti               2.320e+00  6.344e-01   3.657 0.000255 ***\nPriv                8.504e+00  4.396e+01   0.193 0.846597    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 200979  on 145306  degrees of freedom\nResidual deviance: 176745  on 145280  degrees of freedom\nAIC: 176799\n\nNumber of Fisher Scoring iterations: 7\n\n\n\n\nNULL\n\n\n\n\n        8        14        34        51        55        63 \n0.6194700 0.5866468 0.4000703 0.3530457 0.4693526 0.7737040 \n\n\n\n\n 8 14 34 51 55 63 \n 1  1  0  0  0  1 \n\n\n\n\n[1] 0.6688758"
  },
  {
    "objectID": "ml_procurement.html#knn",
    "href": "ml_procurement.html#knn",
    "title": "Procurement Contracts",
    "section": "KNN",
    "text": "KNN\n\n\n[1] 0.7562713\n\n\n\n\n[1] 0.7529266\n\n\n\n\n[1] 0.7591205"
  },
  {
    "objectID": "ml_procurement.html#beyond-linearity-splines",
    "href": "ml_procurement.html#beyond-linearity-splines",
    "title": "Procurement Contracts",
    "section": "Beyond Linearity: Splines",
    "text": "Beyond Linearity: Splines\n\n\n\nCall:\nlm(formula = train_y ~ bs(nrc, degree = 3) + Edu + Ener + Multi, \n    data = train_x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.3907  -1.3039   0.0776   1.3431   9.4819 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          11.99366    0.01865 643.154   &lt;2e-16 ***\nbs(nrc, degree = 3)1 -0.03461    0.06848  -0.505    0.613    \nbs(nrc, degree = 3)2  0.62123    0.06629   9.371   &lt;2e-16 ***\nbs(nrc, degree = 3)3  0.87160    0.02672  32.625   &lt;2e-16 ***\nEdu                   0.27344    0.01862  14.682   &lt;2e-16 ***\nEner                  1.03912    0.02175  47.769   &lt;2e-16 ***\nMulti                -0.21348    0.42393  -0.504    0.615    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.077 on 145300 degrees of freedom\nMultiple R-squared:  0.03805,   Adjusted R-squared:  0.03801 \nF-statistic: 957.9 on 6 and 145300 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n       8       14       34       51       55       63 \n11.99398 12.26742 12.26742 12.00141 12.00141 12.00141 \n\n\n\n\n[1]  8.978580 12.802028 13.114779  9.409601 12.325201 14.210191\n\n\n[1] 0.01513323\n\n\n[1] 70996.55\n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6446 10.9005 12.3221 12.2694 13.6952 20.3544 \n\n\n\n\n\nCall:\nlm(formula = train_y ~ bs(nrc, degree = 3) + bs(ca_bids_all, \n    degree = 3) + Edu + Ener + Multi, data = train_x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.3182  -1.2970   0.0857   1.3408   9.3605 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  11.90826    0.01882 632.741  &lt; 2e-16 ***\nbs(nrc, degree = 3)1          0.01477    0.06829   0.216  0.82881    \nbs(nrc, degree = 3)2          0.53280    0.06617   8.052  8.2e-16 ***\nbs(nrc, degree = 3)3          0.84187    0.02665  31.586  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)1 11.07475    0.38349  28.879  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)2 -2.73186    1.81815  -1.503  0.13296    \nbs(ca_bids_all, degree = 3)3  2.58227    0.83824   3.081  0.00207 ** \nEdu                           0.24309    0.01860  13.072  &lt; 2e-16 ***\nEner                          1.04128    0.02169  48.014  &lt; 2e-16 ***\nMulti                        -0.20574    0.42263  -0.487  0.62639    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.07 on 145297 degrees of freedom\nMultiple R-squared:  0.04394,   Adjusted R-squared:  0.04388 \nF-statistic:   742 on 9 and 145297 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n[1] 70454.6\n\n\n[1] 0.01356418\n\n\n\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n[1] 23225\n\n\n[1] 294\n\n\n[1] 0.002017656\n\n\n[1] 0.1599802\n\n\n\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] -0.8413083 -0.8413083 -0.8413083 -0.8413083 -0.8413083 -0.8413083\n\n\n[1]  0.5530255 -0.1404550 -0.1404550 -0.6027754  0.3218654  0.5530255\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-1.0156 -0.6685 -0.4557  0.0000  0.2466  2.1803 \n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.6028 -0.6028 -0.3716  0.0000  0.0907 10.2618 \n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -3.632  10.933  12.344  12.286  13.681  21.480 \n\n\n\n\n\nCall:\nlm(formula = train_y.sp ~ bs(nrc, degree = 3) + bs(ca_bids_all, \n    degree = 3) + Edu + Ener + Multi, data = train_x.sp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.2785  -1.2860   0.0873   1.3266   8.8929 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  11.68783    0.01908 612.483  &lt; 2e-16 ***\nbs(nrc, degree = 3)1          0.10699    0.06769   1.581    0.114    \nbs(nrc, degree = 3)2          0.40187    0.06573   6.114 9.74e-10 ***\nbs(nrc, degree = 3)3          0.74986    0.02645  28.347  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)1  3.52129    0.06223  56.588  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)2 -3.71759    0.16550 -22.463  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)3  0.90606    0.17384   5.212 1.87e-07 ***\nEdu                           0.23924    0.01849  12.941  &lt; 2e-16 ***\nEner                          0.98373    0.02148  45.797  &lt; 2e-16 ***\nMulti                        -0.24564    0.39442  -0.623    0.533    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.049 on 145002 degrees of freedom\nMultiple R-squared:  0.06263,   Adjusted R-squared:  0.06257 \nF-statistic:  1076 on 9 and 145002 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n       8       14       34       51       55       56 \n12.49270 12.33904 11.94622 12.11588 11.93343 12.11588 \n\n\n\n\n[1] 69440.65\n\n\n[1] 0.01712034\n\n\n\n\n\nCall:\nlm(formula = train_y.sp ~ bs(nrc, degree = 1) + bs(ca_bids_all, \n    degree = 1) + Edu + Ener + Multi, data = train_x.sp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.3155  -1.2925   0.0838   1.3407   9.3581 \n\nCoefficients:\n                             Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)                 11.808730   0.008531 1384.211   &lt;2e-16 ***\nbs(nrc, degree = 1)          0.888195   0.017549   50.612   &lt;2e-16 ***\nbs(ca_bids_all, degree = 1)  1.883331   0.059426   31.692   &lt;2e-16 ***\nEdu                          0.225089   0.018657   12.064   &lt;2e-16 ***\nEner                         1.032723   0.021591   47.832   &lt;2e-16 ***\nMulti                       -0.178013   0.398264   -0.447    0.655    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.069 on 145006 degrees of freedom\nMultiple R-squared:  0.04415,   Adjusted R-squared:  0.04412 \nF-statistic:  1340 on 5 and 145006 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n       8       14       34       51       55       56 \n12.05751 12.16239 12.08225 11.97277 11.93270 11.97277 \n\n\n\n\n[1] 0.01716597\n\n\n[1] 70568.34\n\n\n\n\n[1] 0\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  11.05   11.81   12.15   12.29   12.65   14.43 \n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.6561 10.9044 12.3478 12.2704 13.6981 20.3544 \n\n\n\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:Ecdat':\n\n    Gasoline\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.8-39. For overview type 'help(\"mgcv-package\")'.\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ntrain_y.sp ~ bs(nrc, degree = 3) + bs(ca_bids_all, degree = 3) + \n    Edu + Ener + Multi\n\nParametric coefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  11.68783    0.01908 612.483  &lt; 2e-16 ***\nbs(nrc, degree = 3)1          0.10699    0.06769   1.581    0.114    \nbs(nrc, degree = 3)2          0.40187    0.06573   6.114 9.74e-10 ***\nbs(nrc, degree = 3)3          0.74986    0.02645  28.347  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)1  3.52129    0.06223  56.588  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)2 -3.71759    0.16550 -22.463  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)3  0.90606    0.17384   5.212 1.87e-07 ***\nEdu                           0.23924    0.01849  12.941  &lt; 2e-16 ***\nEner                          0.98373    0.02148  45.797  &lt; 2e-16 ***\nMulti                        -0.24564    0.39442  -0.623    0.533    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  0.0626   Deviance explained = 6.26%\nGCV = 4.1992  Scale est. = 4.1989    n = 145012\n\n\n\n\n[1] 69440.65\n\n\n[1] 0.01712034\n\n\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ntrain_y ~ bs(nrc, degree = 3) + bs(ca_bids_all, degree = 3) + \n    Edu + Ener + Multi\n\nParametric coefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  11.90826    0.01882 632.741  &lt; 2e-16 ***\nbs(nrc, degree = 3)1          0.01477    0.06829   0.216  0.82881    \nbs(nrc, degree = 3)2          0.53280    0.06617   8.052  8.2e-16 ***\nbs(nrc, degree = 3)3          0.84187    0.02665  31.586  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)1 11.07475    0.38349  28.879  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)2 -2.73186    1.81815  -1.503  0.13296    \nbs(ca_bids_all, degree = 3)3  2.58227    0.83824   3.081  0.00207 ** \nEdu                           0.24309    0.01860  13.072  &lt; 2e-16 ***\nEner                          1.04128    0.02169  48.014  &lt; 2e-16 ***\nMulti                        -0.20574    0.42263  -0.487  0.62639    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  0.0439   Deviance explained = 4.39%\nGCV = 4.2859  Scale est. = 4.2856    n = 145307\n\n\n\n\n[1] 70454.6\n\n\n[1] 0.01356418\n\n\n\n\n                 (Intercept)         bs(nrc, degree = 3)1 \n                  11.6878264                    0.1069901 \n        bs(nrc, degree = 3)2         bs(nrc, degree = 3)3 \n                   0.4018697                    0.7498629 \nbs(ca_bids_all, degree = 3)1 bs(ca_bids_all, degree = 3)2 \n                   3.5212868                   -3.7175900 \nbs(ca_bids_all, degree = 3)3                          Edu \n                   0.9060622                    0.2392372 \n                        Ener                        Multi \n                   0.9837282                   -0.2456402 \n\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = train_y.sp ~ nrc + ca_bids_all + Edu + Ener + Multi, \n    data = train_x.sp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.3155  -1.2925   0.0838   1.3407   9.3581 \n\nCoefficients:\n             Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) 12.195462   0.005944 2051.686   &lt;2e-16 ***\nnrc          0.277917   0.005491   50.612   &lt;2e-16 ***\nca_bids_all  0.173347   0.005470   31.692   &lt;2e-16 ***\nEdu          0.225089   0.018657   12.064   &lt;2e-16 ***\nEner         1.032723   0.021591   47.832   &lt;2e-16 ***\nMulti       -0.178013   0.398264   -0.447    0.655    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.069 on 145006 degrees of freedom\nMultiple R-squared:  0.04415,   Adjusted R-squared:  0.04412 \nF-statistic:  1340 on 5 and 145006 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n[1] 0.01716597\n\n\n[1] 70568.34\n\n\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ntrain_y ~ bs(nrc, degree = 3) + bs(ca_bids_all, degree = 3) + \n    AFR + EAP + ECA + LCR + MNA + SAR + ca_Consul.Service + ca_Goods + \n    ca_proc_open + ca_proc_quality + ca_proc_source + ca_proc_restricted + \n    corr_signp1 + corr_signp2 + corr_signp3 + taxhaven + not.taxhaven + \n    P + Health + Edu + W + I + Tran + Fin + A + Ener + Info + \n    H + Multi + Priv\n\nParametric coefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  10.24556    0.22866  44.806  &lt; 2e-16 ***\nbs(nrc, degree = 3)1          0.83188    0.07045  11.807  &lt; 2e-16 ***\nbs(nrc, degree = 3)2          0.64910    0.07630   8.507  &lt; 2e-16 ***\nbs(nrc, degree = 3)3          0.71961    0.02955  24.356  &lt; 2e-16 ***\nbs(ca_bids_all, degree = 3)1 -0.07827    0.36783  -0.213  0.83150    \nbs(ca_bids_all, degree = 3)2  6.43106    1.63680   3.929 8.53e-05 ***\nbs(ca_bids_all, degree = 3)3 -0.64031    0.75370  -0.850  0.39557    \nAFR                           1.82642    0.03962  46.100  &lt; 2e-16 ***\nEAP                           1.70413    0.04063  41.944  &lt; 2e-16 ***\nECA                           1.63749    0.03979  41.158  &lt; 2e-16 ***\nLCR                           1.30747    0.03976  32.884  &lt; 2e-16 ***\nMNA                           1.98542    0.04169  47.623  &lt; 2e-16 ***\nSAR                           1.78463    0.04135  43.156  &lt; 2e-16 ***\nca_Consul.Service            -1.26199    0.03122 -40.428  &lt; 2e-16 ***\nca_Goods                     -0.63796    0.01323 -48.204  &lt; 2e-16 ***\nca_proc_open                 -0.42812    0.03361 -12.739  &lt; 2e-16 ***\nca_proc_quality              -0.91561    0.02399 -38.174  &lt; 2e-16 ***\nca_proc_source               -0.76538    0.01804 -42.417  &lt; 2e-16 ***\nca_proc_restricted           -1.38403    0.03981 -34.767  &lt; 2e-16 ***\ncorr_signp1                   0.53926    0.01612  33.450  &lt; 2e-16 ***\ncorr_signp2                   0.32408    0.01723  18.805  &lt; 2e-16 ***\ncorr_signp3                   0.75086    0.02056  36.521  &lt; 2e-16 ***\ntaxhaven                      0.86965    0.03552  24.482  &lt; 2e-16 ***\nnot.taxhaven                  0.83840    0.01265  66.259  &lt; 2e-16 ***\nP                            -0.16916    0.26336  -0.642  0.52066    \nHealth                        0.42271    0.26350   1.604  0.10867    \nEdu                           0.70060    0.26369   2.657  0.00789 ** \nW                             0.79292    0.26368   3.007  0.00264 ** \nI                             0.15557    0.26414   0.589  0.55589    \nTran                          1.27607    0.26362   4.841 1.30e-06 ***\nFin                           0.15867    0.26461   0.600  0.54876    \nA                             0.82551    0.26374   3.130  0.00175 ** \nEner                          1.34328    0.26388   5.090 3.58e-07 ***\nInfo                         -0.04931    0.26670  -0.185  0.85333    \nH                             0.70120    0.36051   1.945  0.05178 .  \nMulti                         0.15113    0.46191   0.327  0.74352    \nPriv                          1.80728    1.87819   0.962  0.33593    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRank: 36/37\nR-sq.(adj) =  0.229   Deviance explained = 22.9%\nGCV = 3.4586  Scale est. = 3.4577    n = 145307\n\n\n\n\n[1] 0.01400472\n\n\n[1] 56856.29"
  },
  {
    "objectID": "ml_procurement.html#regression-tree",
    "href": "ml_procurement.html#regression-tree",
    "title": "Procurement Contracts",
    "section": "Regression Tree",
    "text": "Regression Tree\n\n\n[1] 0\n\n\n\n\n'data.frame':   161452 obs. of  39 variables:\n $ ca_contract_valuec: chr  \"50.000-199.999\" \"200.000-\" \"200.000-\" \"200.000-\" ...\n $ lca_contract_value: num  12 12.4 12.3 13.5 11.3 ...\n $ AFR               : num  1 1 1 1 1 1 1 1 1 1 ...\n $ EAP               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ ECA               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ LCR               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ MNA               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ SAR               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ ca_Consul.Service : num  0 0 0 1 0 0 0 0 0 0 ...\n $ ca_Goods          : num  0 0 0 0 1 0 1 1 1 1 ...\n $ ca_bids_all       : int  6 3 3 1 5 6 1 6 5 2 ...\n $ ca_proc_open      : num  1 1 1 0 1 1 0 1 1 1 ...\n $ ca_proc_quality   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ ca_proc_source    : num  0 0 0 1 0 0 1 0 0 0 ...\n $ ca_proc_restricted: num  0 0 0 0 0 0 0 0 0 0 ...\n $ singleb           : num  0 0 0 1 0 0 1 0 0 0 ...\n $ corr_signp1       : num  1 0 0 0 1 0 0 1 1 0 ...\n $ corr_signp2       : int  0 0 0 0 0 0 1 0 0 1 ...\n $ corr_signp3       : int  0 0 0 1 0 0 0 0 0 0 ...\n $ nrc               : int  858 858 858 858 858 858 858 858 858 858 ...\n $ taxhaven          : num  0 0 0 0 0 0 0 0 0 0 ...\n $ not.taxhaven      : num  0 0 0 1 1 0 1 1 1 0 ...\n $ cri_wb.d0         : num  1 0 0 0 1 0 0 1 1 0 ...\n $ cri_wb.d25        : num  0 1 1 0 0 1 0 0 0 1 ...\n $ cri_wb.d50        : num  0 0 0 0 0 0 0 0 0 0 ...\n $ cri_wb.d75        : num  0 0 0 1 0 0 1 0 0 0 ...\n $ P                 : num  1 0 0 0 0 1 0 0 0 0 ...\n $ Health            : num  0 1 0 0 0 0 1 0 0 0 ...\n $ Edu               : num  0 0 1 0 0 0 0 0 0 0 ...\n $ W                 : num  0 0 0 1 0 0 0 0 0 0 ...\n $ I                 : num  0 0 0 0 1 0 0 1 0 0 ...\n $ Tran              : num  0 0 0 0 0 0 0 0 1 0 ...\n $ Fin               : num  0 0 0 0 0 0 0 0 0 1 ...\n $ A                 : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Ener              : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Info              : num  0 0 0 0 0 0 0 0 0 0 ...\n $ H                 : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Multi             : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Priv              : num  0 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:13] 59655 59815 138472 138473 138474 138475 138476 138477 138478 138479 ...\n  ..- attr(*, \"names\")= chr [1:13] \"59655\" \"59815\" \"138472\" \"138473\" ...\n\n\nWe need to recode dummies as factors for the decision tree.\n\n\nWarning: Using `all_of()` outside of a selecting function was deprecated in tidyselect\n1.2.0.\nℹ See details at\n  &lt;https://tidyselect.r-lib.org/reference/faq-selection-context.html&gt;\n\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(cols_for_factor)\n\n  # Now:\n  data %&gt;% select(all_of(cols_for_factor))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n\n\n\n'data.frame':   161452 obs. of  34 variables:\n $ lca_contract_value: num  12 12.4 12.3 13.5 11.3 ...\n $ AFR               : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 2 2 2 2 ...\n $ EAP               : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ ECA               : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ LCR               : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ MNA               : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ SAR               : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ ca_Consul.Service : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 1 1 1 ...\n $ ca_Goods          : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 2 1 2 2 2 2 ...\n $ ca_bids_all       : int  6 3 3 1 5 6 1 6 5 2 ...\n $ ca_proc_open      : Factor w/ 2 levels \"0\",\"1\": 2 2 2 1 2 2 1 2 2 2 ...\n $ ca_proc_quality   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ ca_proc_source    : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 2 1 1 1 ...\n $ ca_proc_restricted: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ singleb           : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 2 1 1 1 ...\n $ corr_signp1       : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 2 1 1 2 2 1 ...\n $ corr_signp2       : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 2 1 1 2 ...\n $ corr_signp3       : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 1 1 1 ...\n $ nrc               : int  858 858 858 858 858 858 858 858 858 858 ...\n $ taxhaven          : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ not.taxhaven      : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 2 1 2 2 2 1 ...\n $ P                 : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 1 2 1 1 1 1 ...\n $ Health            : Factor w/ 2 levels \"0\",\"1\": 1 2 1 1 1 1 2 1 1 1 ...\n $ Edu               : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 1 1 1 1 1 1 ...\n $ W                 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 1 1 1 ...\n $ I                 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 2 1 1 2 1 1 ...\n $ Tran              : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ Fin               : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 2 ...\n $ A                 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Ener              : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Info              : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ H                 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Multi             : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Priv              : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:13] 59655 59815 138472 138473 138474 138475 138476 138477 138478 138479 ...\n  ..- attr(*, \"names\")= chr [1:13] \"59655\" \"59815\" \"138472\" \"138473\" ...\n\n\n\n\n\n\n\n\n\nn= 145307 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 145307 651308.50 12.28944  \n   2) ca_proc_open=0 64392 263413.50 11.67573  \n     4) not.taxhaven=0 43530 174856.70 11.34876  \n       8) nrc&lt; 4495.5 32645 124934.50 11.12766  \n        16) AFR=0 23100  90592.66 10.81381 *\n        17) AFR=1 9545  26559.77 11.88722 *\n       9) nrc&gt;=4495.5 10885  43540.74 12.01183 *\n     5) not.taxhaven=1 20862  74191.81 12.35800 *\n   3) ca_proc_open=1 80915 344342.10 12.77783  \n     6) P=1 22287  76991.40 11.79576 *\n     7) P=0 58628 237684.10 13.15116  \n      14) Tran=0 48010 179324.80 12.97448  \n        28) Ener=0 41969 148938.10 12.82764 *\n        29) Ener=1 6041  23194.13 13.99469 *\n      15) Tran=1 10618  50084.31 13.95003 *\n\n\n\n\n\n\n\n\n\n          CP nsplit rel error    xerror        xstd\n1 0.06686981      0 1.0000000 1.0000376 0.004230378\n2 0.04554926      1 0.9331302 0.9331805 0.003978562\n3 0.02205551      2 0.8875809 0.8876452 0.003861356\n4 0.01270517      3 0.8655254 0.8656022 0.003788718\n5 0.01104325      4 0.8528203 0.8529071 0.003725318\n6 0.01087315      5 0.8417770 0.8448465 0.003705916\n7 0.01000000      7 0.8200307 0.8201653 0.003598033\n\n\n\n\n[1] 0.01486034\n\n\n[1] 60814.71"
  },
  {
    "objectID": "ml_procurement.html#tuning-regression-tree",
    "href": "ml_procurement.html#tuning-regression-tree",
    "title": "Procurement Contracts",
    "section": "Tuning Regression Tree",
    "text": "Tuning Regression Tree\nWARNING: Takes a lot of time to run\n\n\nTime difference of 12.26575 mins\n\n\n\n\n  minsplit maxdepth   cp     error\n1        6       10 0.01 0.8200839\n2        9       15 0.01 0.8200990\n3       13        8 0.01 0.8200991\n4       17       15 0.01 0.8201020\n5       15       15 0.01 0.8201025\n\n\n\n\n[1] 0.01486034\n\n\n[1] 60814.71\n\n\n\n\n\n\n\nAbsolutely no difference between the automatically optimized regression tree and the tuned one."
  },
  {
    "objectID": "ml_procurement.html#bagging",
    "href": "ml_procurement.html#bagging",
    "title": "Procurement Contracts",
    "section": "Bagging",
    "text": "Bagging\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\n\n\n\nBagging regression trees with 25 bootstrap replications \n\nCall: bagging.data.frame(formula = train_y.t ~ ., data = train_x.t, \n    coob = TRUE, importance = TRUE)\n\nOut-of-bag estimate of root mean squared error:  1.9175 \n\n\n\n\n\n\n\n\n\n[1] 0.01477689\n\n\n[1] 60813.53\n\n\nWARNING: Takes time to run\n\n\nTime difference of 22.36131 mins\n\n\n\n\n\n\n\nWe can see that the error drops and suddenly rises. After that the fall is slower. Given that 11 bootstraps give the lowest error before the default of 25, we can use it to reduce computation time.\n\n\n[1] 1.917357\n\n\n\n\n\nBagging regression trees with 11 bootstrap replications \n\nCall: bagging.data.frame(formula = train_y.t ~ ., data = train_x.t, \n    coob = TRUE, nbagg = 11)\n\nOut-of-bag estimate of root mean squared error:  1.9174 \n\n\n\n\n[1] 0.01353659\n\n\n[1] 60816.51"
  },
  {
    "objectID": "ml_procurement.html#support-vector-machine",
    "href": "ml_procurement.html#support-vector-machine",
    "title": "Procurement Contracts",
    "section": "Support Vector Machine",
    "text": "Support Vector Machine\n\n\n\n\n\n\n\n'data.frame':   161125 obs. of  39 variables:\n $ ca_contract_valuec: chr  \"50.000-199.999\" \"200.000-\" \"200.000-\" \"200.000-\" ...\n $ lca_contract_value: num  12 12.4 12.3 13.5 11.3 ...\n $ AFR               : num  1 1 1 1 1 1 1 1 1 1 ...\n $ EAP               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ ECA               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ LCR               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ MNA               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ SAR               : num  0 0 0 0 0 0 0 0 0 0 ...\n $ ca_Consul.Service : num  0 0 0 1 0 0 0 0 0 0 ...\n $ ca_Goods          : num  0 0 0 0 1 0 1 1 1 1 ...\n $ ca_bids_all       : num  0.553 -0.14 -0.14 -0.603 0.322 ...\n $ ca_proc_open      : num  1 1 1 0 1 1 0 1 1 1 ...\n $ ca_proc_quality   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ ca_proc_source    : num  0 0 0 1 0 0 1 0 0 0 ...\n $ ca_proc_restricted: num  0 0 0 0 0 0 0 0 0 0 ...\n $ singleb           : num  0 0 0 1 0 0 1 0 0 0 ...\n $ corr_signp1       : num  1 0 0 0 1 0 0 1 1 0 ...\n $ corr_signp2       : int  0 0 0 0 0 0 1 0 0 1 ...\n $ corr_signp3       : int  0 0 0 1 0 0 0 0 0 0 ...\n $ nrc               : num  -0.841 -0.841 -0.841 -0.841 -0.841 ...\n $ taxhaven          : num  0 0 0 0 0 0 0 0 0 0 ...\n $ not.taxhaven      : num  0 0 0 1 1 0 1 1 1 0 ...\n $ cri_wb.d0         : num  1 0 0 0 1 0 0 1 1 0 ...\n $ cri_wb.d25        : num  0 1 1 0 0 1 0 0 0 1 ...\n $ cri_wb.d50        : num  0 0 0 0 0 0 0 0 0 0 ...\n $ cri_wb.d75        : num  0 0 0 1 0 0 1 0 0 0 ...\n $ P                 : num  1 0 0 0 0 1 0 0 0 0 ...\n $ Health            : num  0 1 0 0 0 0 1 0 0 0 ...\n $ Edu               : num  0 0 1 0 0 0 0 0 0 0 ...\n $ W                 : num  0 0 0 1 0 0 0 0 0 0 ...\n $ I                 : num  0 0 0 0 1 0 0 1 0 0 ...\n $ Tran              : num  0 0 0 0 0 0 0 0 1 0 ...\n $ Fin               : num  0 0 0 0 0 0 0 0 0 1 ...\n $ A                 : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Ener              : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Info              : num  0 0 0 0 0 0 0 0 0 0 ...\n $ H                 : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Multi             : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Priv              : num  0 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:13] 59655 59815 138472 138473 138474 138475 138476 138477 138478 138479 ...\n  ..- attr(*, \"names\")= chr [1:13] \"59655\" \"59815\" \"138472\" \"138473\" ...\n\n\nWe need scaled data, because the svm models take very long to run.\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\n\n\n       abv200k        nrc ca_bids_all P\n122931       0 -0.4101392  1.01534588 0\n98909        1 -0.1583185 -0.14045500 0\n65611        1  0.9206223  0.09070517 0\n95847        1 -0.4101392  2.63346713 0\n91914        1 -0.7741290 -0.37161518 0\n62368        1  0.9206223 -0.14045500 0\n\n\n\n\n119261 139363 107621 129186  93690  85110 \n     1      1      0      0      1      0 \nLevels: 0 1\n\n\n\n\n[1] 0.6296549\n\n\nWARNING: The next chunk is very time-consuming.\n\n\n\nCall:\nbest.tune(METHOD = svm, train.x = abv200k ~ ., data = Procurement.svm[tune_idx.svm, \n    ], ranges = list(cost = c(0.1, 1, 10, 100), gamma = c(0.25, 0.5, \n    1, 2)), kernel = \"radial\")\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  3783\n\n\nTime difference of 3.187033 mins\n\n\n\n\n[1] 0.6310204"
  },
  {
    "objectID": "ml_procurement.html#comparing-rmse-sse-prediction-rate",
    "href": "ml_procurement.html#comparing-rmse-sse-prediction-rate",
    "title": "Procurement Contracts",
    "section": "Comparing RMSE, SSE, & Prediction rate",
    "text": "Comparing RMSE, SSE, & Prediction rate\n\n\n          model.var   rmse.var  sse.var\n1        lm_ridge.c 0.01447194 56996.77\n2       lm_ridge.c2 0.01409955 57044.26\n3 spline_model.bids 0.01356418 70454.60\n4 spline_model.gam3 0.01400472 56856.29\n5        tuned_tree 0.01486034 60814.71\n6         bag_tree2 0.01353659 60816.51\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  pred.rate        pred.model\n1 0.6688758 log_model_train.2\n2 0.7591205              knn3\n3 0.6310204          svm.tune"
  },
  {
    "objectID": "ml_procurement.html#further-discussion",
    "href": "ml_procurement.html#further-discussion",
    "title": "Procurement Contracts",
    "section": "Further discussion",
    "text": "Further discussion\nThere is a variable for period between the time of award of the contract and the signing of the said contract. I excluded the variable for now, since there are many missing values and the effect on the value is unclear.\n\n\n   Length     Class      Mode \n   248000 character character \n\n\nWarning: NAs introduced by coercion\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.      NA's \n-29231.00      5.00     20.00     30.14     48.00   4384.00     11857 \n\n\n\n\n[1] 0.02747345\n\n\n\n\n\n\n\n\n\n    n\n1 195\n\n\n\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 13 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 13 rows containing missing values (geom_point)."
  },
  {
    "objectID": "rents.html",
    "href": "rents.html",
    "title": "Boston Renters",
    "section": "",
    "text": "Affordable rental housing or income-restricted rental housing usually means a dwelling unit for a lease that is capped for qualified households. Usually, they are reserved for people earning less than 80%, 50%, or 30% of Area Median Income (AMI). Boston already has one of the highest shares of income restricted housing per total units in the country. According to City of Boston, around 27% of all rental units are income restricted and 95% of all income restricted units are rental. Nevertheless, CHMA reports that for the next three years, the demand is estimated to be 34,675 new rental units, with only 16,450 units underway, facilitating shortage and further rent increase. Not to mention the record costs of building in Boston and limited space.\nBelow are the illustrations of the sample dimensions, calculated using American Community Survey public use samples (2021).\nThe tables illustrate the proportion of cost-burdened and severely cost-burdened households by race, size, and income groups in Boston. We can see that the distribution of different races, for example, is different from general population. According to the Boston Neighborhoods Demographics, in Boston overall, there are 44% of White and 22.7% of Black population, in contrast of around 36% and 18% in the target population, respectively. Not proportionate sample might introduce bias by overrepresenting and underrepresenting different groups. We can also observe that 9.6% do qualify for 80% AMI housing, even though they are cost burdened."
  },
  {
    "objectID": "rents.html#abstract",
    "href": "rents.html#abstract",
    "title": "Boston Renters",
    "section": "",
    "text": "Affordable rental housing or income-restricted rental housing usually means a dwelling unit for a lease that is capped for qualified households. Usually, they are reserved for people earning less than 80%, 50%, or 30% of Area Median Income (AMI). Boston already has one of the highest shares of income restricted housing per total units in the country. According to City of Boston, around 27% of all rental units are income restricted and 95% of all income restricted units are rental. Nevertheless, CHMA reports that for the next three years, the demand is estimated to be 34,675 new rental units, with only 16,450 units underway, facilitating shortage and further rent increase. Not to mention the record costs of building in Boston and limited space.\nBelow are the illustrations of the sample dimensions, calculated using American Community Survey public use samples (2021).\nThe tables illustrate the proportion of cost-burdened and severely cost-burdened households by race, size, and income groups in Boston. We can see that the distribution of different races, for example, is different from general population. According to the Boston Neighborhoods Demographics, in Boston overall, there are 44% of White and 22.7% of Black population, in contrast of around 36% and 18% in the target population, respectively. Not proportionate sample might introduce bias by overrepresenting and underrepresenting different groups. We can also observe that 9.6% do qualify for 80% AMI housing, even though they are cost burdened."
  },
  {
    "objectID": "rents.html#data-preparation",
    "href": "rents.html#data-preparation",
    "title": "Boston Renters",
    "section": "Data Preparation",
    "text": "Data Preparation\n\n\nCode\nlibrary(readr)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.5     ✔ dplyr   1.1.0\n✔ tibble  3.1.8     ✔ stringr 1.4.0\n✔ tidyr   1.2.0     ✔ forcats 0.5.1\n✔ purrr   1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nrents &lt;- read_csv(\"usa_00003.csv\")\n\n\nRows: 6605 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (17): YEAR, SAMPLE, SERIAL, CBSERIAL, HHWT, CLUSTER, CITY, STRATA, GQ, R...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nrents = rents %&gt;% \n  dplyr::mutate(rentyearly = RENTGRS*12) %&gt;% \n  dplyr::mutate(rentratio = rentyearly/HHINCOME)\n\n\n\n\nCode\ncount(rents, rentratio==0)\n\n\n# A tibble: 3 × 2\n  `rentratio == 0`     n\n  &lt;lgl&gt;            &lt;int&gt;\n1 FALSE             3439\n2 TRUE              3145\n3 NA                  21\n\n\n\n\nCode\nwhich(is.na(rents$rentratio))\n\n\n [1] 1266 1519 1527 1765 1766 1998 2019 2020 2827 2873 2890 3990 4037 4373 5229\n[16] 5739 5740 6122 6138 6139 6140\n\n\n\n\nCode\nrents[1266,\"rentratio\"]\n\n\n# A tibble: 1 × 1\n  rentratio\n      &lt;dbl&gt;\n1       NaN\n\n\n\n\nCode\nsum(is.na(rents))\n\n\n[1] 21\n\n\n\n\nCode\nrents = na.omit(rents)\n\n\n\n\nCode\nrents2 = rents %&gt;% \n  filter(rentratio!=0)\n\n\n\n\nCode\nplot(density(rents2$HHINCOME))\n\n\n\n\n\n\n\nCode\nplot(density(rents2$rentratio))\n\n\n\n\n\n\n\nCode\ncount(rents2, rentratio&gt;=0.3)\n\n\n# A tibble: 2 × 2\n  `rentratio &gt;= 0.3`     n\n  &lt;lgl&gt;              &lt;int&gt;\n1 FALSE               1781\n2 TRUE                1658\n\n\n\n\nCode\ncount(rents2, rentratio&gt;=0.5)\n\n\n# A tibble: 2 × 2\n  `rentratio &gt;= 0.5`     n\n  &lt;lgl&gt;              &lt;int&gt;\n1 FALSE               2526\n2 TRUE                 913\n\n\n\n\nCode\nrents2 = rents2 %&gt;% \n  dplyr::mutate(costbur = ifelse(rentratio&lt;0.3,\"Not CB\",\n                        ifelse(0.3&lt;=rentratio&rentratio&lt;0.5,\"CB\", \"Severely CB\")))\n\n\n\n\nCode\nrents2 = rents2 %&gt;% \n  filter(HHINCOME!=0)\n\n\n\n\nCode\nunique(rents2$RACE)\n\n\n[1] 1 2 7 8 6 4 9 5 3\n\n\n\n\nCode\nrace_df = data.frame(RACE = c(1,2,3,4,5,6,7,8,9),\n                        LRACE = c(\"White\", \n               \"Black\",\n               \"Native\",\n               \"Chinese\",\n               \"Japanese\",\n               \"Other Asian\",\n               \"Other\",\n               \"Two Races\",\n               \"Three Races\"))\n\nrents3 = merge(rents2, race_df, by = \"RACE\", all = FALSE)\n\n\n\n\nCode\nrents3 = rents3 %&gt;% \n  mutate(LRACE = ifelse(RACE==7,\"Hispanic\",LRACE)) %&gt;% \n  mutate(LRACE = ifelse(LRACE==\"Chinese\"|LRACE==\"Japanese\"|LRACE==\"Other Asian\",\"Asian\",LRACE)) %&gt;%\n  rename(Race=LRACE) %&gt;% \n  group_by(SERIAL) %&gt;% \n  mutate(hhsize = max(PERNUM))%&gt;% \n  mutate(ami = ifelse(hhsize==1&HHINCOME&lt;=78550,1,\n               ifelse(hhsize==2&HHINCOME&lt;=89750,1,\n               ifelse(hhsize==3&HHINCOME&lt;=100950,1,\n               ifelse(hhsize==4&HHINCOME&lt;=112150,1,\n               ifelse(hhsize&gt;=5&HHINCOME&lt;=130100,1,\n                      0)))))) %&gt;% \n  ungroup()\n\n\n\n\nCode\nrents3 %&gt;% \n  filter(costbur!=\"Not CB\") %&gt;% \n  filter(ami!=0) %&gt;% \n  count()\n\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  1400"
  },
  {
    "objectID": "rents.html#tables",
    "href": "rents.html#tables",
    "title": "Boston Renters",
    "section": "Tables",
    "text": "Tables\n\n\nCode\nrents3 %&gt;% \n  filter(ami!=0) %&gt;% \n  filter(costbur!=\"Not CB\") %&gt;%\n  mutate_at(vars(\"Race\"), as.factor) %&gt;% \n  group_by(Race, costbur) %&gt;% \n  summarize(Count = n()) %&gt;%\n  group_by(costbur) %&gt;% \n  mutate(Percent = Count/1400*100) %&gt;% \n  ungroup() %&gt;%\n  mutate(costbur = factor(costbur, levels = c(\"Not CB\",\n                                              \"Severely CB\",\n                                              \"CB\"))) %&gt;%\n  ggplot(aes(x=reorder(Race, desc(Percent)), \n             y=Percent, \n             fill=costbur)) + \n  scale_fill_manual(values = c(\"steelblue\",\n                               \"blue\"),\n                    name = element_blank()) +\n  geom_bar(stat='identity', position= \"stack\") +\n  geom_text(aes(label = round(Percent, 2)),\n            size = 2,\n            position = position_stack(vjust = 0.5),\n            color = \"white\") +\n  theme_minimal() +\n  xlab(element_blank()) +\n  labs(title = \"Cost Burdened Renter Households by Race in Boston\",\n       subtitle = \"Excluding those who do not qualify by 80% AMI\",\n       caption = \"Source: IPUMS USA (2021 ACS), Own Calculations\n       Note: Shares of cost burdened households were derived by deviding annual rent by income\") +\n  theme(legend.position = c(0.85,0.9),\n        title = element_text(size = 10))\n\n\n`summarise()` has grouped output by 'Race'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\nCode\nrents3 %&gt;% \n  group_by(SERIAL) %&gt;% \n  distinct(SERIAL, hhsize, costbur, ami) %&gt;% \n  filter(costbur!=\"Not CB\" & ami!=0)\n\n\n# A tibble: 713 × 4\n# Groups:   SERIAL [713]\n   SERIAL hhsize costbur       ami\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 641897      1 Severely CB     1\n 2 636155      6 Severely CB     1\n 3 633351      2 CB              1\n 4 642035      1 Severely CB     1\n 5 633475      2 Severely CB     1\n 6 642054      2 CB              1\n 7 623095      3 Severely CB     1\n 8 633640      3 Severely CB     1\n 9 623115      2 Severely CB     1\n10 642183      1 CB              1\n# … with 703 more rows\n\n\n\n\nCode\nrents3 %&gt;% \n  group_by(SERIAL) %&gt;% \n  dplyr::select(SERIAL,PERNUM,hhsize,HHINCOME,rentratio,costbur,Race,ami) %&gt;% \n  distinct(SERIAL, hhsize, HHINCOME, rentratio, costbur, ami) %&gt;% \n  mutate(hhsize = ifelse(hhsize&gt;=5, 6, hhsize)) %&gt;%\n  dplyr::mutate_at(vars(hhsize), ~ as.character(.)) %&gt;%\n  mutate(hhsize = ifelse(hhsize==\"6\", \"5+\", hhsize)) %&gt;% \n  filter(ami!=0) %&gt;% \n  filter(costbur!=\"Not CB\") %&gt;% \n  group_by(hhsize, costbur) %&gt;% \n  summarize(Count = n()) %&gt;%\n  group_by(costbur) %&gt;% \n  mutate(Percent = Count/713*100) %&gt;% \n  ungroup() %&gt;%\n  mutate(costbur = factor(costbur, levels = c(\"Not CB\",\n                                              \"Severely CB\",\n                                              \"CB\"))) %&gt;%\n  ggplot(aes(x=reorder(hhsize, desc(Percent)), \n             y=Percent, \n             fill=costbur)) + \n  scale_fill_manual(values = c(\"steelblue\",\n                               \"blue\"),\n                    name = element_blank()) +\n  geom_bar(stat='identity', position= \"stack\") +\n  geom_text(aes(label = round(Percent, 2)),\n            size = 2,\n            position = position_stack(vjust = 0.5),\n            color = \"white\") +\n  theme_minimal() +\n  xlab(\"Number of Household Members\") +\n  labs(title = \"Cost Burdened Renter Households by Size in Boston\",\n       subtitle = \"Excluding those who do not qualify by 80% AMI\",\n       caption = \"Source: IPUMS USA (2021 ACS), Own Calculations\") +\n  theme(legend.position = c(0.85,0.9),\n        title = element_text(size = 10))\n\n\n`summarise()` has grouped output by 'hhsize'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\nCode\nsum(!is.na(unique(rents3$SERIAL)))\n\n\n[1] 1576\n\n\n\n\nCode\nrents4 = rents3 %&gt;% \n  mutate(ami30 = ifelse(hhsize==1&HHINCOME&lt;=29450,1,\n               ifelse(hhsize==2&HHINCOME&lt;=33650,1,\n               ifelse(hhsize==3&HHINCOME&lt;=37850,1,\n               ifelse(hhsize==4&HHINCOME&lt;=42050,1,\n               ifelse(hhsize&gt;=5&HHINCOME&lt;=48800,1,\n                      0)))))) %&gt;%\n  mutate(ami50 = ifelse(hhsize==1&HHINCOME&lt;=49100,1,\n               ifelse(hhsize==2&HHINCOME&lt;=56100,1,\n               ifelse(hhsize==3&HHINCOME&lt;=63100,1,\n               ifelse(hhsize==4&HHINCOME&lt;=70100,1,\n               ifelse(hhsize&gt;=5&HHINCOME&lt;=81350,1,\n                      0)))))) %&gt;% \n  mutate(ami.cat = ifelse(ami30==1,\"30%&lt;\",\n                   ifelse(ami50==1,\"50%&lt;\",\n                   ifelse(ami==1,\"80%&lt;\",\n                          \"80%&gt;\"))))\n\n\n\n\nCode\nrents4 %&gt;% \n  group_by(SERIAL) %&gt;% \n  distinct(SERIAL,ami.cat,costbur) %&gt;% \n  filter(costbur!=\"Not CB\") %&gt;% \n  group_by(ami.cat, costbur) %&gt;% \n  summarize(Count = n()) %&gt;%\n  group_by(costbur) %&gt;% \n  mutate(Percent = Count/796*100) %&gt;% \n  ungroup() %&gt;%\n  mutate(costbur = factor(costbur, levels = c(\"Not CB\",\n                                              \"Severely CB\",\n                                              \"CB\"))) %&gt;%\n  ggplot(aes(x=reorder(ami.cat, desc(Percent)), \n             y=Percent, \n             fill=costbur)) + \n  scale_fill_manual(values = c(\"steelblue\",\n                               \"blue\"),\n                    name = element_blank()) +\n  geom_bar(stat='identity', position= \"stack\") +\n  geom_text(aes(label = round(Percent, 2)),\n            size = 2,\n            position = position_stack(vjust = 0.5),\n            color = \"white\") +\n  theme_minimal() +\n  xlab(\"Most Common AMI thresholds\") +\n  labs(title = \"Cost Burdened Renter Households by AMI % category\nin Boston\",\n       subtitle = \"Household Income as % of AMI\",\n       caption = \"Source: IPUMS USA (2021 ACS), Own Calculations\nNote: Percentages of the households are on the right side\") +\n  theme(legend.position = c(0.85,0.9),\n        title = element_text(size = 10))\n\n\n`summarise()` has grouped output by 'ami.cat'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "ml_procurement.html#comparing-rmse-sse-prediction-rate-and-conclusion",
    "href": "ml_procurement.html#comparing-rmse-sse-prediction-rate-and-conclusion",
    "title": "Procurement Contracts",
    "section": "Comparing RMSE, SSE, & Prediction rate and Conclusion",
    "text": "Comparing RMSE, SSE, & Prediction rate and Conclusion\n\n\n          model.var   rmse.var  sse.var\n1        lm_ridge.c 0.01447194 56996.77\n2       lm_ridge.c2 0.01409955 57044.26\n3 spline_model.bids 0.01356418 70454.60\n4 spline_model.gam3 0.01400472 56856.29\n5        tuned_tree 0.01486034 60814.71\n6         bag_tree2 0.01353659 60816.51\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  pred.rate        pred.model\n1 0.6688758 log_model_train.2\n2 0.7591205              knn3\n3 0.6310204          svm.tune"
>>>>>>> b72a3af (Seventh Commit with new projects and ignored procurement file)
  }
]